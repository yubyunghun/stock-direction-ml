{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "853a91fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows/Features: 2686 / 22\n",
      "Span: 2015-02-06 → 2025-10-10\n",
      "Pos rate: 0.53\n"
     ]
    }
   ],
   "source": [
    "# NB13 — Feature Refresh + Ablation + Prod Comparison\n",
    "import os, json, math, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, average_precision_score, brier_score_loss, log_loss\n",
    ")\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "DATA_DIR = Path(\"data\")\n",
    "ART_DIR  = Path(\"artifacts\")\n",
    "FIG_DIR  = Path(\"reports/figures\")\n",
    "for p in [DATA_DIR, ART_DIR, FIG_DIR]: p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load df (parquet preferred)\n",
    "df_path = DATA_DIR/\"df_nb02.parquet\" if (DATA_DIR/\"df_nb02.parquet\").exists() else DATA_DIR/\"df_nb02.csv\"\n",
    "df = pd.read_parquet(df_path) if df_path.suffix==\".parquet\" else pd.read_csv(df_path, parse_dates=[\"date\"])\n",
    "\n",
    "# Normalize date & sort\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "df = df.dropna(subset=[\"date\"]).sort_values(\"date\").reset_index(drop=True)\n",
    "\n",
    "# Ensure label 'y' exists (next-day up)\n",
    "if \"y\" not in df.columns:\n",
    "    if \"ret1\" in df.columns:\n",
    "        df[\"y\"] = (df[\"ret1\"].shift(-1) > 0).astype(int)\n",
    "    elif \"close\" in df.columns:\n",
    "        df[\"y\"] = (df[\"close\"].pct_change().shift(-1) > 0).astype(int)\n",
    "    else:\n",
    "        raise KeyError(\"No label y, ret1, or close available to derive labels.\")\n",
    "\n",
    "print(f\"Rows/Features: {len(df)} / {df.shape[1]}\")\n",
    "print(f\"Span: {df['date'].min().date()} → {df['date'].max().date()}\")\n",
    "print(\"Pos rate:\", round(float(df[\"y\"].mean()), 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "305c56ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_block(y_true, y_prob):\n",
    "    p = np.clip(np.asarray(y_prob), 1e-15, 1 - 1e-15)\n",
    "    return {\n",
    "        \"AUC\": roc_auc_score(y_true, p),\n",
    "        \"PR_AUC\": average_precision_score(y_true, p),\n",
    "        \"Brier\": brier_score_loss(y_true, p),\n",
    "        \"LogLoss\": log_loss(y_true, p),\n",
    "    }\n",
    "\n",
    "def sincos_time(dates, period=7):\n",
    "    dt = pd.to_datetime(dates, errors=\"coerce\")\n",
    "    dtd = dt.dt if hasattr(dt, \"dt\") else dt\n",
    "    if period == 7:\n",
    "        idx = dtd.dayofweek\n",
    "    else:\n",
    "        idx = (dtd.dayofyear % period)\n",
    "    idx = np.asarray(idx, dtype=float)\n",
    "    ang = 2*np.pi*idx/float(period)\n",
    "    return np.cos(ang), np.sin(ang)\n",
    "\n",
    "def zscore(a, w):\n",
    "    s = pd.Series(a).astype(float)\n",
    "    mu = s.rolling(w, min_periods=max(3, w//3)).mean()\n",
    "    sd = s.rolling(w, min_periods=max(3, w//3)).std(ddof=0)\n",
    "    return ((s - mu) / (sd.replace(0, np.nan))).fillna(0.0).values\n",
    "\n",
    "def kpis_from_signal(returns, signal, fee_bps=5.0, slippage_bps=0.0, freq=252):\n",
    "    fee = fee_bps/10000.0 + slippage_bps/10000.0\n",
    "    flips = (np.abs(np.diff(np.r_[0, signal])) > 0).astype(int)\n",
    "    r = (signal * returns) - flips * fee\n",
    "    r = pd.Series(r).fillna(0.0)\n",
    "    eq = (1 + r).cumprod()\n",
    "    cagr = (1 + r).prod() ** (freq / max(len(r),1)) - 1\n",
    "    vol  = r.std() * np.sqrt(freq)\n",
    "    sharpe = (cagr / vol) if vol > 0 else np.nan\n",
    "    maxdd = (eq / eq.cummax() - 1).min()\n",
    "    return dict(Sharpe=float(sharpe), CAGR=float(cagr),\n",
    "                total_return=float(eq.iloc[-1]-1), MaxDD=float(maxdd), vol_annual=float(vol))\n",
    "\n",
    "def strat_sharpe_from_probs(p, ret_next, fee_bps=5.0, slippage_bps=0.0, thr=0.55):\n",
    "    s = (p >= thr).astype(int)\n",
    "    return kpis_from_signal(ret_next, s, fee_bps=fee_bps, slippage_bps=slippage_bps)[\"Sharpe\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fd0ce8bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New features created: 26\n",
      "Examples: ['vol_std5', 'vol_std10', 'vol_std20', 'trend_slope10', 'trend_slope20', 'trend_slope60', 'ret1_z5', 'ret1_z10', 'ret1_z20', 'dow_cos', 'dow_sin', 'mkt_ret1_x_macd']\n"
     ]
    }
   ],
   "source": [
    "new_feats = []\n",
    "\n",
    "# Volatility (rolling std of daily returns) if close present\n",
    "if \"close\" in df.columns:\n",
    "    ret1 = pd.Series(df[\"close\"]).pct_change()\n",
    "    for w in [5, 10, 20]:\n",
    "        df[f\"vol_std{w}\"] = ret1.rolling(w, min_periods=max(3, w//3)).std(ddof=0)\n",
    "        new_feats.append(f\"vol_std{w}\")\n",
    "\n",
    "# Trend via simple slopes on 'close' (if present)\n",
    "def slope(series, w):\n",
    "    s = pd.Series(series).astype(float)\n",
    "    idx = np.arange(len(s))\n",
    "    # rolling slope via cov/var\n",
    "    return (s.rolling(w).apply(\n",
    "        lambda x: np.cov(np.arange(len(x)), x, ddof=0)[0,1]/np.var(np.arange(len(x)), ddof=0) if np.var(np.arange(len(x)), ddof=0)>0 else 0.0,\n",
    "        raw=False\n",
    "    ))\n",
    "if \"close\" in df.columns:\n",
    "    for w in [10, 20, 60]:\n",
    "        nm = f\"trend_slope{w}\"\n",
    "        df[nm] = slope(df[\"close\"], w).fillna(0.0).values\n",
    "        new_feats.append(nm)\n",
    "\n",
    "# Ret1 z-scores (if ret1 present or derivable)\n",
    "if \"ret1\" not in df.columns and \"close\" in df.columns:\n",
    "    df[\"ret1\"] = pd.Series(df[\"close\"]).pct_change()\n",
    "if \"ret1\" in df.columns:\n",
    "    for w in [5, 10, 20]:\n",
    "        df[f\"ret1_z{w}\"] = zscore(df[\"ret1\"].fillna(0).to_numpy(), w)\n",
    "        new_feats.append(f\"ret1_z{w}\")\n",
    "\n",
    "# Seasonality (day-of-week)\n",
    "c7, s7 = sincos_time(df[\"date\"], period=7)\n",
    "df[\"dow_cos\"], df[\"dow_sin\"] = c7, s7\n",
    "new_feats += [\"dow_cos\",\"dow_sin\"]\n",
    "\n",
    "# Cross-interactions with market context if available\n",
    "cross_feats = []\n",
    "ctx = [\"mkt_ret1\",\"mkt_ret5\",\"vix_chg1\"]\n",
    "base = []\n",
    "for b in [\"macd\",\"macd_signal\",\"ret1_z5\",\"ret1_z10\",\"ret1_z20\"]:\n",
    "    if b in df.columns: base.append(b)\n",
    "if \"ret1\" in df.columns and \"ret1_z5\" not in base:\n",
    "    base.append(\"ret1\")\n",
    "for a in ctx:\n",
    "    if a in df.columns:\n",
    "        for b in base:\n",
    "            nm = f\"{a}_x_{b.split('_')[0]}\"\n",
    "            df[nm] = df[a].astype(float) * df[b].astype(float)\n",
    "            cross_feats.append(nm)\n",
    "\n",
    "print(\"New features created:\", len(new_feats)+len(cross_feats))\n",
    "print(\"Examples:\", (new_feats+cross_feats)[:12])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a13232ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-split with engineered features:\n",
      "train/val/test: (1611, 39) (537, 39) (538, 39)\n"
     ]
    }
   ],
   "source": [
    "df = df.sort_values(\"date\").reset_index(drop=True)\n",
    "n = len(df); n_tr = int(0.6*n); n_va = int(0.2*n)\n",
    "df_tr = df.iloc[:n_tr].copy()\n",
    "df_va = df.iloc[n_tr:n_tr+n_va].copy()\n",
    "df_te = df.iloc[n_tr+n_va:].copy()\n",
    "\n",
    "print(\"Re-split with engineered features:\")\n",
    "print(\"train/val/test:\", df_tr.shape, df_va.shape, df_te.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9924672b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final feature count: 33\n",
      "First 10: ['close', 'high', 'low', 'macd', 'macd_signal', 'mkt_ret1', 'mkt_ret5', 'open', 'ret1', 'ret10']\n",
      "Shapes -> X_tr: (1611, 33) X_va: (537, 33) X_te: (538, 33)\n"
     ]
    }
   ],
   "source": [
    "# Base features from artifacts (if exist)\n",
    "try:\n",
    "    feat_art = json.loads((ART_DIR/\"feature_list.json\").read_text(encoding=\"utf-8\"))\n",
    "except Exception:\n",
    "    feat_art = []\n",
    "\n",
    "candidate_lists = []\n",
    "for name in [\"new_feats\", \"cross_feats\"]:\n",
    "    if name in globals() and isinstance(globals()[name], (list, tuple)):\n",
    "        candidate_lists.extend(list(globals()[name]))\n",
    "\n",
    "requested = list(feat_art) + list(candidate_lists)\n",
    "\n",
    "drop_cols = {\"y\",\"ret_next\",\"date\",\"ticker\",\"symbol\"}\n",
    "seen, all_feats, skipped = set(), [], []\n",
    "for c in requested:\n",
    "    if c in seen or c in drop_cols: \n",
    "        continue\n",
    "    if c in df.columns and pd.api.types.is_numeric_dtype(df[c]):\n",
    "        all_feats.append(c); seen.add(c)\n",
    "    else:\n",
    "        skipped.append(c)\n",
    "\n",
    "if skipped:\n",
    "    print(\"Skipping (missing/non-numeric):\", skipped[:12], (\"...\" if len(skipped) > 12 else \"\"))\n",
    "print(\"Final feature count:\", len(all_feats))\n",
    "print(\"First 10:\", all_feats[:10])\n",
    "\n",
    "# Ensure returns available for VA/TE for threshold tuning/backtest\n",
    "def ensure_ret_next(d):\n",
    "    if \"ret_next\" in d.columns:\n",
    "        return d[\"ret_next\"].astype(float).to_numpy()\n",
    "    if \"close\" in d.columns:\n",
    "        return pd.Series(d[\"close\"]).pct_change().shift(-1).to_numpy()\n",
    "    raise KeyError(\"Need 'ret_next' or 'close' to compute returns.\")\n",
    "r_va = ensure_ret_next(df_va)\n",
    "r_te = ensure_ret_next(df_te)\n",
    "\n",
    "# Arrays\n",
    "X_tr = df_tr[all_feats].to_numpy(); y_tr = df_tr[\"y\"].astype(int).to_numpy()\n",
    "X_va = df_va[all_feats].to_numpy(); y_va = df_va[\"y\"].astype(int).to_numpy()\n",
    "X_te = df_te[all_feats].to_numpy(); y_te = df_te[\"y\"].astype(int).to_numpy()\n",
    "\n",
    "print(\"Shapes -> X_tr:\", X_tr.shape, \"X_va:\", X_va.shape, \"X_te:\", X_te.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "64a84438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top LR configs (by val AUC):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C</th>\n",
       "      <th>val_AUC</th>\n",
       "      <th>test_AUC</th>\n",
       "      <th>val_PR</th>\n",
       "      <th>test_PR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.474235</td>\n",
       "      <td>0.475888</td>\n",
       "      <td>0.491897</td>\n",
       "      <td>0.530520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.472625</td>\n",
       "      <td>0.476250</td>\n",
       "      <td>0.489142</td>\n",
       "      <td>0.529797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.472042</td>\n",
       "      <td>0.475162</td>\n",
       "      <td>0.488436</td>\n",
       "      <td>0.528502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.472000</td>\n",
       "      <td>0.470782</td>\n",
       "      <td>0.487706</td>\n",
       "      <td>0.525420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.471834</td>\n",
       "      <td>0.473907</td>\n",
       "      <td>0.488461</td>\n",
       "      <td>0.527526</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     C   val_AUC  test_AUC    val_PR   test_PR\n",
       "0  0.1  0.474235  0.475888  0.491897  0.530520\n",
       "1  0.5  0.472625  0.476250  0.489142  0.529797\n",
       "2  1.0  0.472042  0.475162  0.488436  0.528502\n",
       "4  5.0  0.472000  0.470782  0.487706  0.525420\n",
       "3  2.0  0.471834  0.473907  0.488461  0.527526"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best (val AUC): 0.4742\n",
      "NaN check (should be 0s): 0 0 0\n"
     ]
    }
   ],
   "source": [
    "# PATCHED Cell 6 — Impute → Scale → Sweep LR\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# 1) Impute on TRAIN only, then apply to VAL/TEST\n",
    "imp = SimpleImputer(strategy=\"median\")\n",
    "X_tr_i = imp.fit_transform(X_tr)\n",
    "X_va_i = imp.transform(X_va)\n",
    "X_te_i = imp.transform(X_te)\n",
    "\n",
    "# Replace any inf that might sneak in (just in case)\n",
    "X_tr_i = np.nan_to_num(X_tr_i, posinf=0.0, neginf=0.0)\n",
    "X_va_i = np.nan_to_num(X_va_i, posinf=0.0, neginf=0.0)\n",
    "X_te_i = np.nan_to_num(X_te_i, posinf=0.0, neginf=0.0)\n",
    "\n",
    "# 2) Standardize\n",
    "scaler = StandardScaler().fit(X_tr_i)\n",
    "X_tr_s = scaler.transform(X_tr_i)\n",
    "X_va_s = scaler.transform(X_va_i)\n",
    "X_te_s = scaler.transform(X_te_i)\n",
    "\n",
    "# 3) Hyperparam sweep for LR (balanced)\n",
    "LR_GRID = {\"C\":[0.1, 0.5, 1.0, 2.0, 5.0]}\n",
    "\n",
    "def train_eval_lr(C=1.0, penalty=\"l2\", class_weight=\"balanced\"):\n",
    "    lr = LogisticRegression(\n",
    "        C=C, penalty=penalty, solver=\"lbfgs\", max_iter=2000,\n",
    "        class_weight=class_weight, random_state=SEED\n",
    "    )\n",
    "    lr.fit(X_tr_s, y_tr)\n",
    "    p_va = lr.predict_proba(X_va_s)[:,1]\n",
    "    p_te = lr.predict_proba(X_te_s)[:,1]\n",
    "    m_va = metric_block(y_va, p_va)\n",
    "    m_te = metric_block(y_te, p_te)\n",
    "    return lr, p_va, p_te, m_va, m_te\n",
    "\n",
    "rows = []; best = None\n",
    "for C in LR_GRID[\"C\"]:\n",
    "    lr, pva, pte, mva, mte = train_eval_lr(C=C)\n",
    "    row = {\"C\": C, \"val_AUC\": mva[\"AUC\"], \"test_AUC\": mte[\"AUC\"], \"val_PR\": mva[\"PR_AUC\"], \"test_PR\": mte[\"PR_AUC\"]}\n",
    "    rows.append(row)\n",
    "    if best is None or row[\"val_AUC\"] > best[\"val_AUC\"]:\n",
    "        best = dict(**row, model=lr, p_va=pva, p_te=pte)\n",
    "\n",
    "hgrid = pd.DataFrame(rows).sort_values(\"val_AUC\", ascending=False)\n",
    "hgrid.to_csv(DATA_DIR/\"nb13_hparam_lr.csv\", index=False)\n",
    "print(\"Top LR configs (by val AUC):\")\n",
    "display(hgrid.head(10))\n",
    "print(\"Best (val AUC):\", round(best[\"val_AUC\"], 4))\n",
    "print(\"NaN check (should be 0s):\", \n",
    "      np.isnan(X_tr_s).sum(), np.isnan(X_va_s).sum(), np.isnan(X_te_s).sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f054834c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAL Sharpe tuned threshold: 0.505 | Sharpe(val): 0.288\n",
      "TEST block: {'AUC_test': 0.4759, 'Brier': 0.2585, 'LogLoss': 0.7102, 'Sharpe': -0.9772, 'CAGR': -0.1607, 'total_return': -0.3121, 'MaxDD': -0.334, 'thr': 0.505}\n"
     ]
    }
   ],
   "source": [
    "FEE_BPS = 5.0\n",
    "SLIP_BPS = 0.0\n",
    "\n",
    "thr_grid = np.linspace(0.50, 0.65, 31)\n",
    "scores = [(t, strat_sharpe_from_probs(best[\"p_va\"], r_va, fee_bps=FEE_BPS, slippage_bps=SLIP_BPS, thr=t)) for t in thr_grid]\n",
    "t_star, val_sharpe = max(scores, key=lambda x: (x[1], -abs(x[0]-0.59)))  # tie-break near 0.59\n",
    "\n",
    "# TEST block\n",
    "p_te = best[\"model\"].predict_proba(X_te_s)[:,1]\n",
    "eval_te = metric_block(y_te, p_te)\n",
    "sig_te = (p_te >= t_star).astype(int)\n",
    "bt_te  = kpis_from_signal(r_te, sig_te, fee_bps=FEE_BPS, slippage_bps=SLIP_BPS)\n",
    "\n",
    "print(f\"VAL Sharpe tuned threshold: {t_star:.3f} | Sharpe(val): {val_sharpe:.3f}\")\n",
    "test_block = {\n",
    "    \"AUC_test\": round(eval_te[\"AUC\"], 4),\n",
    "    \"Brier\": round(eval_te[\"Brier\"], 4),\n",
    "    \"LogLoss\": round(eval_te[\"LogLoss\"], 4),\n",
    "    \"Sharpe\": round(bt_te[\"Sharpe\"], 4),\n",
    "    \"CAGR\": round(bt_te[\"CAGR\"], 4),\n",
    "    \"total_return\": round(bt_te[\"total_return\"], 4),\n",
    "    \"MaxDD\": round(bt_te[\"MaxDD\"], 4),\n",
    "    \"thr\": round(float(t_star), 3),\n",
    "}\n",
    "print(\"TEST block:\", test_block)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "13de5153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC(PROD) : 0.479\n",
      "AUC(NB13) : 0.4759\n",
      "ΔAUC (NB13-PROD): -0.0031\n"
     ]
    }
   ],
   "source": [
    "# Load PROD model+scaler+feature list if available\n",
    "auc_prod = None\n",
    "try:\n",
    "    prod_lr = json.loads((ART_DIR/\"backtest_summary.json\").read_text())  # also confirms artifacts exist\n",
    "    lr_prod  = pd.read_pickle(ART_DIR/\"lr.joblib\") if (ART_DIR/\"lr.joblib\").suffix==\".pkl\" else None\n",
    "except Exception:\n",
    "    lr_prod = None\n",
    "\n",
    "# Safer loader (joblib)\n",
    "from joblib import load\n",
    "try:\n",
    "    lr_prod = load(ART_DIR/\"lr.joblib\")\n",
    "    scaler_prod = load(ART_DIR/\"scaler.joblib\") if (ART_DIR/\"scaler.joblib\").exists() else None\n",
    "    feat_prod = json.loads((ART_DIR/\"feature_list.json\").read_text(encoding=\"utf-8\"))\n",
    "    Xp = df_te[[c for c in feat_prod if c in df_te.columns]].to_numpy()\n",
    "    if scaler_prod is not None: Xp = scaler_prod.transform(Xp)\n",
    "    p_prod_te = lr_prod.predict_proba(Xp)[:,1]\n",
    "    auc_prod = roc_auc_score(y_te, p_prod_te)\n",
    "except Exception as e:\n",
    "    print(\"PROD compare skipped:\", e)\n",
    "\n",
    "auc_nb13 = roc_auc_score(y_te, p_te)\n",
    "if auc_prod is not None:\n",
    "    print(\"AUC(PROD) :\", round(float(auc_prod), 4))\n",
    "print(\"AUC(NB13) :\", round(float(auc_nb13), 4))\n",
    "if auc_prod is not None:\n",
    "    print(\"ΔAUC (NB13-PROD):\", round(float(auc_nb13 - auc_prod), 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5a177a62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ablations (val AUC drop when group removed):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group</th>\n",
       "      <th>removed</th>\n",
       "      <th>val_AUC</th>\n",
       "      <th>delta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>trend</td>\n",
       "      <td>3</td>\n",
       "      <td>0.463241</td>\n",
       "      <td>-0.010995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>interactions</td>\n",
       "      <td>6</td>\n",
       "      <td>0.466600</td>\n",
       "      <td>-0.007635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>zscore</td>\n",
       "      <td>3</td>\n",
       "      <td>0.470987</td>\n",
       "      <td>-0.003248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>price_core</td>\n",
       "      <td>5</td>\n",
       "      <td>0.473610</td>\n",
       "      <td>-0.000625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>seasonality</td>\n",
       "      <td>2</td>\n",
       "      <td>0.474915</td>\n",
       "      <td>0.000680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ta</td>\n",
       "      <td>3</td>\n",
       "      <td>0.475235</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>volatility</td>\n",
       "      <td>5</td>\n",
       "      <td>0.476956</td>\n",
       "      <td>0.002721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>market_ctx</td>\n",
       "      <td>3</td>\n",
       "      <td>0.491393</td>\n",
       "      <td>0.017158</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          group  removed   val_AUC     delta\n",
       "0         trend        3  0.463241 -0.010995\n",
       "1  interactions        6  0.466600 -0.007635\n",
       "2        zscore        3  0.470987 -0.003248\n",
       "3    price_core        5  0.473610 -0.000625\n",
       "4   seasonality        2  0.474915  0.000680\n",
       "5            ta        3  0.475235  0.001000\n",
       "6    volatility        5  0.476956  0.002721\n",
       "7    market_ctx        3  0.491393  0.017158"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: data\\nb13_ablation_groups.csv\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Define groups (then intersect with all_feats to avoid missing cols)\n",
    "groups = {\n",
    "    \"price_core\": [\"open\",\"high\",\"low\",\"close\",\"volume\"],\n",
    "    \"volatility\": [c for c in all_feats if c.startswith(\"vol_std\") or c in [\"vol10\",\"volz\"]],\n",
    "    \"trend\": [c for c in all_feats if c.startswith(\"trend_slope\")],\n",
    "    \"zscore\": [c for c in all_feats if c.startswith(\"ret1_z\")],\n",
    "    \"seasonality\": [c for c in [\"dow_cos\",\"dow_sin\"] if c in all_feats],\n",
    "    \"ta\": [c for c in [\"macd\",\"macd_signal\",\"rsi14\"] if c in all_feats],\n",
    "    \"market_ctx\": [c for c in [\"mkt_ret1\",\"mkt_ret5\",\"vix_chg1\",\"spy_close\",\"vix_close\"] if c in all_feats],\n",
    "    \"interactions\": [c for c in all_feats if \"_x_\" in c],\n",
    "}\n",
    "\n",
    "base_auc = roc_auc_score(y_va, best[\"p_va\"])\n",
    "abl_rows = []\n",
    "\n",
    "for gname, cols in groups.items():\n",
    "    cols = [c for c in cols if c in all_feats]\n",
    "    if not cols:\n",
    "        continue\n",
    "    keep = [c for c in all_feats if c not in cols]\n",
    "    if not keep:\n",
    "        continue\n",
    "\n",
    "    # Build reduced TRAIN/VAL matrices\n",
    "    Xtr_g = df_tr[keep].to_numpy()\n",
    "    Xva_g = df_va[keep].to_numpy()\n",
    "\n",
    "    # Impute (fit on TRAIN only) → sanitize → scale (fit on TRAIN only)\n",
    "    imp_g = SimpleImputer(strategy=\"median\")\n",
    "    Xtr_g = imp_g.fit_transform(Xtr_g)\n",
    "    Xva_g = imp_g.transform(Xva_g)\n",
    "\n",
    "    Xtr_g = np.nan_to_num(Xtr_g, posinf=0.0, neginf=0.0)\n",
    "    Xva_g = np.nan_to_num(Xva_g, posinf=0.0, neginf=0.0)\n",
    "\n",
    "    sc_g = StandardScaler().fit(Xtr_g)\n",
    "    Xtr_g = sc_g.transform(Xtr_g)\n",
    "    Xva_g = sc_g.transform(Xva_g)\n",
    "\n",
    "    # Retrain LR with same C as the best model\n",
    "    C = best[\"C\"]\n",
    "    lr_g = LogisticRegression(\n",
    "        C=C, penalty=\"l2\", solver=\"lbfgs\", max_iter=2000,\n",
    "        class_weight=\"balanced\", random_state=SEED\n",
    "    )\n",
    "    lr_g.fit(Xtr_g, y_tr)\n",
    "    p_va_g = lr_g.predict_proba(Xva_g)[:, 1]\n",
    "    auc_g = roc_auc_score(y_va, p_va_g)\n",
    "\n",
    "    abl_rows.append({\n",
    "        \"group\": gname,\n",
    "        \"removed\": len(cols),\n",
    "        \"val_AUC\": auc_g,\n",
    "        \"delta\": auc_g - base_auc\n",
    "    })\n",
    "\n",
    "ablt = pd.DataFrame(abl_rows).sort_values(\"delta\").reset_index(drop=True)\n",
    "print(\"Ablations (val AUC drop when group removed):\")\n",
    "display(ablt.head(10))\n",
    "ablt.to_csv(DATA_DIR/\"nb13_ablation_groups.csv\", index=False)\n",
    "print(\"Saved:\", DATA_DIR/\"nb13_ablation_groups.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "62164de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: artifacts\\nb13_lr_20251023T011130Z.joblib | artifacts\\nb13_scaler_20251023T011130Z.joblib | artifacts\\nb13_threshold_20251023T011130Z.json\n"
     ]
    }
   ],
   "source": [
    "from joblib import dump\n",
    "ts = datetime.utcnow().strftime(\"%Y%m%dT%H%M%SZ\")\n",
    "dump(best[\"model\"], ART_DIR/f\"nb13_lr_{ts}.joblib\")\n",
    "dump(scaler, ART_DIR/f\"nb13_scaler_{ts}.joblib\")\n",
    "with open(ART_DIR/f\"nb13_threshold_{ts}.json\",\"w\",encoding=\"utf-8\") as f:\n",
    "    json.dump({\"tau\": float(t_star), \"grid\": list(map(float, thr_grid)), \"val_sharpe\": float(val_sharpe)}, f, indent=2)\n",
    "print(\"Saved:\", ART_DIR/f\"nb13_lr_{ts}.joblib\", \"|\", ART_DIR/f\"nb13_scaler_{ts}.joblib\", \"|\", ART_DIR/f\"nb13_threshold_{ts}.json\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
