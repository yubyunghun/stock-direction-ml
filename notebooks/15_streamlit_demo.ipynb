{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b1d8b2bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repo root â†’ C:\\.projects\\stock-direction-ml\\notebooks\n",
      "Has data?       True\n",
      "Has artifacts?  True\n"
     ]
    }
   ],
   "source": [
    "# NB15 â€” Repo root autodetect (data/ + artifacts/ locator)\n",
    "from pathlib import Path\n",
    "\n",
    "def find_repo_root(start: Path, must_have=(\"data\", \"artifacts\")) -> Path:\n",
    "    cur = start.resolve()\n",
    "    for _ in range(6):\n",
    "        if all((cur / m).exists() for m in must_have):\n",
    "            return cur\n",
    "        cur = cur.parent\n",
    "    if start.name.lower() == \"notebooks\" and all((start.parent / m).exists() for m in must_have):\n",
    "        return start.parent.resolve()\n",
    "    raise FileNotFoundError(f\"Could not locate repo root containing {must_have} starting at {start}\")\n",
    "\n",
    "CWD = Path.cwd()\n",
    "ROOT = find_repo_root(CWD)\n",
    "print(\"Repo root â†’\", ROOT)\n",
    "print(\"Has data?      \", (ROOT/\"data\").exists())\n",
    "print(\"Has artifacts? \", (ROOT/\"artifacts\").exists())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a30dc41b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: C:\\.projects\\stock-direction-ml\\notebooks\\app\\streamlit_app.py\n"
     ]
    }
   ],
   "source": [
    "# NB15 â€” Write the Streamlit app to ROOT/app/streamlit_app.py\n",
    "from pathlib import Path\n",
    "\n",
    "APP_DIR = (ROOT / \"app\")\n",
    "APP_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "APP_CODE = r'''\n",
    "# Streamlit demo for Stock/Crypto Direction Classifier (NB15)\n",
    "# - Loads df_nb02 (csv or parquet) and artifacts (feature_list.json, scaler.joblib, lr.joblib, threshold.json)\n",
    "# - UI: set threshold (tau) and fee_bps\n",
    "# - Metrics: AUC, AP (area under PR), Brier, LogLoss on the selected date range\n",
    "# - Plots: Equity vs Buy&Hold, ROC, PR, Calibration\n",
    "import os, json, pathlib\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    average_precision_score,\n",
    "    brier_score_loss,\n",
    "    log_loss,\n",
    "    roc_curve,\n",
    "    precision_recall_curve\n",
    ")\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "import streamlit as st\n",
    "\n",
    "# ----------------------\n",
    "# Paths & loaders\n",
    "# ----------------------\n",
    "HERE = Path(__file__).resolve()\n",
    "ROOT = HERE.parent.parent  # repo root (this file lives in app/)\n",
    "\n",
    "def load_df(root: Path) -> pd.DataFrame:\n",
    "    data_dir = root / \"data\"\n",
    "    csv_path = data_dir / \"df_nb02.csv\"\n",
    "    pq_path  = data_dir / \"df_nb02.parquet\"\n",
    "    if csv_path.exists():\n",
    "        df = pd.read_csv(csv_path)\n",
    "    elif pq_path.exists():\n",
    "        df = pd.read_parquet(pq_path)\n",
    "    else:\n",
    "        st.error(\"Missing data file: expected data/df_nb02.csv or data/df_nb02.parquet\")\n",
    "        st.stop()\n",
    "\n",
    "    for c in [\"date\", \"Date\", \"timestamp\", \"ts\"]:\n",
    "        if c in df.columns:\n",
    "            try:\n",
    "                df[c] = pd.to_datetime(df[c])\n",
    "            except Exception:\n",
    "                pass\n",
    "            if c != \"date\":\n",
    "                df[\"date\"] = df[c]\n",
    "            break\n",
    "    return df\n",
    "\n",
    "def load_artifacts(root: Path):\n",
    "    art = root / \"artifacts\"\n",
    "    with open(art / \"feature_list.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        feature_list = json.load(f)\n",
    "    scaler = joblib.load(art / \"scaler.joblib\")\n",
    "    model  = joblib.load(art / \"lr.joblib\")\n",
    "    tau_file = art / \"threshold.json\"\n",
    "    tau_val = None\n",
    "    if tau_file.exists():\n",
    "        try:\n",
    "            t = json.load(open(tau_file, \"r\", encoding=\"utf-8\"))\n",
    "            tau_val = t.get(\"tau\") or t.get(\"threshold\") or t.get(\"value\")\n",
    "        except Exception:\n",
    "            tau_val = None\n",
    "    return feature_list, scaler, model, tau_val\n",
    "\n",
    "def infer_target(df: pd.DataFrame):\n",
    "    candidates = [\"y\", \"label\", \"target\", \"y_bin\", \"direction\", \"is_up\", \"class\", \"cls\"]\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            y = df[c].astype(int).clip(0, 1).values\n",
    "            return y, c\n",
    "    if \"ret_next\" in df.columns:\n",
    "        y = (df[\"ret_next\"].astype(float) > 0).astype(int).values\n",
    "        return y, \"ret_next>0\"\n",
    "    if \"close\" in df.columns:\n",
    "        ret_next = df[\"close\"].astype(float).pct_change().shift(-1).fillna(0.0)\n",
    "        df[\"ret_next\"] = ret_next\n",
    "        y = (ret_next > 0).astype(int).values\n",
    "        return y, \"ret_next_from_close>0\"\n",
    "    return None, None\n",
    "\n",
    "def make_dataset(df: pd.DataFrame, features: list):\n",
    "    cols = [c for c in features if c in df.columns]\n",
    "    if not cols:\n",
    "        raise ValueError(\"None of the expected features are present in df_nb02. Check artifacts/feature_list.json vs data columns.\")\n",
    "    tmp = df[cols].replace([np.inf, -np.inf], np.nan)\n",
    "    y_vals, y_name = infer_target(df)\n",
    "    if y_vals is None:\n",
    "        return None, None, None, None, None\n",
    "    if \"ret_next\" in df.columns:\n",
    "        retn = df[\"ret_next\"].astype(float).values\n",
    "    elif \"close\" in df.columns:\n",
    "        retn = df[\"close\"].astype(float).pct_change().shift(-1).fillna(0.0).values\n",
    "    else:\n",
    "        retn = np.zeros(len(df), dtype=float)\n",
    "    tmp[\"__y__\"] = y_vals\n",
    "    tmp[\"__ret_next__\"] = retn\n",
    "    tmp = tmp.dropna()\n",
    "    X  = tmp[cols].to_numpy()\n",
    "    y  = tmp[\"__y__\"].astype(int).to_numpy()\n",
    "    retn = tmp[\"__ret_next__\"].astype(float).to_numpy()\n",
    "    idx = tmp.index\n",
    "    return X, y, retn, idx, y_name\n",
    "\n",
    "def predict_proba(model, X: np.ndarray) -> np.ndarray:\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        p = model.predict_proba(X)\n",
    "        if p.ndim == 2 and p.shape[1] == 2:\n",
    "            return p[:, 1]\n",
    "        if p.ndim == 1:\n",
    "            return p\n",
    "    if hasattr(model, \"decision_function\"):\n",
    "        s = model.decision_function(X)\n",
    "        return 1 / (1 + np.exp(-s))\n",
    "    pred = model.predict(X)\n",
    "    return np.clip(pred.astype(float), 0.0, 1.0)\n",
    "\n",
    "# ----------------------\n",
    "# Streamlit UI\n",
    "# ----------------------\n",
    "st.set_page_config(page_title=\"Direction Classifier Demo\", layout=\"wide\")\n",
    "st.title(\"ðŸ“ˆ Direction Classifier â€” Streamlit Demo (NB15)\")\n",
    "\n",
    "df = load_df(ROOT)\n",
    "feature_list, scaler, model, tau_art = load_artifacts(ROOT)\n",
    "\n",
    "with st.sidebar:\n",
    "    st.header(\"Settings\")\n",
    "    if \"date\" in df.columns:\n",
    "        dmin, dmax = df[\"date\"].min(), df[\"date\"].max()\n",
    "        start, end = st.date_input(\n",
    "            \"Date range\",\n",
    "            value=(dmin.date(), dmax.date()),\n",
    "            min_value=dmin.date(),\n",
    "            max_value=dmax.date()\n",
    "        )\n",
    "        mask = df[\"date\"].dt.date.between(start, end)\n",
    "        df_view = df.loc[mask].copy()\n",
    "    else:\n",
    "        df_view = df.copy()\n",
    "        st.caption(\"No 'date' column found; using all rows.\")\n",
    "\n",
    "    default_tau = float(tau_art) if tau_art is not None else 0.59\n",
    "    tau = st.slider(\"Decision threshold (Ï„)\", 0.00, 1.00, value=float(round(default_tau, 2)), step=0.01)\n",
    "    fee_bps = st.number_input(\"Fee (bps) per position flip)\", value=5, min_value=0, max_value=100, step=1)\n",
    "    st.caption(\"A flip is any change in position (enter/exit). Fee applied per flip.\")\n",
    "\n",
    "try:\n",
    "    X, y, retn, idx, y_name = make_dataset(df_view, feature_list)\n",
    "except Exception as e:\n",
    "    st.error(str(e)); st.stop()\n",
    "if X is None:\n",
    "    st.error(\"Could not infer a binary target; ensure your df_nb02 has one of the supported label columns or ret_next/close to derive one.\")\n",
    "    st.stop()\n",
    "\n",
    "Xs = scaler.transform(X)\n",
    "proba = predict_proba(model, Xs)\n",
    "proba = np.clip(proba, 1e-6, 1 - 1e-6)\n",
    "\n",
    "# Metrics\n",
    "import numpy as np\n",
    "metrics_col1, metrics_col2, metrics_col3, metrics_col4 = st.columns(4)\n",
    "try:   auc = roc_auc_score(y, proba)\n",
    "except Exception: auc = float(\"nan\")\n",
    "try:   ap = average_precision_score(y, proba)\n",
    "except Exception: ap = float(\"nan\")\n",
    "try:   brier = brier_score_loss(y, proba)\n",
    "except Exception: brier = float(\"nan\")\n",
    "try:   ll = log_loss(y, proba)\n",
    "except Exception: ll = float(\"nan\")\n",
    "\n",
    "metrics_col1.metric(\"ROC AUC\", f\"{auc:.3f}\" if np.isfinite(auc) else \"n/a\")\n",
    "metrics_col2.metric(\"Average Precision (PR AUC)\", f\"{ap:.3f}\" if np.isfinite(ap) else \"n/a\")\n",
    "metrics_col3.metric(\"Brier Score\", f\"{brier:.4f}\" if np.isfinite(brier) else \"n/a\")\n",
    "metrics_col4.metric(\"Log Loss\", f\"{ll:.4f}\" if np.isfinite(ll) else \"n/a\")\n",
    "\n",
    "# Strategy equity vs Buy&Hold\n",
    "sig = (proba >= tau).astype(int)\n",
    "flips = np.zeros_like(sig)\n",
    "if len(flips) > 1:\n",
    "    flips[1:] = (sig[1:] != sig[:-1]).astype(int)\n",
    "fee = flips * (fee_bps / 10000.0)\n",
    "strategy_ret = (retn * sig) - fee\n",
    "\n",
    "eq_strategy = np.cumprod(1.0 + strategy_ret)\n",
    "eq_bh = np.cumprod(1.0 + retn)\n",
    "\n",
    "# Dates for x-axis\n",
    "if \"date\" in df_view.columns:\n",
    "    dates = df_view.iloc[idx][\"date\"].values\n",
    "else:\n",
    "    dates = df_view.index.values\n",
    "\n",
    "st.subheader(\"Equity Curve vs. Buy & Hold\")\n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.plot(dates, eq_bh, label=\"Buy & Hold\")\n",
    "ax1.plot(dates, eq_strategy, label=f\"Strategy (Ï„={tau:.2f}, fee={fee_bps}bps)\")\n",
    "ax1.set_xlabel(\"Date\" if \"date\" in df_view.columns else \"Index\")\n",
    "ax1.set_ylabel(\"Equity (Ã—)\")\n",
    "ax1.legend()\n",
    "st.pyplot(fig1)\n",
    "\n",
    "st.subheader(\"ROC Curve\")\n",
    "fpr, tpr, _ = roc_curve(y, proba)\n",
    "fig2, ax2 = plt.subplots()\n",
    "ax2.plot(fpr, tpr, label=f\"AUC={auc:.3f}\" if np.isfinite(auc) else \"AUC=n/a\")\n",
    "ax2.plot([0, 1], [0, 1], linestyle=\"--\")\n",
    "ax2.set_xlabel(\"FPR\"); ax2.set_ylabel(\"TPR\"); ax2.legend()\n",
    "st.pyplot(fig2)\n",
    "\n",
    "st.subheader(\"Precisionâ€“Recall Curve\")\n",
    "prec, rec, _ = precision_recall_curve(y, proba)\n",
    "fig3, ax3 = plt.subplots()\n",
    "ax3.plot(rec, prec, label=f\"AP={ap:.3f}\" if np.isfinite(ap) else \"AP=n/a\")\n",
    "ax3.set_xlabel(\"Recall\"); ax3.set_ylabel(\"Precision\"); ax3.legend()\n",
    "st.pyplot(fig3)\n",
    "\n",
    "st.subheader(\"Calibration\")\n",
    "prob_true, prob_pred = calibration_curve(y, proba, n_bins=10, strategy=\"uniform\")\n",
    "fig4, ax4 = plt.subplots()\n",
    "ax4.plot(prob_pred, prob_true, marker=\"o\", label=\"Model\")\n",
    "ax4.plot([0, 1], [0, 1], linestyle=\"--\")\n",
    "ax4.set_xlabel(\"Predicted probability\"); ax4.set_ylabel(\"Observed frequency\"); ax4.legend()\n",
    "st.pyplot(fig4)\n",
    "\n",
    "st.subheader(\"Latest predictions (tail)\")\n",
    "tail_n = min(12, len(proba))\n",
    "preview = {}\n",
    "if \"date\" in df_view.columns:\n",
    "    preview[\"date\"] = list(dates[-tail_n:])\n",
    "if \"close\" in df_view.columns:\n",
    "    preview[\"close\"] = list(df_view.iloc[idx][\"close\"].values[-tail_n:])\n",
    "preview[\"proba\"] = list(proba[-tail_n:])\n",
    "preview[\"signal\"] = list(sig[-tail_n:])\n",
    "st.dataframe(pd.DataFrame(preview))\n",
    "st.caption(\"Signals are long-only (1=long, 0=cash); flips incur fee in equity curve.\")\n",
    "'''\n",
    "\n",
    "out_path = (APP_DIR / \"streamlit_app.py\")\n",
    "out_path.write_text(APP_CODE, encoding=\"utf-8\")\n",
    "print(\"Wrote:\", out_path.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9e7636c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== NB15 PRESENCE CHECK ===\n",
      "app/streamlit_app.py                         : True\n",
      "artifacts/feature_list.json                  : True\n",
      "artifacts/lr.joblib                          : True\n",
      "artifacts/scaler.joblib                      : True\n",
      "artifacts/threshold.json (optional)          : True\n",
      "data/df_nb02.csv or data/df_nb02.parquet     : True\n"
     ]
    }
   ],
   "source": [
    "# NB15 â€” Presence check (root-aware) for app + artifacts + data\n",
    "from pathlib import Path\n",
    "\n",
    "checks = {\n",
    "    \"app/streamlit_app.py\": (ROOT/\"app/streamlit_app.py\").exists(),\n",
    "    \"artifacts/feature_list.json\": (ROOT/\"artifacts/feature_list.json\").exists(),\n",
    "    \"artifacts/lr.joblib\": (ROOT/\"artifacts/lr.joblib\").exists(),\n",
    "    \"artifacts/scaler.joblib\": (ROOT/\"artifacts/scaler.joblib\").exists(),\n",
    "    \"artifacts/threshold.json (optional)\": (ROOT/\"artifacts/threshold.json\").exists(),\n",
    "    \"data/df_nb02.csv or data/df_nb02.parquet\": (ROOT/\"data/df_nb02.csv\").exists() or (ROOT/\"data/df_nb02.parquet\").exists(),\n",
    "}\n",
    "print(\"=== NB15 PRESENCE CHECK ===\")\n",
    "for k, v in checks.items():\n",
    "    print(f\"{k:45s}: {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fd4a40eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns (first 30): ['date', 'open', 'high', 'low', 'close', 'volume', 'ret1', 'ret5', 'ret10', 'vol10', 'volz', 'rsi14', 'macd', 'macd_signal', 'ticker', 'spy_close', 'vix_close', 'mkt_ret1', 'mkt_ret5', 'vix_chg1', 'ret_next', 'y']\n",
      "Shape: (2686, 22)\n",
      "Tail:\n",
      "            date        open        high         low       close      volume  \\\n",
      "2683  2025-10-08  256.519989  258.519989  256.109985  258.059998  36496900.0   \n",
      "2684  2025-10-09  257.809998  258.000000  253.139999  254.039993  38322000.0   \n",
      "2685  2025-10-10  254.940002  256.380005  244.000000  245.270004  61782400.0   \n",
      "\n",
      "          ret1      ret5     ret10     vol10  ...      macd  macd_signal  \\\n",
      "2683  0.006160  0.010217  0.022789  0.006727  ...  6.812987     6.977929   \n",
      "2684 -0.015578 -0.012017 -0.011017  0.006393  ...  6.293902     6.841124   \n",
      "2685 -0.034522 -0.049415 -0.039889  0.011929  ...  5.115887     6.496076   \n",
      "\n",
      "      ticker   spy_close  vix_close  mkt_ret1  mkt_ret5  vix_chg1  ret_next  y  \n",
      "2683    AAPL  673.109985  16.299999  0.005963  0.006971 -0.054524 -0.015578  0  \n",
      "2684    AAPL  671.159973  16.430000 -0.002897  0.002899  0.007976 -0.034522  0  \n",
      "2685    AAPL  653.020020  21.660000 -0.027028 -0.024193  0.318320  0.014718  1  \n",
      "\n",
      "[3 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "# NB15 â€” Data peek (columns + shape + tail)\n",
    "import pandas as pd\n",
    "\n",
    "df = None\n",
    "if (ROOT/\"data/df_nb02.csv\").exists():\n",
    "    df = pd.read_csv(ROOT/\"data/df_nb02.csv\")\n",
    "elif (ROOT/\"data/df_nb02.parquet\").exists():\n",
    "    df = pd.read_parquet(ROOT/\"data/df_nb02.parquet\")\n",
    "\n",
    "if df is None:\n",
    "    print(\"Place df_nb02.csv or df_nb02.parquet in data/.\")\n",
    "else:\n",
    "    print(\"Columns (first 30):\", list(df.columns)[:30])\n",
    "    print(\"Shape:\", df.shape)\n",
    "    print(\"Tail:\")\n",
    "    print(df.tail(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b6453f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows used: 2686 | features: 16 | tau: 0.59\n"
     ]
    }
   ],
   "source": [
    "# NB15 â€” Smoke test (load artifacts + compute probabilities)\n",
    "import json, numpy as np, pandas as pd, joblib\n",
    "\n",
    "# Load data\n",
    "if (ROOT/\"data/df_nb02.csv\").exists():\n",
    "    df = pd.read_csv(ROOT/\"data/df_nb02.csv\")\n",
    "elif (ROOT/\"data/df_nb02.parquet\").exists():\n",
    "    df = pd.read_parquet(ROOT/\"data/df_nb02.parquet\")\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Missing df_nb02.* in {ROOT/'data'}\")\n",
    "\n",
    "# Infer target\n",
    "if \"y\" in df.columns:\n",
    "    y = df[\"y\"].astype(int).clip(0,1).values\n",
    "elif \"ret_next\" in df.columns:\n",
    "    y = (df[\"ret_next\"].astype(float) > 0).astype(int).values\n",
    "elif \"close\" in df.columns:\n",
    "    rn = df[\"close\"].astype(float).pct_change().shift(-1).fillna(0.0)\n",
    "    df[\"ret_next\"] = rn\n",
    "    y = (rn > 0).astype(int).values\n",
    "else:\n",
    "    raise ValueError(\"No target inferable (need y/ret_next/close).\")\n",
    "\n",
    "retn = df[\"ret_next\"].astype(float).values if \"ret_next\" in df.columns else np.zeros(len(df))\n",
    "\n",
    "# Artifacts\n",
    "feature_list = json.load(open(ROOT/\"artifacts/feature_list.json\",\"r\",encoding=\"utf-8\"))\n",
    "scaler = joblib.load(ROOT/\"artifacts/scaler.joblib\")\n",
    "model  = joblib.load(ROOT/\"artifacts/lr.joblib\")\n",
    "tau = 0.59\n",
    "tfile = ROOT/\"artifacts/threshold.json\"\n",
    "if tfile.exists():\n",
    "    t = json.load(open(tfile,\"r\",encoding=\"utf-8\"))\n",
    "    tau = float(t.get(\"tau\") or t.get(\"threshold\") or t.get(\"value\") or 0.59)\n",
    "\n",
    "# Align features\n",
    "cols = [c for c in feature_list if c in df.columns]\n",
    "if not cols:\n",
    "    raise ValueError(\"No overlap between feature_list.json and df columns.\")\n",
    "X = df[cols].replace([np.inf, -np.inf], np.nan).dropna()\n",
    "mask = X.index\n",
    "y = np.asarray(y)[mask]\n",
    "retn = np.asarray(retn)[mask]\n",
    "\n",
    "# Probabilities\n",
    "Xs = scaler.transform(X.values)\n",
    "if hasattr(model, \"predict_proba\"):\n",
    "    p = model.predict_proba(Xs); p = p[:,1] if p.ndim==2 else p\n",
    "elif hasattr(model, \"decision_function\"):\n",
    "    s = model.decision_function(Xs); p = 1/(1+np.exp(-s))\n",
    "else:\n",
    "    p = np.clip(model.predict(Xs).astype(float), 0, 1)\n",
    "p = np.clip(p, 1e-6, 1-1e-6)\n",
    "\n",
    "print(\"Rows used:\", len(p), \"| features:\", len(cols), \"| tau:\", tau)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "06045fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC: 0.5214842239444841\n",
      "PR AUC : 0.5499732888657342\n",
      "Brier  : 0.25037234088911753\n",
      "LogLoss: 0.6938680024259393\n"
     ]
    }
   ],
   "source": [
    "# NB15 â€” Metrics (AUC, AP, Brier, LogLoss)\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, brier_score_loss, log_loss\n",
    "import numpy as np\n",
    "\n",
    "def safe(fn, *a):\n",
    "    try: return fn(*a)\n",
    "    except Exception: return float(\"nan\")\n",
    "\n",
    "print(\"ROC AUC:\", safe(roc_auc_score, y, p))\n",
    "print(\"PR AUC :\", safe(average_precision_score, y, p))\n",
    "print(\"Brier  :\", safe(brier_score_loss, y, p))\n",
    "print(\"LogLoss:\", safe(log_loss, y, p))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2f8deff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final equity â€” Strategy: 1.217781699144111 | Buy&Hold: 9.393293184865605\n",
      "Recent preview (p, sig): [(np.float64(0.537), np.int64(0)), (np.float64(0.542), np.int64(0)), (np.float64(0.529), np.int64(0)), (np.float64(0.562), np.int64(0)), (np.float64(0.565), np.int64(0))]\n"
     ]
    }
   ],
   "source": [
    "# NB15 â€” Equity calc (long-only; fee_bps=5)\n",
    "fee_bps = 5\n",
    "sig = (p >= tau).astype(int)\n",
    "flips = np.zeros_like(sig); \n",
    "if len(flips) > 1:\n",
    "    flips[1:] = (sig[1:] != sig[:-1]).astype(int)\n",
    "fee = flips * (fee_bps/10000.0)\n",
    "strategy_ret = (retn * sig) - fee\n",
    "\n",
    "eq  = np.cumprod(1 + strategy_ret)\n",
    "bh  = np.cumprod(1 + retn)\n",
    "\n",
    "print(\"Final equity â€” Strategy:\", float(eq[-1]), \"| Buy&Hold:\", float(bh[-1]))\n",
    "print(\"Recent preview (p, sig):\", list(zip(np.round(p[-5:],3), sig[-5:])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "95761eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From REPO ROOT, run:\n",
      "  streamlit run app/streamlit_app.py\n",
      "\n",
      "Optional: choose a port\n",
      "  streamlit run app/streamlit_app.py --server.port=8501\n"
     ]
    }
   ],
   "source": [
    "# NB15 â€” How to launch the Streamlit app\n",
    "print(\"From REPO ROOT, run:\")\n",
    "print(\"  streamlit run app/streamlit_app.py\")\n",
    "print(\"\\nOptional: choose a port\")\n",
    "print(\"  streamlit run app/streamlit_app.py --server.port=8501\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
