{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4208127d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel Python: c:\\.projects\\stock-direction-ml\\.venv\\Scripts\\python.exe\n",
      "Pandas: 2.3.2\n",
      "pyarrow import failed: ModuleNotFoundError(\"No module named 'pyarrow'\")\n"
     ]
    }
   ],
   "source": [
    "import sys, pandas as pd\n",
    "print(\"Kernel Python:\", sys.executable)\n",
    "print(\"Pandas:\", pd.__version__)\n",
    "try:\n",
    "    import pyarrow as pa, pyarrow.parquet as pq\n",
    "    print(\"pyarrow:\", pa.__version__)\n",
    "except Exception as e:\n",
    "    print(\"pyarrow import failed:\", repr(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "157bf761",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from importlib import reload\n",
    "\n",
    "# make repo root importable (this notebook lives in notebooks/)\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "# local modules (no market/news here)\n",
    "from src import data as data_mod, features as features_mod, utils as utils_mod\n",
    "reload(data_mod); reload(features_mod); reload(utils_mod)\n",
    "\n",
    "from src.data import get_data\n",
    "from src.features import add_features\n",
    "from src.utils import make_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e72ea0ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AAPL 2015-01-01 2023-12-31 0.001 True\n"
     ]
    }
   ],
   "source": [
    "TICKER = \"AAPL\"\n",
    "START, END = \"2015-01-01\", \"2023-12-31\"\n",
    "\n",
    "# labeling\n",
    "TAU = 0.001\n",
    "DEAD_ZONE = True\n",
    "\n",
    "print(TICKER, START, END, TAU, DEAD_ZONE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "110d5fb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2078,\n",
       " 8,\n",
       " ['ret1', 'ret5', 'ret10', 'vol10', 'volz', 'rsi14', 'macd', 'macd_signal'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = get_data(TICKER, start=START, end=END)\n",
    "df = add_features(df)\n",
    "df = make_labels(df, tau=TAU, dead_zone=DEAD_ZONE)\n",
    "\n",
    "# Clean & order\n",
    "if \"date\" in df.columns:\n",
    "    df = df.dropna(subset=[\"date\"]).sort_values(\"date\").reset_index(drop=True)\n",
    "\n",
    "# Exclude non-feature columns\n",
    "exclude = {\"date\",\"open\",\"high\",\"low\",\"close\",\"volume\",\"ret_next\",\"y\"}\n",
    "feat_cols = [c for c in df.columns if c not in exclude]\n",
    "\n",
    "len(df), len(feat_cols), feat_cols[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "202fe4cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1454, 312, 312)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 70% train, 15% val, 15% test (time-ordered split)\n",
    "n = len(df)\n",
    "i1 = int(0.70 * n)\n",
    "i2 = int(0.85 * n)\n",
    "\n",
    "df_tr = df.iloc[:i1].copy()\n",
    "df_va = df.iloc[i1:i2].copy()\n",
    "df_te = df.iloc[i2:].copy()\n",
    "\n",
    "Xtr, ytr = df_tr[feat_cols].values, df_tr[\"y\"].values\n",
    "Xva, yva = df_va[feat_cols].values, df_va[\"y\"].values\n",
    "Xte, yte = df_te[feat_cols].values, df_te[\"y\"].values\n",
    "\n",
    "len(df_tr), len(df_va), len(df_te)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7a09969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC (VAL) — LR: 0.4847037509778893\n",
      "AUC (TEST) — LR: 0.4734255626677679\n",
      "AUC (VAL) — XGB: 0.5282661506155556\n",
      "AUC (TEST) — XGB: 0.47003923188106544\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "\n",
    "# scaler + LR\n",
    "scaler = StandardScaler().fit(Xtr)\n",
    "Xtr_s, Xva_s, Xte_s = scaler.transform(Xtr), scaler.transform(Xva), scaler.transform(Xte)\n",
    "\n",
    "lr = LogisticRegression(max_iter=2000)\n",
    "lr.fit(Xtr_s, ytr)\n",
    "p_va_lr = lr.predict_proba(Xva_s)[:,1]\n",
    "p_te_lr = lr.predict_proba(Xte_s)[:,1]\n",
    "\n",
    "print(\"AUC (VAL) — LR:\", roc_auc_score(yva, p_va_lr))\n",
    "print(\"AUC (TEST) — LR:\", roc_auc_score(yte, p_te_lr))\n",
    "\n",
    "# optional XGB (skip if xgboost not installed)\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    xgb = XGBClassifier(\n",
    "        n_estimators=300, max_depth=3, subsample=0.9, colsample_bytree=0.9,\n",
    "        learning_rate=0.05, random_state=42, n_jobs=0, reg_lambda=1.0\n",
    "    )\n",
    "    xgb.fit(Xtr, ytr)\n",
    "    p_va_xgb = xgb.predict_proba(Xva)[:,1]\n",
    "    p_te_xgb = xgb.predict_proba(Xte)[:,1]\n",
    "    print(\"AUC (VAL) — XGB:\", roc_auc_score(yva, p_va_xgb))\n",
    "    print(\"AUC (TEST) — XGB:\", roc_auc_score(yte, p_te_xgb))\n",
    "except Exception as e:\n",
    "    print(\"XGBoost not available:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7933ae0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet save failed: Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.\n",
      "A suitable version of pyarrow or fastparquet is required for parquet support.\n",
      "Trying to import the above resulted in these errors:\n",
      " - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n",
      " - Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet.\n",
      "Saved CSV -> data/df_nb02.csv\n",
      "Wrote: artifacts/feature_list.json and data/storage_format.json\n"
     ]
    }
   ],
   "source": [
    "import json, os\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "os.makedirs(\"artifacts\", exist_ok=True)\n",
    "\n",
    "storage_meta = {\"format\": None}\n",
    "\n",
    "# Try Parquet first; fall back to CSV if engine missing\n",
    "try:\n",
    "    df.to_parquet(\"data/df_nb02.parquet\", index=False)\n",
    "    storage_meta[\"format\"] = \"parquet\"\n",
    "    print(\"Saved Parquet -> data/df_nb02.parquet\")\n",
    "except Exception as e:\n",
    "    print(\"Parquet save failed:\", e)\n",
    "    df.to_csv(\"data/df_nb02.csv\", index=False)\n",
    "    storage_meta[\"format\"] = \"csv\"\n",
    "    print(\"Saved CSV -> data/df_nb02.csv\")\n",
    "\n",
    "# Save features + storage format\n",
    "with open(\"artifacts/feature_list.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(sorted(feat_cols), f, indent=2)\n",
    "with open(\"data/storage_format.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(storage_meta, f)\n",
    "\n",
    "print(\"Wrote: artifacts/feature_list.json and data/storage_format.json\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.13 (.venv stock-direction-ml)",
   "language": "python",
   "name": "stock-direction-ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
