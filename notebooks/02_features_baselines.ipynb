{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b7872a2",
   "metadata": {},
   "source": [
    "# Phase 2 — Data Build (modern snapshot)\n",
    "Fetch OHLCV, engineer baseline features, and write `data/df_nb02.csv`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04295189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Imports & params ---\n",
    "import warnings, json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    import yfinance as yf\n",
    "except Exception:\n",
    "    yf = None\n",
    "\n",
    "DATA = Path(\"data\"); ART = Path(\"artifacts\"); FIG = Path(\"reports/figures\")\n",
    "for p in (DATA, ART, FIG): p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "TICKER = \"AAPL\"\n",
    "START  = \"2015-01-01\"\n",
    "END    = (pd.Timestamp.now(tz=\"America/Los_Angeles\") + pd.Timedelta(days=1)).date().isoformat()\n",
    "USE_MARKET = True\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"Build params →\", {\"TICKER\":TICKER, \"START\":START, \"END\":END, \"USE_MARKET\":USE_MARKET})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463e7763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run once if the import failed\n",
    "%pip install yfinance\n",
    "import yfinance as yf\n",
    "print(\"yfinance\", yf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d520c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Feature helpers (no external TA libs) ---\n",
    "def rsi(series: pd.Series, window: int = 14) -> pd.Series:\n",
    "    s = series.astype(float)\n",
    "    delta = s.diff()\n",
    "    up = delta.clip(lower=0); down = -delta.clip(upper=0)\n",
    "    roll_up = up.ewm(alpha=1/window, adjust=False).mean()\n",
    "    roll_down = down.ewm(alpha=1/window, adjust=False).mean()\n",
    "    rs = roll_up / roll_down.replace(0, np.nan)\n",
    "    return 100 - (100 / (1 + rs))\n",
    "\n",
    "def macd(series: pd.Series, fast: int = 12, slow: int = 26, signal: int = 9):\n",
    "    s = series.astype(float)\n",
    "    ema_fast = s.ewm(span=fast, adjust=False).mean()\n",
    "    ema_slow = s.ewm(span=slow, adjust=False).mean()\n",
    "    macd_line = ema_fast - ema_slow\n",
    "    macd_sig  = macd_line.ewm(span=signal, adjust=False).mean()\n",
    "    return macd_line, macd_sig\n",
    "\n",
    "def zscore(s: pd.Series, win: int = 20) -> pd.Series:\n",
    "    m = s.rolling(win, min_periods=win).mean()\n",
    "    v = s.rolling(win, min_periods=win).std(ddof=0)\n",
    "    return (s - m) / v.replace(0, np.nan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2319676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Download base OHLCV for the main ticker ---\n",
    "if yf is None:\n",
    "    raise ImportError(\"Please `pip install yfinance` to run this notebook.\")\n",
    "\n",
    "raw = yf.download(TICKER, start=START, end=END, auto_adjust=True, progress=False)\n",
    "if raw is None or raw.empty:\n",
    "    raise ValueError(f\"No price data for {TICKER} in {START}..{END}\")\n",
    "\n",
    "# Normalize index to tz-naive dates\n",
    "idx = pd.to_datetime(raw.index, errors=\"coerce\")\n",
    "try:\n",
    "    if getattr(idx, \"tz\", None) is not None:\n",
    "        idx = idx.tz_localize(None)\n",
    "except Exception:\n",
    "    idx = pd.to_datetime(idx, errors=\"coerce\").tz_localize(None)\n",
    "\n",
    "px = raw.copy()\n",
    "px.index = idx\n",
    "px = px.sort_index()\n",
    "px = px[~px.index.duplicated(keep=\"last\")]\n",
    "print(px.shape, \"rows →\", px.index.min().date(), \"→\", px.index.max().date())\n",
    "px.tail(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c11e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Normalize yfinance columns to 1-D Series (handles MultiIndex) ---\n",
    "def get_price_series(df, field: str, ticker: str = None):\n",
    "    if isinstance(df.columns, pd.MultiIndex):\n",
    "        s = None\n",
    "        try: s = df.xs(field, axis=1, level=0)\n",
    "        except Exception: pass\n",
    "        if s is None or isinstance(s, pd.DataFrame):\n",
    "            try: s = df.xs(field, axis=1, level=1)\n",
    "            except Exception: pass\n",
    "        if isinstance(s, pd.DataFrame):\n",
    "            if ticker is not None and ticker in s.columns: s = s[ticker]\n",
    "            else: s = s.iloc[:, 0]\n",
    "    else:\n",
    "        s = df[field]\n",
    "    return pd.to_numeric(s, errors=\"coerce\")\n",
    "\n",
    "open_s   = get_price_series(px, \"Open\",   TICKER)\n",
    "high_s   = get_price_series(px, \"High\",   TICKER)\n",
    "low_s    = get_price_series(px, \"Low\",    TICKER)\n",
    "close_s  = get_price_series(px, \"Close\",  TICKER)\n",
    "volume_s = get_price_series(px, \"Volume\", TICKER)\n",
    "\n",
    "px_clean = pd.DataFrame({\n",
    "    \"Open\":   open_s.astype(float),\n",
    "    \"High\":   high_s.astype(float),\n",
    "    \"Low\":    low_s.astype(float),\n",
    "    \"Close\":  close_s.astype(float),\n",
    "    \"Volume\": volume_s.astype(float),\n",
    "}, index=px.index).sort_index().dropna()\n",
    "\n",
    "print(\"px_clean:\", px_clean.shape, \"rows →\", px_clean.index.min().date(), \"→\", px_clean.index.max().date())\n",
    "px_clean.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bea64a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Feature engineering (returns, vol, RSI, MACD, volume stats) + label ---\n",
    "df = pd.DataFrame({\n",
    "    \"date\":   px_clean.index,\n",
    "    \"open\":   px_clean[\"Open\"].values,\n",
    "    \"high\":   px_clean[\"High\"].values,\n",
    "    \"low\":    px_clean[\"Low\"].values,\n",
    "    \"close\":  px_clean[\"Close\"].values,\n",
    "    \"volume\": px_clean[\"Volume\"].values,\n",
    "})\n",
    "\n",
    "# Simple returns\n",
    "df[\"ret1\"]  = df[\"close\"].pct_change()\n",
    "df[\"ret5\"]  = df[\"close\"].pct_change(5)\n",
    "df[\"ret10\"] = df[\"close\"].pct_change(10)\n",
    "\n",
    "# Rolling volatility (10d)\n",
    "df[\"vol10\"] = df[\"ret1\"].rolling(10, min_periods=10).std(ddof=0)\n",
    "\n",
    "# Volume z-score (20d)\n",
    "df[\"volz\"] = zscore(df[\"volume\"], win=20)\n",
    "\n",
    "# RSI(14) and MACD(12,26,9)\n",
    "df[\"rsi14\"] = rsi(df[\"close\"], window=14)\n",
    "df[\"macd\"], df[\"macd_signal\"] = macd(df[\"close\"], fast=12, slow=26, signal=9)\n",
    "\n",
    "# Drop warm-up NaNs & tag ticker\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "df[\"ticker\"] = TICKER\n",
    "\n",
    "# === Label (next-day direction) ===\n",
    "df[\"ret_next\"] = df[\"ret1\"].shift(-1)\n",
    "df = df.dropna(subset=[\"ret_next\"]).copy()  # drop last unknown next-day return\n",
    "df[\"y\"] = (df[\"ret_next\"] > 0).astype(int)\n",
    "\n",
    "print(\"Engineered:\", df.shape, \"| first:\", df['date'].min().date(), \"| last:\", df['date'].max().date(), \"| y rate:\", round(float(df['y'].mean()),3))\n",
    "df.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6fd3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === NB2 VERIFY (robust) ===\n",
    "from pathlib import Path\n",
    "import json, pandas as pd, numpy as np\n",
    "\n",
    "DATA = Path(\"data\"); ART = Path(\"artifacts\")\n",
    "csv_path = DATA / \"df_nb02.csv\"\n",
    "parq_path = DATA / \"df_nb02.parquet\"\n",
    "lp_path = DATA / \"label_params.json\"\n",
    "feat_path = ART / \"feature_list.json\"\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(csv_path, parse_dates=[\"date\"]).sort_values(\"date\").reset_index(drop=True)\n",
    "print(\"NB2 dataset:\", df.shape, \"|\", df[\"date\"].min().date(), \"→\", df[\"date\"].max().date())\n",
    "print(\"NaNs total:\", int(df.isna().sum().sum()), \"| duplicate dates:\", int(df[\"date\"].duplicated().sum()))\n",
    "\n",
    "# Label check: y should equal sign of NEXT-day ret1\n",
    "label_col = next((c for c in [\"y\",\"target\",\"label\",\"y_next_up\"] if c in df.columns), None)\n",
    "ret_next = df[\"ret1\"].shift(-1)\n",
    "mask = ret_next.notna()\n",
    "y_from_ret = (ret_next[mask] > 0).astype(int).values\n",
    "y_true = df.loc[mask, label_col].astype(int).values if label_col else None\n",
    "match = (y_true == y_from_ret).mean() if y_true is not None else np.nan\n",
    "print(\"Label matches next-day(ret1>0):\", None if np.isnan(match) else round(float(match), 3))\n",
    "\n",
    "# Feature list sanity (no leaks)\n",
    "feats = json.load(open(feat_path, \"r\", encoding=\"utf-8\"))\n",
    "bad = set(feats) & {\"y\",\"ret_next\",\"date\",\"ticker\",\"spy_close\",\"vix_close\"}\n",
    "print(\"Features count:\", len(feats))\n",
    "print(\"Leaky/bad cols inside features:\", bad)\n",
    "\n",
    "# Files present?\n",
    "print(\"Files exist →\",\n",
    "      \"df_nb02.csv:\", csv_path.exists(),\n",
    "      \"| parquet:\", parq_path.exists(),\n",
    "      \"| label_params.json:\", lp_path.exists(),\n",
    "      \"| feature_list.json:\", feat_path.exists())\n",
    "\n",
    "# Quick preview\n",
    "print(\"\\nPreview:\")\n",
    "keep = [\"date\"] + feats[:5] + ([label_col] if label_col else [])\n",
    "print(df[keep].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31598236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Save outputs (robust) ---\n",
    "out_csv = DATA / \"df_nb02.csv\"\n",
    "df.to_csv(out_csv, index=False)\n",
    "print(\"Saved CSV:\", out_csv, \"| bytes:\", out_csv.stat().st_size)\n",
    "\n",
    "def _has_fastparquet():\n",
    "    try:\n",
    "        import fastparquet  # noqa: F401\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def _sanitize_periods(df_: pd.DataFrame) -> pd.DataFrame:\n",
    "    for c in df_.columns:\n",
    "        if pd.api.types.is_period_dtype(df_[c]):\n",
    "            df_[c] = df_[c].astype(str)\n",
    "    return df_\n",
    "\n",
    "out_parq = DATA / \"df_nb02.parquet\"\n",
    "df_parq = _sanitize_periods(df.copy())\n",
    "saved_parquet = False\n",
    "try:\n",
    "    import pyarrow as pa  # noqa: F401\n",
    "    try:\n",
    "        pa.unregister_extension_type(\"pandas.period\")\n",
    "    except Exception:\n",
    "        pass\n",
    "    df_parq.to_parquet(out_parq, index=False, engine=\"pyarrow\")\n",
    "    saved_parquet = True\n",
    "    print(\"Saved Parquet (pyarrow):\", out_parq, \"| bytes:\", out_parq.stat().st_size)\n",
    "except Exception as e:\n",
    "    print(\"pyarrow failed →\", e)\n",
    "    if _has_fastparquet():\n",
    "        try:\n",
    "            df_parq.to_parquet(out_parq, index=False, engine=\"fastparquet\")\n",
    "            saved_parquet = True\n",
    "            print(\"Saved Parquet (fastparquet):\", out_parq, \"| bytes:\", out_parq.stat().st_size)\n",
    "        except Exception as e2:\n",
    "            print(\"fastparquet also failed →\", e2)\n",
    "\n",
    "if not saved_parquet:\n",
    "    print(\"Parquet save skipped (CSV written).\")\n",
    "\n",
    "# Quick QA\n",
    "dts = pd.to_datetime(df[\"date\"])\n",
    "print(\"Rows:\", len(df), \"| date span:\", dts.min().date(), \"→\", dts.max().date())\n",
    "print(\"Columns:\", list(df.columns))\n",
    "print(\"NaNs total:\", int(df.isna().sum().sum()))\n",
    "print(\"\\nret1 describe:\\n\", df[\"ret1\"].describe().to_string())\n",
    "\n",
    "# Record storage format\n",
    "meta_path = DATA / \"storage_format.json\"\n",
    "record = {\"path\": str(out_csv), \"format\": \"csv\"}\n",
    "try:\n",
    "    if meta_path.exists():\n",
    "        meta = json.load(open(meta_path, \"r\", encoding=\"utf-8\"))\n",
    "        if isinstance(meta, dict):\n",
    "            meta = [meta]\n",
    "    else:\n",
    "        meta = []\n",
    "    meta = [m for m in meta if m.get(\"path\") != record[\"path\"]] + [record]\n",
    "    json.dump(meta, open(meta_path, \"w\", encoding=\"utf-8\"), indent=2)\n",
    "except Exception:\n",
    "    json.dump([record], open(meta_path, \"w\", encoding=\"utf-8\"), indent=2)\n",
    "print(\"Updated:\", meta_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7b8cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === NB2 VERIFY (robust) ===\n",
    "from pathlib import Path\n",
    "import json, pandas as pd, numpy as np\n",
    "\n",
    "DATA = Path(\"data\"); ART = Path(\"artifacts\")\n",
    "csv_path = DATA / \"df_nb02.csv\"\n",
    "parq_path = DATA / \"df_nb02.parquet\"\n",
    "lp_path = DATA / \"label_params.json\"\n",
    "feat_path = ART / \"feature_list.json\"\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(csv_path, parse_dates=[\"date\"]).sort_values(\"date\").reset_index(drop=True)\n",
    "print(\"NB2 dataset:\", df.shape, \"|\", df[\"date\"].min().date(), \"→\", df[\"date\"].max().date())\n",
    "print(\"NaNs total:\", int(df.isna().sum().sum()), \"| duplicate dates:\", int(df[\"date\"].duplicated().sum()))\n",
    "\n",
    "# Label check: y should equal sign of NEXT-day ret1\n",
    "label_col = next((c for c in [\"y\",\"target\",\"label\",\"y_next_up\"] if c in df.columns), None)\n",
    "ret_next = df[\"ret1\"].shift(-1)\n",
    "mask = ret_next.notna()\n",
    "y_from_ret = (ret_next[mask] > 0).astype(int).values\n",
    "y_true = df.loc[mask, label_col].astype(int).values if label_col else None\n",
    "match = (y_true == y_from_ret).mean() if y_true is not None else np.nan\n",
    "print(\"Label matches next-day(ret1>0):\", None if np.isnan(match) else round(float(match), 3))\n",
    "\n",
    "# Feature list sanity (no leaks)\n",
    "feats = json.load(open(feat_path, \"r\", encoding=\"utf-8\"))\n",
    "bad = set(feats) & {\"y\",\"ret_next\",\"date\",\"ticker\",\"spy_close\",\"vix_close\"}\n",
    "print(\"Features count:\", len(feats))\n",
    "print(\"Leaky/bad cols inside features:\", bad)\n",
    "\n",
    "# Files present?\n",
    "print(\"Files exist →\",\n",
    "      \"df_nb02.csv:\", csv_path.exists(),\n",
    "      \"| parquet:\", parq_path.exists(),\n",
    "      \"| label_params.json:\", lp_path.exists(),\n",
    "      \"| feature_list.json:\", feat_path.exists())\n",
    "\n",
    "# Quick preview\n",
    "print(\"\\nPreview:\")\n",
    "keep = [\"date\"] + feats[:5] + ([label_col] if label_col else [])\n",
    "print(df[keep].head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
