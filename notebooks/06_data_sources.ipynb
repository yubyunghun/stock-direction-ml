{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209bb1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Imports & params ---\n",
    "import os, json, pathlib, warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Optional dep (market data)\n",
    "try:\n",
    "    import yfinance as yf\n",
    "except Exception:\n",
    "    yf = None\n",
    "\n",
    "# Optional deps (models/explainability)\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import roc_auc_score, brier_score_loss, roc_curve, precision_recall_curve, average_precision_score\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "try:\n",
    "    import joblib\n",
    "except Exception:\n",
    "    joblib = None\n",
    "\n",
    "try:\n",
    "    import shap\n",
    "except Exception:\n",
    "    shap = None\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Base I/O\n",
    "DATA_DIR = pathlib.Path(\"data\")\n",
    "ART_DIR  = pathlib.Path(\"artifacts\")\n",
    "FIG_DIR  = pathlib.Path(\"reports/figures\")\n",
    "for p in [DATA_DIR, ART_DIR, FIG_DIR]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Experiment switches\n",
    "USE_MARKET        = True     # SPY/VIX context\n",
    "USE_FUNDAMENTALS  = False    # placeholder\n",
    "USE_NEWS          = False    # placeholder\n",
    "\n",
    "# Run settings\n",
    "TICKER = \"AAPL\"\n",
    "START, END = \"2015-01-01\", \"2023-12-31\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d9989c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load the base features/labels from Phase 2 ---\n",
    "base_csv = DATA_DIR / \"df_nb02.csv\"\n",
    "if not base_csv.exists():\n",
    "    raise FileNotFoundError(\"Expected Phase-2 output at data/df_nb02.csv. Run notebook 02 first.\")\n",
    "\n",
    "df = pd.read_csv(base_csv)\n",
    "\n",
    "# Ensure tz-naive datetime\n",
    "if \"date\" not in df.columns:\n",
    "    raise KeyError(\"'date' column missing in df_nb02.csv\")\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\").dt.tz_localize(None)\n",
    "df = df.dropna(subset=[\"date\"]).reset_index(drop=True)\n",
    "\n",
    "print(df.shape)\n",
    "df.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d44bee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Market context via alignment (no merges) ---\n",
    "def fetch_close_series(ticker: str, start: str, end: str) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Return a pd.Series of adjusted Close with a tz-naive DatetimeIndex.\n",
    "    Works whether yfinance returns single-level or MultiIndex columns.\n",
    "    \"\"\"\n",
    "    if yf is None:\n",
    "        raise ImportError(\"Please `pip install yfinance` to enable USE_MARKET=True.\")\n",
    "\n",
    "    r = yf.download(ticker, start=start, end=end, auto_adjust=True, progress=False)\n",
    "    if r is None or r.empty:\n",
    "        raise ValueError(f\"No data for {ticker} in {start}..{end}\")\n",
    "\n",
    "    idx = pd.to_datetime(r.index, errors=\"coerce\")\n",
    "    try:\n",
    "        if getattr(idx, \"tz\", None) is not None:\n",
    "            idx = idx.tz_localize(None)\n",
    "    except Exception:\n",
    "        idx = pd.to_datetime(idx, errors=\"coerce\").tz_localize(None)\n",
    "\n",
    "    if isinstance(r.columns, pd.MultiIndex):\n",
    "        close = r.xs(\"Close\", axis=1, level=0)\n",
    "        if isinstance(close, pd.DataFrame):\n",
    "            close = close.iloc[:, 0]\n",
    "    else:\n",
    "        close = r[\"Close\"]\n",
    "\n",
    "    s = pd.Series(np.asarray(close).reshape(-1), index=idx, name=\"Close\")\n",
    "    s = s.sort_index()\n",
    "    s = s[~s.index.duplicated(keep=\"last\")]\n",
    "    return s\n",
    "\n",
    "if USE_MARKET:\n",
    "    dti = pd.to_datetime(df[\"date\"], errors=\"coerce\").dt.tz_localize(None)\n",
    "\n",
    "    spy = fetch_close_series(\"SPY\", START, END)\n",
    "    vix = fetch_close_series(\"^VIX\", START, END)\n",
    "\n",
    "    # Align by label (same-day)\n",
    "    df[\"spy_close\"] = dti.map(spy)\n",
    "    df[\"vix_close\"] = dti.map(vix)\n",
    "\n",
    "    # Context features\n",
    "    df[\"mkt_ret1\"] = df[\"spy_close\"].pct_change(1)\n",
    "    df[\"mkt_ret5\"] = df[\"spy_close\"].pct_change(5)\n",
    "    df[\"vix_chg1\"] = df[\"vix_close\"].pct_change(1)\n",
    "\n",
    "    # Keep rows with all context features present\n",
    "    df = df.dropna(subset=[\"spy_close\",\"vix_close\",\"mkt_ret1\",\"mkt_ret5\",\"vix_chg1\"]).reset_index(drop=True)\n",
    "\n",
    "print(df.shape)\n",
    "df.filter([\"date\",\"spy_close\",\"mkt_ret1\",\"mkt_ret5\",\"vix_close\",\"vix_chg1\"]).head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ee437a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Quality checks ---\n",
    "req = [\"spy_close\",\"vix_close\",\"mkt_ret1\",\"mkt_ret5\",\"vix_chg1\"]\n",
    "present = [c for c in req if c in df.columns]\n",
    "missing = [c for c in req if c not in df.columns]\n",
    "\n",
    "print(\"Shape:\", df.shape)\n",
    "print(\"Present:\", present)\n",
    "print(\"Missing:\", missing)\n",
    "\n",
    "if present:\n",
    "    print(\"\\nNulls in context cols:\")\n",
    "    print(df[present].isna().sum())\n",
    "\n",
    "    print(\"\\nReturn stats:\")\n",
    "    print(df[[\"mkt_ret1\",\"mkt_ret5\",\"vix_chg1\"]].describe().T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987c2a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Save experiment output (separate file so we don't touch Phase-2) ---\n",
    "out_name = \"df_nb06_market.csv\" if USE_MARKET else \"df_nb06_base.csv\"\n",
    "out_path = DATA_DIR / out_name\n",
    "df.to_csv(out_path, index=False)\n",
    "print(\"Saved:\", out_path)\n",
    "\n",
    "# --- Record storage format (quiet) ---\n",
    "meta_path = DATA_DIR / \"storage_format.json\"\n",
    "record = {\"path\": str(out_path), \"format\": \"csv\"}\n",
    "\n",
    "try:\n",
    "    if meta_path.exists():\n",
    "        meta = json.load(open(meta_path, \"r\", encoding=\"utf-8\"))\n",
    "        if isinstance(meta, dict):\n",
    "            meta = [meta]\n",
    "    else:\n",
    "        meta = []\n",
    "    meta = [m for m in meta if m.get(\"path\") != record[\"path\"]] + [record]\n",
    "    json.dump(meta, open(meta_path, \"w\", encoding=\"utf-8\"), indent=2)\n",
    "except Exception:\n",
    "    json.dump([record], open(meta_path, \"w\", encoding=\"utf-8\"), indent=2)\n",
    "\n",
    "print(\"Updated:\", meta_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc93bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load test split (preferred) or fallback to a simple date split ---\n",
    "test_path = DATA_DIR / \"test.parquet\"\n",
    "train_path = DATA_DIR / \"train.parquet\"\n",
    "\n",
    "if test_path.exists() and train_path.exists():\n",
    "    X_tr = pd.read_parquet(train_path)\n",
    "    X_te = pd.read_parquet(test_path)\n",
    "    # If labels are stored inside, try to peel them out; otherwise expect separate y columns\n",
    "    label_col = None\n",
    "    for cand in [\"y\",\"target\",\"label\",\"y_next_up\"]:\n",
    "        if cand in X_tr.columns:\n",
    "            label_col = cand\n",
    "            break\n",
    "    if label_col:\n",
    "        y_tr = X_tr.pop(label_col).astype(int).values\n",
    "        y_te = X_te.pop(label_col).astype(int).values\n",
    "    else:\n",
    "        # Try separate label files\n",
    "        y_train_path = DATA_DIR / \"y_train.parquet\"\n",
    "        y_test_path  = DATA_DIR / \"y_test.parquet\"\n",
    "        if y_train_path.exists() and y_test_path.exists():\n",
    "            y_tr = pd.read_parquet(y_train_path).iloc[:,0].astype(int).values\n",
    "            y_te = pd.read_parquet(y_test_path).iloc[:,0].astype(int).values\n",
    "        else:\n",
    "            raise RuntimeError(\"Could not find labels in train/test parquet or separate y files.\")\n",
    "else:\n",
    "    # Fallback: infer features from df, drop non-numeric and obvious non-features\n",
    "    df2 = df.copy()\n",
    "    # Identify label column heuristically\n",
    "    label_col = None\n",
    "    for cand in [\"y\",\"target\",\"label\",\"y_next_up\"]:\n",
    "        if cand in df2.columns:\n",
    "            label_col = cand\n",
    "            break\n",
    "    if not label_col:\n",
    "        raise RuntimeError(\"Label column not found. Expected one of ['y','target','label','y_next_up']\")\n",
    "\n",
    "    y_all = df2.pop(label_col).astype(int).values\n",
    "\n",
    "    drop_cols = {\"date\",\"ticker\",\"spy_close\",\"vix_close\"}  # keep engineered returns though\n",
    "    for c in list(drop_cols):\n",
    "        if c not in df2.columns:\n",
    "            drop_cols.discard(c)\n",
    "    df2 = df2.drop(columns=list(drop_cols), errors=\"ignore\")\n",
    "    # Numeric only\n",
    "    X_all = df2.select_dtypes(include=[np.number]).copy()\n",
    "\n",
    "    # Time split: last 20% as test\n",
    "    n = len(X_all)\n",
    "    cut = int(n*0.8)\n",
    "    X_tr, X_te = X_all.iloc[:cut].copy(), X_all.iloc[cut:].copy()\n",
    "    y_tr, y_te = y_all[:cut], y_all[cut:]\n",
    "\n",
    "print(X_tr.shape, X_te.shape, len(y_tr), len(y_te))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8007f8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load trained models from Phase 5 (if available) ---\n",
    "path_lr  = ART_DIR / \"model_logreg.pkl\"\n",
    "path_xgb = ART_DIR / \"model_xgb.pkl\"\n",
    "\n",
    "HAS_LR  = path_lr.exists() and joblib is not None\n",
    "HAS_XGB = path_xgb.exists() and joblib is not None\n",
    "\n",
    "mdl_lr  = joblib.load(path_lr)  if HAS_LR  else None\n",
    "mdl_xgb = joblib.load(path_xgb) if HAS_XGB else None\n",
    "\n",
    "print(\"HAS_LR:\", HAS_LR, \"| HAS_XGB:\", HAS_XGB)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e6edd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Predict on test set ---\n",
    "p_te_lr = mdl_lr.predict_proba(X_te)[:,1] if HAS_LR else None\n",
    "p_te_xgb = mdl_xgb.predict_proba(X_te)[:,1] if HAS_XGB else None\n",
    "\n",
    "rows = []\n",
    "if p_te_lr is not None:\n",
    "    rows.append({\n",
    "        \"model\": \"logreg\",\n",
    "        \"AUC_test\": float(roc_auc_score(y_te, p_te_lr)),\n",
    "        \"Brier_test\": float(brier_score_loss(y_te, p_te_lr))\n",
    "    })\n",
    "if p_te_xgb is not None:\n",
    "    rows.append({\n",
    "        \"model\": \"xgb\",\n",
    "        \"AUC_test\": float(roc_auc_score(y_te, p_te_xgb)),\n",
    "        \"Brier_test\": float(brier_score_loss(y_te, p_te_xgb))\n",
    "    })\n",
    "\n",
    "summary = pd.DataFrame(rows).round(4)\n",
    "display(summary)\n",
    "\n",
    "(DATA_DIR / \"explainability_summary.csv\").write_text(summary.to_csv(index=False))\n",
    "print(\"Saved:\", DATA_DIR / \"explainability_summary.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c59f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- ROC and PR curves ---\n",
    "def plot_and_save_curves(y, p, name):\n",
    "    # ROC\n",
    "    fpr, tpr, _ = roc_curve(y, p)\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.plot(fpr, tpr, label=f\"{name}\")\n",
    "    plt.plot([0,1],[0,1],\"--\")\n",
    "    plt.xlabel(\"FPR\")\n",
    "    plt.ylabel(\"TPR\")\n",
    "    plt.title(f\"ROC – {name}\")\n",
    "    plt.grid(True, alpha=.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(FIG_DIR / f\"roc_{name}.png\", dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "    # PR\n",
    "    prec, rec, _ = precision_recall_curve(y, p)\n",
    "    ap = average_precision_score(y, p)\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.plot(rec, prec, label=f\"AP={ap:.3f}\")\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.title(f\"Precision–Recall – {name}\")\n",
    "    plt.grid(True, alpha=.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(FIG_DIR / f\"pr_{name}.png\", dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "if p_te_lr is not None:  plot_and_save_curves(y_te, p_te_lr, \"logreg\")\n",
    "if p_te_xgb is not None: plot_and_save_curves(y_te, p_te_xgb, \"xgb\")\n",
    "\n",
    "print(\"Saved ROC/PR figures to\", FIG_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73149932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Calibration / reliability ---\n",
    "def reliability_summary(y_true, p, model_name, n_bins=10):\n",
    "    bins = pd.qcut(p, q=n_bins, labels=False, duplicates=\"drop\")\n",
    "    df_bin = pd.DataFrame({\"y\": y_true, \"p\": p, \"bin\": bins})\n",
    "    grp = df_bin.groupby(\"bin\", as_index=False).agg(\n",
    "        n=(\"y\",\"size\"),\n",
    "        avg_pred=(\"p\",\"mean\"),\n",
    "        avg_true=(\"y\",\"mean\")\n",
    "    )\n",
    "    grp.insert(0, \"model\", model_name)\n",
    "    return grp\n",
    "\n",
    "to_save = []\n",
    "for name, p in [(\"logreg\", p_te_lr), (\"xgb\", p_te_xgb)]:\n",
    "    if p is None: \n",
    "        continue\n",
    "    prob_true, prob_pred = calibration_curve(y_te, p, n_bins=10, strategy=\"quantile\")\n",
    "\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.plot(prob_pred, prob_true, marker=\"o\", label=f\"{name}\")\n",
    "    plt.plot([0,1],[0,1], linestyle=\"--\")\n",
    "    plt.xlabel(\"Predicted probability\")\n",
    "    plt.ylabel(\"Observed frequency\")\n",
    "    plt.title(f\"Calibration – {name}\")\n",
    "    plt.grid(True, alpha=.3)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(FIG_DIR / f\"calibration_{name}.png\", dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "    to_save.append(reliability_summary(y_te, p, name, n_bins=10))\n",
    "\n",
    "if to_save:\n",
    "    rel = pd.concat(to_save, ignore_index=True)\n",
    "    rel.to_csv(DATA_DIR / \"reliability_by_decile.csv\", index=False)\n",
    "    print(\"Saved calibration_*.png and reliability_by_decile.csv\")\n",
    "else:\n",
    "    print(\"No model probabilities found for calibration.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ce1852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Global importance ---\n",
    "saved_any = False\n",
    "\n",
    "if (shap is not None) and (mdl_xgb is not None) and hasattr(mdl_xgb, \"get_booster\"):\n",
    "    # XGBoost SHAP\n",
    "    expl = shap.TreeExplainer(mdl_xgb)\n",
    "    shap_vals = expl.shap_values(X_te)\n",
    "    # Mean |SHAP|\n",
    "    imp = pd.DataFrame({\n",
    "        \"feature\": X_te.columns,\n",
    "        \"mean_abs_shap\": np.mean(np.abs(shap_vals), axis=0)\n",
    "    }).sort_values(\"mean_abs_shap\", ascending=False)\n",
    "\n",
    "    imp.to_csv(DATA_DIR / \"shap_importance_top20.csv\", index=False)\n",
    "    plt.figure(figsize=(7,8))\n",
    "    shap.summary_plot(shap_vals, X_te, show=False)  # creates a beeswarm\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(FIG_DIR / \"shap_importance.png\", dpi=150, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    saved_any = True\n",
    "    print(\"Saved SHAP importance artifacts.\")\n",
    "\n",
    "# Logistic regression absolute coefficients as a fallback/extra\n",
    "if mdl_lr is not None and hasattr(mdl_lr, \"coef_\"):\n",
    "    coefs = pd.Series(mdl_lr.coef_.ravel(), index=X_te.columns).abs().sort_values(ascending=False)\n",
    "    coef_df = coefs.reset_index()\n",
    "    coef_df.columns = [\"feature\", \"abs_coef\"]\n",
    "    coef_df.to_csv(DATA_DIR / \"logreg_abscoef_top20.csv\", index=False)\n",
    "\n",
    "    plt.figure(figsize=(7,5))\n",
    "    top = coef_df.head(20).sort_values(\"abs_coef\")\n",
    "    plt.barh(top[\"feature\"], top[\"abs_coef\"])\n",
    "    plt.title(\"LogReg |coeff| top-20\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(FIG_DIR / \"logreg_coef_importance.png\", dpi=150)\n",
    "    plt.close()\n",
    "    saved_any = True\n",
    "    print(\"Saved logistic-regression coefficient importance.\")\n",
    "\n",
    "if not saved_any:\n",
    "    print(\"No importance plot saved (need SHAP or LR coefficients).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd9aeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Local explanation for a single test example (optional) ---\n",
    "if shap is not None and mdl_xgb is not None and hasattr(mdl_xgb, \"get_booster\"):\n",
    "    idx = int(np.random.randint(0, len(X_te)))\n",
    "    expl = shap.TreeExplainer(mdl_xgb)\n",
    "    sv = expl.shap_values(X_te.iloc[[idx]])\n",
    "    try:\n",
    "        shap.plots.waterfall(shap.Explanation(values=sv[0],\n",
    "                                              base_values=expl.expected_value,\n",
    "                                              data=X_te.iloc[idx].values,\n",
    "                                              feature_names=list(X_te.columns)),\n",
    "                             show=False)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(FIG_DIR / f\"shap_local_idx{idx}.png\", dpi=150, bbox_inches=\"tight\")\n",
    "        plt.close()\n",
    "        print(f\"Saved local SHAP waterfall for idx={idx}.\")\n",
    "    except Exception as e:\n",
    "        print(\"Waterfall plot not available with current SHAP version:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bc8316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Feature drift (KS) between train and test ---\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "rows = []\n",
    "for col in X_tr.columns:\n",
    "    a, b = pd.Series(X_tr[col]).dropna(), pd.Series(X_te[col]).dropna()\n",
    "    # only run KS if both vary\n",
    "    if a.nunique() > 1 and b.nunique() > 1:\n",
    "        stat, pval = ks_2samp(a, b)\n",
    "    else:\n",
    "        stat, pval = np.nan, np.nan\n",
    "    rows.append({\"feature\": col, \"ks_stat\": stat, \"p_value\": pval})\n",
    "\n",
    "drift = pd.DataFrame(rows).sort_values(\"p_value\", na_position=\"last\")\n",
    "drift.to_csv(DATA_DIR / \"feature_drift_ks.csv\", index=False)\n",
    "print(\"Saved:\", DATA_DIR / \"feature_drift_ks.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.13 (.venv stock-direction-ml)",
   "language": "python",
   "name": "stock-direction-ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
