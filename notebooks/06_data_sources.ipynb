{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "209bb1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Imports & params ---\n",
    "import os, json, pathlib, warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Optional dep (market data)\n",
    "try:\n",
    "    import yfinance as yf\n",
    "except Exception:\n",
    "    yf = None\n",
    "\n",
    "# Optional deps (models/explainability)\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import roc_auc_score, brier_score_loss, roc_curve, precision_recall_curve, average_precision_score\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "try:\n",
    "    import joblib\n",
    "except Exception:\n",
    "    joblib = None\n",
    "\n",
    "try:\n",
    "    import shap\n",
    "except Exception:\n",
    "    shap = None\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Base I/O\n",
    "DATA_DIR = pathlib.Path(\"data\")\n",
    "ART_DIR  = pathlib.Path(\"artifacts\")\n",
    "FIG_DIR  = pathlib.Path(\"reports/figures\")\n",
    "for p in [DATA_DIR, ART_DIR, FIG_DIR]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Experiment switches\n",
    "USE_MARKET        = True     # SPY/VIX context\n",
    "USE_FUNDAMENTALS  = False    # placeholder\n",
    "USE_NEWS          = False    # placeholder\n",
    "\n",
    "# Run settings\n",
    "TICKER = \"AAPL\"\n",
    "START, END = \"2015-01-01\", \"2023-12-31\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6d9989c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2687, 20)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>ret1</th>\n",
       "      <th>ret5</th>\n",
       "      <th>ret10</th>\n",
       "      <th>vol10</th>\n",
       "      <th>volz</th>\n",
       "      <th>rsi14</th>\n",
       "      <th>macd</th>\n",
       "      <th>macd_signal</th>\n",
       "      <th>ticker</th>\n",
       "      <th>spy_close</th>\n",
       "      <th>vix_close</th>\n",
       "      <th>mkt_ret1</th>\n",
       "      <th>mkt_ret5</th>\n",
       "      <th>vix_chg1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-02-06</td>\n",
       "      <td>26.738338</td>\n",
       "      <td>26.789579</td>\n",
       "      <td>26.388570</td>\n",
       "      <td>26.495506</td>\n",
       "      <td>174826400.0</td>\n",
       "      <td>-0.008421</td>\n",
       "      <td>0.019114</td>\n",
       "      <td>0.056819</td>\n",
       "      <td>0.023677</td>\n",
       "      <td>-0.879268</td>\n",
       "      <td>47.873804</td>\n",
       "      <td>0.616323</td>\n",
       "      <td>0.436496</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>171.193649</td>\n",
       "      <td>17.290001</td>\n",
       "      <td>-0.002765</td>\n",
       "      <td>0.030584</td>\n",
       "      <td>0.026113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-02-09</td>\n",
       "      <td>26.410844</td>\n",
       "      <td>26.698232</td>\n",
       "      <td>26.384110</td>\n",
       "      <td>26.671499</td>\n",
       "      <td>155559200.0</td>\n",
       "      <td>0.006642</td>\n",
       "      <td>0.013171</td>\n",
       "      <td>0.062710</td>\n",
       "      <td>0.023624</td>\n",
       "      <td>-1.026490</td>\n",
       "      <td>49.439594</td>\n",
       "      <td>0.633758</td>\n",
       "      <td>0.475949</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>170.427444</td>\n",
       "      <td>18.549999</td>\n",
       "      <td>-0.004476</td>\n",
       "      <td>0.013421</td>\n",
       "      <td>0.072874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-02-10</td>\n",
       "      <td>26.771758</td>\n",
       "      <td>27.212867</td>\n",
       "      <td>26.769531</td>\n",
       "      <td>27.183905</td>\n",
       "      <td>248034000.0</td>\n",
       "      <td>0.019212</td>\n",
       "      <td>0.032461</td>\n",
       "      <td>0.122426</td>\n",
       "      <td>0.019335</td>\n",
       "      <td>-0.104747</td>\n",
       "      <td>53.791711</td>\n",
       "      <td>0.681070</td>\n",
       "      <td>0.516973</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>172.243042</td>\n",
       "      <td>17.230000</td>\n",
       "      <td>0.010653</td>\n",
       "      <td>0.009617</td>\n",
       "      <td>-0.071159</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date       open       high        low      close       volume  \\\n",
       "0 2015-02-06  26.738338  26.789579  26.388570  26.495506  174826400.0   \n",
       "1 2015-02-09  26.410844  26.698232  26.384110  26.671499  155559200.0   \n",
       "2 2015-02-10  26.771758  27.212867  26.769531  27.183905  248034000.0   \n",
       "\n",
       "       ret1      ret5     ret10     vol10      volz      rsi14      macd  \\\n",
       "0 -0.008421  0.019114  0.056819  0.023677 -0.879268  47.873804  0.616323   \n",
       "1  0.006642  0.013171  0.062710  0.023624 -1.026490  49.439594  0.633758   \n",
       "2  0.019212  0.032461  0.122426  0.019335 -0.104747  53.791711  0.681070   \n",
       "\n",
       "   macd_signal ticker   spy_close  vix_close  mkt_ret1  mkt_ret5  vix_chg1  \n",
       "0     0.436496   AAPL  171.193649  17.290001 -0.002765  0.030584  0.026113  \n",
       "1     0.475949   AAPL  170.427444  18.549999 -0.004476  0.013421  0.072874  \n",
       "2     0.516973   AAPL  172.243042  17.230000  0.010653  0.009617 -0.071159  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Load the base features/labels from Phase 2 ---\n",
    "base_csv = DATA_DIR / \"df_nb02.csv\"\n",
    "if not base_csv.exists():\n",
    "    raise FileNotFoundError(\"Expected Phase-2 output at data/df_nb02.csv. Run notebook 02 first.\")\n",
    "\n",
    "df = pd.read_csv(base_csv)\n",
    "\n",
    "# Ensure tz-naive datetime\n",
    "if \"date\" not in df.columns:\n",
    "    raise KeyError(\"'date' column missing in df_nb02.csv\")\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\").dt.tz_localize(None)\n",
    "df = df.dropna(subset=[\"date\"]).reset_index(drop=True)\n",
    "\n",
    "print(df.shape)\n",
    "df.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d44bee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2235, 20)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>spy_close</th>\n",
       "      <th>mkt_ret1</th>\n",
       "      <th>mkt_ret5</th>\n",
       "      <th>vix_close</th>\n",
       "      <th>vix_chg1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-02-13</td>\n",
       "      <td>174.716675</td>\n",
       "      <td>0.004117</td>\n",
       "      <td>0.020579</td>\n",
       "      <td>14.69</td>\n",
       "      <td>-0.042373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-02-17</td>\n",
       "      <td>174.991470</td>\n",
       "      <td>0.001573</td>\n",
       "      <td>0.026780</td>\n",
       "      <td>15.80</td>\n",
       "      <td>0.075562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-02-18</td>\n",
       "      <td>175.008133</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>0.016054</td>\n",
       "      <td>15.45</td>\n",
       "      <td>-0.022152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-02-19</td>\n",
       "      <td>174.883163</td>\n",
       "      <td>-0.000714</td>\n",
       "      <td>0.014739</td>\n",
       "      <td>15.29</td>\n",
       "      <td>-0.010356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-02-20</td>\n",
       "      <td>175.932587</td>\n",
       "      <td>0.006001</td>\n",
       "      <td>0.011105</td>\n",
       "      <td>14.30</td>\n",
       "      <td>-0.064748</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date   spy_close  mkt_ret1  mkt_ret5  vix_close  vix_chg1\n",
       "0 2015-02-13  174.716675  0.004117  0.020579      14.69 -0.042373\n",
       "1 2015-02-17  174.991470  0.001573  0.026780      15.80  0.075562\n",
       "2 2015-02-18  175.008133  0.000095  0.016054      15.45 -0.022152\n",
       "3 2015-02-19  174.883163 -0.000714  0.014739      15.29 -0.010356\n",
       "4 2015-02-20  175.932587  0.006001  0.011105      14.30 -0.064748"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Market context via alignment (no merges) ---\n",
    "def fetch_close_series(ticker: str, start: str, end: str) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Return a pd.Series of adjusted Close with a tz-naive DatetimeIndex.\n",
    "    Works whether yfinance returns single-level or MultiIndex columns.\n",
    "    \"\"\"\n",
    "    if yf is None:\n",
    "        raise ImportError(\"Please `pip install yfinance` to enable USE_MARKET=True.\")\n",
    "\n",
    "    r = yf.download(ticker, start=start, end=end, auto_adjust=True, progress=False)\n",
    "    if r is None or r.empty:\n",
    "        raise ValueError(f\"No data for {ticker} in {start}..{end}\")\n",
    "\n",
    "    idx = pd.to_datetime(r.index, errors=\"coerce\")\n",
    "    try:\n",
    "        if getattr(idx, \"tz\", None) is not None:\n",
    "            idx = idx.tz_localize(None)\n",
    "    except Exception:\n",
    "        idx = pd.to_datetime(idx, errors=\"coerce\").tz_localize(None)\n",
    "\n",
    "    if isinstance(r.columns, pd.MultiIndex):\n",
    "        close = r.xs(\"Close\", axis=1, level=0)\n",
    "        if isinstance(close, pd.DataFrame):\n",
    "            close = close.iloc[:, 0]\n",
    "    else:\n",
    "        close = r[\"Close\"]\n",
    "\n",
    "    s = pd.Series(np.asarray(close).reshape(-1), index=idx, name=\"Close\")\n",
    "    s = s.sort_index()\n",
    "    s = s[~s.index.duplicated(keep=\"last\")]\n",
    "    return s\n",
    "\n",
    "if USE_MARKET:\n",
    "    dti = pd.to_datetime(df[\"date\"], errors=\"coerce\").dt.tz_localize(None)\n",
    "\n",
    "    spy = fetch_close_series(\"SPY\", START, END)\n",
    "    vix = fetch_close_series(\"^VIX\", START, END)\n",
    "\n",
    "    # Align by label (same-day)\n",
    "    df[\"spy_close\"] = dti.map(spy)\n",
    "    df[\"vix_close\"] = dti.map(vix)\n",
    "\n",
    "    # Context features\n",
    "    df[\"mkt_ret1\"] = df[\"spy_close\"].pct_change(1)\n",
    "    df[\"mkt_ret5\"] = df[\"spy_close\"].pct_change(5)\n",
    "    df[\"vix_chg1\"] = df[\"vix_close\"].pct_change(1)\n",
    "\n",
    "    # Keep rows with all context features present\n",
    "    df = df.dropna(subset=[\"spy_close\",\"vix_close\",\"mkt_ret1\",\"mkt_ret5\",\"vix_chg1\"]).reset_index(drop=True)\n",
    "\n",
    "print(df.shape)\n",
    "df.filter([\"date\",\"spy_close\",\"mkt_ret1\",\"mkt_ret5\",\"vix_close\",\"vix_chg1\"]).head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8ee437a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (2235, 20)\n",
      "Present: ['spy_close', 'vix_close', 'mkt_ret1', 'mkt_ret5', 'vix_chg1']\n",
      "Missing: []\n",
      "\n",
      "Nulls in context cols:\n",
      "spy_close    0\n",
      "vix_close    0\n",
      "mkt_ret1     0\n",
      "mkt_ret5     0\n",
      "vix_chg1     0\n",
      "dtype: int64\n",
      "\n",
      "Return stats:\n",
      "           count      mean       std       min       25%       50%       75%  \\\n",
      "mkt_ret1  2235.0  0.000506  0.011414 -0.109424 -0.003734  0.000555  0.005916   \n",
      "mkt_ret5  2235.0  0.002508  0.023450 -0.179694 -0.007008  0.004305  0.014748   \n",
      "vix_chg1  2235.0  0.003202  0.085385 -0.259057 -0.044079 -0.007215  0.035938   \n",
      "\n",
      "               max  \n",
      "mkt_ret1  0.090603  \n",
      "mkt_ret5  0.173581  \n",
      "vix_chg1  1.155979  \n"
     ]
    }
   ],
   "source": [
    "# --- Quality checks ---\n",
    "req = [\"spy_close\",\"vix_close\",\"mkt_ret1\",\"mkt_ret5\",\"vix_chg1\"]\n",
    "present = [c for c in req if c in df.columns]\n",
    "missing = [c for c in req if c not in df.columns]\n",
    "\n",
    "print(\"Shape:\", df.shape)\n",
    "print(\"Present:\", present)\n",
    "print(\"Missing:\", missing)\n",
    "\n",
    "if present:\n",
    "    print(\"\\nNulls in context cols:\")\n",
    "    print(df[present].isna().sum())\n",
    "\n",
    "    print(\"\\nReturn stats:\")\n",
    "    print(df[[\"mkt_ret1\",\"mkt_ret5\",\"vix_chg1\"]].describe().T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "987c2a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: data\\df_nb06_market.csv\n",
      "Updated: data\\storage_format.json\n"
     ]
    }
   ],
   "source": [
    "# --- Save experiment output (separate file so we don't touch Phase-2) ---\n",
    "out_name = \"df_nb06_market.csv\" if USE_MARKET else \"df_nb06_base.csv\"\n",
    "out_path = DATA_DIR / out_name\n",
    "df.to_csv(out_path, index=False)\n",
    "print(\"Saved:\", out_path)\n",
    "\n",
    "# --- Record storage format (quiet) ---\n",
    "meta_path = DATA_DIR / \"storage_format.json\"\n",
    "record = {\"path\": str(out_path), \"format\": \"csv\"}\n",
    "\n",
    "try:\n",
    "    if meta_path.exists():\n",
    "        meta = json.load(open(meta_path, \"r\", encoding=\"utf-8\"))\n",
    "        if isinstance(meta, dict):\n",
    "            meta = [meta]\n",
    "    else:\n",
    "        meta = []\n",
    "    meta = [m for m in meta if m.get(\"path\") != record[\"path\"]] + [record]\n",
    "    json.dump(meta, open(meta_path, \"w\", encoding=\"utf-8\"), indent=2)\n",
    "except Exception:\n",
    "    json.dump([record], open(meta_path, \"w\", encoding=\"utf-8\"), indent=2)\n",
    "\n",
    "print(\"Updated:\", meta_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ecc93bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes: (1662, 14) (416, 14) | labels: 1662 416\n",
      "Label balance (train/test): 0.532 / 0.524\n"
     ]
    }
   ],
   "source": [
    "# --- Load train/test splits (Parquet-or-CSV) with robust label fallback ---\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "DATA_DIR = Path(\"data\")\n",
    "\n",
    "def _can_parquet():\n",
    "    try:\n",
    "        import pyarrow  # noqa: F401\n",
    "        return True\n",
    "    except Exception:\n",
    "        try:\n",
    "            import fastparquet  # noqa: F401\n",
    "            return True\n",
    "        except Exception:\n",
    "            return False\n",
    "\n",
    "def read_table(base: Path) -> pd.DataFrame:\n",
    "    \"\"\"Read <base>.parquet if exists else <base>.csv.\"\"\"\n",
    "    p_parq = base.with_suffix(\".parquet\")\n",
    "    p_csv  = base.with_suffix(\".csv\")\n",
    "    if p_parq.exists():\n",
    "        return pd.read_parquet(p_parq)\n",
    "    if p_csv.exists():\n",
    "        return pd.read_csv(p_csv)\n",
    "    raise FileNotFoundError(f\"Neither {p_parq.name} nor {p_csv.name} exists.\")\n",
    "\n",
    "def write_table(df: pd.DataFrame, base: Path):\n",
    "    \"\"\"Write <base>.parquet if engine available else <base>.csv.\"\"\"\n",
    "    if _can_parquet():\n",
    "        df.to_parquet(base.with_suffix(\".parquet\"), index=False)\n",
    "    else:\n",
    "        df.to_csv(base.with_suffix(\".csv\"), index=False)\n",
    "\n",
    "def find_or_build_label(df: pd.DataFrame) -> str:\n",
    "    \"\"\"Return name of label column, creating 'y' if needed from next-day return.\"\"\"\n",
    "    # already present?\n",
    "    for cand in [\"y\",\"target\",\"label\",\"y_next_up\"]:\n",
    "        if cand in df.columns:\n",
    "            return cand\n",
    "    # derive from next-day ret1 or from close\n",
    "    if \"ret1\" in df.columns:\n",
    "        next_ret = pd.Series(df[\"ret1\"]).shift(-1)\n",
    "    else:\n",
    "        price_col = next((c for c in [\"close\",\"Close\",\"Adj Close\",\"adj_close\"] if c in df.columns), None)\n",
    "        if price_col is None:\n",
    "            raise RuntimeError(\"No label found and cannot derive it: need 'ret1' or a close-price column.\")\n",
    "        next_ret = pd.Series(df[price_col]).pct_change().shift(-1)\n",
    "    df[\"y\"] = (next_ret > 0).astype(int)\n",
    "    return \"y\"\n",
    "\n",
    "# 1) Try pre-built train/test (parquet or csv) -------------------------------\n",
    "try:\n",
    "    X_tr = read_table(DATA_DIR / \"train\").copy()\n",
    "    X_te = read_table(DATA_DIR / \"test\").copy()\n",
    "\n",
    "    # peel labels if embedded; else read separate y files\n",
    "    label_col = next((c for c in [\"y\",\"target\",\"label\",\"y_next_up\"] if c in X_tr.columns), None)\n",
    "    if label_col:\n",
    "        y_tr = X_tr.pop(label_col).astype(int).values\n",
    "        y_te = X_te.pop(label_col).astype(int).values\n",
    "    else:\n",
    "        y_tr = read_table(DATA_DIR / \"y_train\").iloc[:,0].astype(int).values\n",
    "        y_te = read_table(DATA_DIR / \"y_test\").iloc[:,0].astype(int).values\n",
    "\n",
    "except FileNotFoundError:\n",
    "    # 2) Fallback: rebuild splits from df_nb02.csv ---------------------------\n",
    "    base_csv = DATA_DIR / \"df_nb02.csv\"\n",
    "    if not base_csv.exists():\n",
    "        raise RuntimeError(\n",
    "            \"Label not found and pre-built splits missing. \"\n",
    "            \"Expected data/df_nb02.csv to rebuild.\"\n",
    "        )\n",
    "\n",
    "    df2 = pd.read_csv(base_csv)\n",
    "    # normalize dates/order\n",
    "    if \"date\" in df2.columns:\n",
    "        df2[\"date\"] = pd.to_datetime(df2[\"date\"], errors=\"coerce\").dt.tz_localize(None)\n",
    "        df2 = df2.dropna(subset=[\"date\"]).sort_values(\"date\").reset_index(drop=True)\n",
    "\n",
    "    # find/derive label\n",
    "    label_col = find_or_build_label(df2)\n",
    "\n",
    "    # drop obvious non-features; keep engineered returns, etc.\n",
    "    drop_cols = [c for c in [\"date\",\"ticker\",\"symbol\",\"spy_close\",\"vix_close\"] if c in df2.columns]\n",
    "    y_all = df2.pop(label_col).astype(int).values\n",
    "    X_all = df2.drop(columns=drop_cols, errors=\"ignore\").select_dtypes(include=[np.number]).copy()\n",
    "\n",
    "    # time split: last 20% as test\n",
    "    n = len(X_all); cut = int(n * 0.8)\n",
    "    X_tr, X_te = X_all.iloc[:cut].copy(), X_all.iloc[cut:].copy()\n",
    "    y_tr, y_te = y_all[:cut], y_all[cut:]\n",
    "\n",
    "    # persist for next runs\n",
    "    write_table(X_tr, DATA_DIR / \"train\")\n",
    "    write_table(X_te, DATA_DIR / \"test\")\n",
    "    write_table(pd.DataFrame({\"y\": y_tr}), DATA_DIR / \"y_train\")\n",
    "    write_table(pd.DataFrame({\"y\": y_te}), DATA_DIR / \"y_test\")\n",
    "    print(\"Rebuilt splits → data/train.*, data/test.*, y_*.{parquet|csv}\")\n",
    "\n",
    "print(\"Shapes:\", X_tr.shape, X_te.shape, \"| labels:\", len(y_tr), len(y_te))\n",
    "print(\"Label balance (train/test):\", float(np.mean(y_tr)).__round__(3), \"/\", float(np.mean(y_te)).__round__(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8007f8ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HAS_LR: False | HAS_XGB: False\n"
     ]
    }
   ],
   "source": [
    "# --- Load trained models from Phase 5 (if available) ---\n",
    "path_lr  = ART_DIR / \"model_logreg.pkl\"\n",
    "path_xgb = ART_DIR / \"model_xgb.pkl\"\n",
    "\n",
    "HAS_LR  = path_lr.exists() and joblib is not None\n",
    "HAS_XGB = path_xgb.exists() and joblib is not None\n",
    "\n",
    "mdl_lr  = joblib.load(path_lr)  if HAS_LR  else None\n",
    "mdl_xgb = joblib.load(path_xgb) if HAS_XGB else None\n",
    "\n",
    "print(\"HAS_LR:\", HAS_LR, \"| HAS_XGB:\", HAS_XGB)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1e6edd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: data\\explainability_summary.csv\n"
     ]
    }
   ],
   "source": [
    "# --- Predict on test set ---\n",
    "p_te_lr = mdl_lr.predict_proba(X_te)[:,1] if HAS_LR else None\n",
    "p_te_xgb = mdl_xgb.predict_proba(X_te)[:,1] if HAS_XGB else None\n",
    "\n",
    "rows = []\n",
    "if p_te_lr is not None:\n",
    "    rows.append({\n",
    "        \"model\": \"logreg\",\n",
    "        \"AUC_test\": float(roc_auc_score(y_te, p_te_lr)),\n",
    "        \"Brier_test\": float(brier_score_loss(y_te, p_te_lr))\n",
    "    })\n",
    "if p_te_xgb is not None:\n",
    "    rows.append({\n",
    "        \"model\": \"xgb\",\n",
    "        \"AUC_test\": float(roc_auc_score(y_te, p_te_xgb)),\n",
    "        \"Brier_test\": float(brier_score_loss(y_te, p_te_xgb))\n",
    "    })\n",
    "\n",
    "summary = pd.DataFrame(rows).round(4)\n",
    "display(summary)\n",
    "\n",
    "(DATA_DIR / \"explainability_summary.csv\").write_text(summary.to_csv(index=False))\n",
    "print(\"Saved:\", DATA_DIR / \"explainability_summary.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3c59f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved ROC/PR figures to reports\\figures\n"
     ]
    }
   ],
   "source": [
    "# --- ROC and PR curves ---\n",
    "def plot_and_save_curves(y, p, name):\n",
    "    # ROC\n",
    "    fpr, tpr, _ = roc_curve(y, p)\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.plot(fpr, tpr, label=f\"{name}\")\n",
    "    plt.plot([0,1],[0,1],\"--\")\n",
    "    plt.xlabel(\"FPR\")\n",
    "    plt.ylabel(\"TPR\")\n",
    "    plt.title(f\"ROC – {name}\")\n",
    "    plt.grid(True, alpha=.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(FIG_DIR / f\"roc_{name}.png\", dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "    # PR\n",
    "    prec, rec, _ = precision_recall_curve(y, p)\n",
    "    ap = average_precision_score(y, p)\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.plot(rec, prec, label=f\"AP={ap:.3f}\")\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.title(f\"Precision–Recall – {name}\")\n",
    "    plt.grid(True, alpha=.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(FIG_DIR / f\"pr_{name}.png\", dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "if p_te_lr is not None:  plot_and_save_curves(y_te, p_te_lr, \"logreg\")\n",
    "if p_te_xgb is not None: plot_and_save_curves(y_te, p_te_xgb, \"xgb\")\n",
    "\n",
    "print(\"Saved ROC/PR figures to\", FIG_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73149932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No model probabilities found for calibration.\n"
     ]
    }
   ],
   "source": [
    "# --- Calibration / reliability ---\n",
    "def reliability_summary(y_true, p, model_name, n_bins=10):\n",
    "    bins = pd.qcut(p, q=n_bins, labels=False, duplicates=\"drop\")\n",
    "    df_bin = pd.DataFrame({\"y\": y_true, \"p\": p, \"bin\": bins})\n",
    "    grp = df_bin.groupby(\"bin\", as_index=False).agg(\n",
    "        n=(\"y\",\"size\"),\n",
    "        avg_pred=(\"p\",\"mean\"),\n",
    "        avg_true=(\"y\",\"mean\")\n",
    "    )\n",
    "    grp.insert(0, \"model\", model_name)\n",
    "    return grp\n",
    "\n",
    "to_save = []\n",
    "for name, p in [(\"logreg\", p_te_lr), (\"xgb\", p_te_xgb)]:\n",
    "    if p is None: \n",
    "        continue\n",
    "    prob_true, prob_pred = calibration_curve(y_te, p, n_bins=10, strategy=\"quantile\")\n",
    "\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.plot(prob_pred, prob_true, marker=\"o\", label=f\"{name}\")\n",
    "    plt.plot([0,1],[0,1], linestyle=\"--\")\n",
    "    plt.xlabel(\"Predicted probability\")\n",
    "    plt.ylabel(\"Observed frequency\")\n",
    "    plt.title(f\"Calibration – {name}\")\n",
    "    plt.grid(True, alpha=.3)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(FIG_DIR / f\"calibration_{name}.png\", dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "    to_save.append(reliability_summary(y_te, p, name, n_bins=10))\n",
    "\n",
    "if to_save:\n",
    "    rel = pd.concat(to_save, ignore_index=True)\n",
    "    rel.to_csv(DATA_DIR / \"reliability_by_decile.csv\", index=False)\n",
    "    print(\"Saved calibration_*.png and reliability_by_decile.csv\")\n",
    "else:\n",
    "    print(\"No model probabilities found for calibration.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55ce1852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No importance plot saved (need SHAP or LR coefficients).\n"
     ]
    }
   ],
   "source": [
    "# --- Global importance ---\n",
    "saved_any = False\n",
    "\n",
    "if (shap is not None) and (mdl_xgb is not None) and hasattr(mdl_xgb, \"get_booster\"):\n",
    "    # XGBoost SHAP\n",
    "    expl = shap.TreeExplainer(mdl_xgb)\n",
    "    shap_vals = expl.shap_values(X_te)\n",
    "    # Mean |SHAP|\n",
    "    imp = pd.DataFrame({\n",
    "        \"feature\": X_te.columns,\n",
    "        \"mean_abs_shap\": np.mean(np.abs(shap_vals), axis=0)\n",
    "    }).sort_values(\"mean_abs_shap\", ascending=False)\n",
    "\n",
    "    imp.to_csv(DATA_DIR / \"shap_importance_top20.csv\", index=False)\n",
    "    plt.figure(figsize=(7,8))\n",
    "    shap.summary_plot(shap_vals, X_te, show=False)  # creates a beeswarm\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(FIG_DIR / \"shap_importance.png\", dpi=150, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    saved_any = True\n",
    "    print(\"Saved SHAP importance artifacts.\")\n",
    "\n",
    "# Logistic regression absolute coefficients as a fallback/extra\n",
    "if mdl_lr is not None and hasattr(mdl_lr, \"coef_\"):\n",
    "    coefs = pd.Series(mdl_lr.coef_.ravel(), index=X_te.columns).abs().sort_values(ascending=False)\n",
    "    coef_df = coefs.reset_index()\n",
    "    coef_df.columns = [\"feature\", \"abs_coef\"]\n",
    "    coef_df.to_csv(DATA_DIR / \"logreg_abscoef_top20.csv\", index=False)\n",
    "\n",
    "    plt.figure(figsize=(7,5))\n",
    "    top = coef_df.head(20).sort_values(\"abs_coef\")\n",
    "    plt.barh(top[\"feature\"], top[\"abs_coef\"])\n",
    "    plt.title(\"LogReg |coeff| top-20\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(FIG_DIR / \"logreg_coef_importance.png\", dpi=150)\n",
    "    plt.close()\n",
    "    saved_any = True\n",
    "    print(\"Saved logistic-regression coefficient importance.\")\n",
    "\n",
    "if not saved_any:\n",
    "    print(\"No importance plot saved (need SHAP or LR coefficients).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ffd9aeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Local explanation for a single test example (optional) ---\n",
    "if shap is not None and mdl_xgb is not None and hasattr(mdl_xgb, \"get_booster\"):\n",
    "    idx = int(np.random.randint(0, len(X_te)))\n",
    "    expl = shap.TreeExplainer(mdl_xgb)\n",
    "    sv = expl.shap_values(X_te.iloc[[idx]])\n",
    "    try:\n",
    "        shap.plots.waterfall(shap.Explanation(values=sv[0],\n",
    "                                              base_values=expl.expected_value,\n",
    "                                              data=X_te.iloc[idx].values,\n",
    "                                              feature_names=list(X_te.columns)),\n",
    "                             show=False)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(FIG_DIR / f\"shap_local_idx{idx}.png\", dpi=150, bbox_inches=\"tight\")\n",
    "        plt.close()\n",
    "        print(f\"Saved local SHAP waterfall for idx={idx}.\")\n",
    "    except Exception as e:\n",
    "        print(\"Waterfall plot not available with current SHAP version:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90bc8316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: data\\feature_drift_ks.csv\n"
     ]
    }
   ],
   "source": [
    "# --- Feature drift (KS) between train and test ---\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "rows = []\n",
    "for col in X_tr.columns:\n",
    "    a, b = pd.Series(X_tr[col]).dropna(), pd.Series(X_te[col]).dropna()\n",
    "    # only run KS if both vary\n",
    "    if a.nunique() > 1 and b.nunique() > 1:\n",
    "        stat, pval = ks_2samp(a, b)\n",
    "    else:\n",
    "        stat, pval = np.nan, np.nan\n",
    "    rows.append({\"feature\": col, \"ks_stat\": stat, \"p_value\": pval})\n",
    "\n",
    "drift = pd.DataFrame(rows).sort_values(\"p_value\", na_position=\"last\")\n",
    "drift.to_csv(DATA_DIR / \"feature_drift_ks.csv\", index=False)\n",
    "print(\"Saved:\", DATA_DIR / \"feature_drift_ks.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
