{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90291461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repo root â†’ C:\\.projects\\stock-direction-ml\\stock-direction-ml\\notebooks\n",
      "Has data?       True\n",
      "Has artifacts?  True\n"
     ]
    }
   ],
   "source": [
    "# NB18 â€” Repo root autodetect\n",
    "from pathlib import Path\n",
    "\n",
    "def find_repo_root(start: Path, must_have=(\"data\", \"artifacts\")) -> Path:\n",
    "    cur = start.resolve()\n",
    "    for _ in range(6):\n",
    "        if all((cur / m).exists() for m in must_have):\n",
    "            return cur\n",
    "        cur = cur.parent\n",
    "    if start.name.lower() == \"notebooks\" and all((start.parent / m).exists() for m in must_have):\n",
    "        return start.parent.resolve()\n",
    "    raise FileNotFoundError(f\"Could not locate repo root containing {must_have} starting at {start}\")\n",
    "\n",
    "CWD = Path.cwd()\n",
    "ROOT = find_repo_root(CWD)\n",
    "print(\"Repo root â†’\", ROOT)\n",
    "print(\"Has data?      \", (ROOT/\"data\").exists())\n",
    "print(\"Has artifacts? \", (ROOT/\"artifacts\").exists())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab5cc12f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "app/ exists: True | __init__.py: True\n"
     ]
    }
   ],
   "source": [
    "# NB18 â€” Ensure app/ package exists\n",
    "from pathlib import Path\n",
    "APP = ROOT / \"app\"\n",
    "APP.mkdir(parents=True, exist_ok=True)\n",
    "init_path = APP / \"__init__.py\"\n",
    "if not init_path.exists():\n",
    "    init_path.write_text(\"# package marker\\n\", encoding=\"utf-8\")\n",
    "print(\"app/ exists:\", APP.exists(), \"| __init__.py:\", init_path.exists())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba632d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: C:\\.projects\\stock-direction-ml\\stock-direction-ml\\notebooks\\app\\config.py\n"
     ]
    }
   ],
   "source": [
    "# NB18 â€” Write app/config.py\n",
    "from textwrap import dedent\n",
    "code = dedent(\"\"\"\n",
    "# app/config.py\n",
    "from pathlib import Path\n",
    "\n",
    "ROOT = Path(__file__).resolve().parents[1]  # repo root\n",
    "\n",
    "# Artifacts\n",
    "ART_DIR_EQUITY = ROOT / \"artifacts\"\n",
    "ART_DIR_CRYPTO = ROOT / \"artifacts_crypto\"  # will be created in NB20\n",
    "\n",
    "# Optional per-ticker thresholds (equities)\n",
    "TAU_MAP_PATH = ART_DIR_EQUITY / \"tau_map.json\"\n",
    "\n",
    "# Defaults\n",
    "DEFAULT_TAU = 0.59\n",
    "\n",
    "# Local data\n",
    "DATA_CSV = ROOT / \"data\" / \"df_nb02.csv\"\n",
    "DATA_PQ  = ROOT / \"data\" / \"df_nb02.parquet\"\n",
    "\"\"\").lstrip()\n",
    "out = (ROOT/\"app\"/\"config.py\")\n",
    "out.write_text(code, encoding=\"utf-8\")\n",
    "print(\"Wrote:\", out.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90b3a588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: C:\\.projects\\stock-direction-ml\\stock-direction-ml\\notebooks\\app\\lib_artifacts.py\n"
     ]
    }
   ],
   "source": [
    "# NB18 â€” Write app/lib_artifacts.py\n",
    "from textwrap import dedent\n",
    "code = dedent(\"\"\"\n",
    "# app/lib_artifacts.py\n",
    "import json, joblib\n",
    "from .config import ART_DIR_EQUITY, ART_DIR_CRYPTO, DEFAULT_TAU, TAU_MAP_PATH\n",
    "\n",
    "def _safe_tau(v, default):\n",
    "    try: return float(v)\n",
    "    except Exception: return default\n",
    "\n",
    "def load_artifacts(asset_class=\"equity\"):\n",
    "    art = ART_DIR_EQUITY if asset_class == \"equity\" else ART_DIR_CRYPTO\n",
    "    feature_list = json.loads((art/\"feature_list.json\").read_text(encoding=\"utf-8\"))\n",
    "    scaler = joblib.load(art/\"scaler.joblib\")\n",
    "    model  = joblib.load(art/\"lr.joblib\")\n",
    "\n",
    "    tau_art = DEFAULT_TAU\n",
    "    tfile = art / \"threshold.json\"\n",
    "    if tfile.exists():\n",
    "        try:\n",
    "            t = json.loads(tfile.read_text(encoding=\"utf-8\"))\n",
    "            tau_art = _safe_tau(t.get(\"tau\") or t.get(\"threshold\") or t.get(\"value\"), DEFAULT_TAU)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    tau_map = {}\n",
    "    if asset_class == \"equity\" and TAU_MAP_PATH.exists():\n",
    "        try:\n",
    "            tau_map = json.loads(TAU_MAP_PATH.read_text(encoding=\"utf-8\"))\n",
    "        except Exception:\n",
    "            tau_map = {}\n",
    "\n",
    "    return feature_list, scaler, model, tau_art, tau_map\n",
    "\"\"\").lstrip()\n",
    "out = (ROOT/\"app\"/\"lib_artifacts.py\")\n",
    "out.write_text(code, encoding=\"utf-8\")\n",
    "print(\"Wrote:\", out.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3cc2d82a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: C:\\.projects\\stock-direction-ml\\stock-direction-ml\\notebooks\\app\\lib_features.py\n"
     ]
    }
   ],
   "source": [
    "# NB18 â€” Write app/lib_features.py\n",
    "from textwrap import dedent\n",
    "code = dedent(\"\"\"\n",
    "# app/lib_features.py\n",
    "import math, numpy as np, pandas as pd\n",
    "\n",
    "def ema(s, span): return s.ewm(span=span, adjust=False).mean()\n",
    "\n",
    "def rsi_wilder(close, length=14):\n",
    "    d = close.diff()\n",
    "    gain = d.clip(lower=0).ewm(alpha=1/length, adjust=False).mean()\n",
    "    loss = (-d.clip(upper=0)).ewm(alpha=1/length, adjust=False).mean()\n",
    "    rs = gain / (loss + 1e-12)\n",
    "    return 100 - (100/(1+rs))\n",
    "\n",
    "def macd_and_signal(close, fast=12, slow=26, sig=9):\n",
    "    macd = ema(close, fast) - ema(close, slow)\n",
    "    signal = ema(macd, sig)\n",
    "    return macd, signal\n",
    "\n",
    "def add_nb02_features(df):\n",
    "    df = df.copy()\n",
    "    ret1  = df[\"close\"].pct_change(1)\n",
    "    ret5  = df[\"close\"].pct_change(5)\n",
    "    ret10 = df[\"close\"].pct_change(10)\n",
    "    vol10 = ret1.rolling(10).std()\n",
    "    volz  = (vol10 - vol10.rolling(100).mean()) / (vol10.rolling(100).std() + 1e-12)\n",
    "    rsi14 = rsi_wilder(df[\"close\"], 14)\n",
    "    macd, macd_signal = macd_and_signal(df[\"close\"], 12, 26, 9)\n",
    "\n",
    "    df[\"ret1\"]=ret1; df[\"ret5\"]=ret5; df[\"ret10\"]=ret10\n",
    "    df[\"vol10\"]=vol10; df[\"volz\"]=volz\n",
    "    df[\"rsi14\"]=rsi14; df[\"macd\"]=macd; df[\"macd_signal\"]=macd_signal\n",
    "    return df\n",
    "\n",
    "def add_market_features_equity(df, spy_series, vix_series):\n",
    "    m = df.merge(spy_series, on=\"date\", how=\"left\").merge(vix_series, on=\"date\", how=\"left\")\n",
    "    m[\"mkt_ret1\"] = m[\"spy_close\"].pct_change(1)\n",
    "    m[\"mkt_ret5\"] = m[\"spy_close\"].pct_change(5)\n",
    "    m[\"vix_chg1\"] = m[\"vix_close\"].pct_change(1)\n",
    "    return m\n",
    "\n",
    "def add_market_features_crypto(df, bench_close):\n",
    "    m = df.merge(bench_close, on=\"date\", how=\"left\").rename(columns={\"btc_close\":\"spy_close\"})\n",
    "    m[\"mkt_ret1\"]  = m[\"spy_close\"].pct_change(1)\n",
    "    m[\"mkt_ret5\"]  = m[\"spy_close\"].pct_change(5)\n",
    "    btc_ret = m[\"spy_close\"].pct_change()\n",
    "    vix_proxy = btc_ret.rolling(30).std() * math.sqrt(365) * 100.0\n",
    "    m[\"vix_close\"] = vix_proxy\n",
    "    m[\"vix_chg1\"]  = m[\"vix_close\"].pct_change(1)\n",
    "    return m\n",
    "\n",
    "def infer_target(df):\n",
    "    for c in [\"y\",\"label\",\"target\",\"y_bin\",\"direction\",\"is_up\",\"class\",\"cls\"]:\n",
    "        if c in df.columns: return df[c].astype(int).clip(0,1).values, c\n",
    "    if \"ret_next\" in df.columns:\n",
    "        y = (df[\"ret_next\"].astype(float) > 0).astype(int).values\n",
    "        return y, \"ret_next>0\"\n",
    "    if \"close\" in df.columns:\n",
    "        rn = df[\"close\"].astype(float).pct_change().shift(-1).fillna(0.0)\n",
    "        df[\"ret_next\"] = rn\n",
    "        return (rn > 0).astype(int).values, \"ret_next_from_close>0\"\n",
    "    return None, None\n",
    "\n",
    "def make_dataset(df, feature_list):\n",
    "    cols = [c for c in feature_list if c in df.columns]\n",
    "    if not cols: raise ValueError(\"No overlap between feature_list.json and data columns.\")\n",
    "    tmp = df[cols].replace([np.inf,-np.inf], np.nan)\n",
    "    y_vals, _ = infer_target(df)\n",
    "    retn = df[\"ret_next\"].astype(float).values if \"ret_next\" in df.columns else np.zeros(len(df))\n",
    "    tmp[\"__y__\"] = y_vals if y_vals is not None else np.nan\n",
    "    tmp[\"__ret_next__\"] = retn\n",
    "    tmp = tmp.dropna()\n",
    "    X = tmp[cols].to_numpy()\n",
    "    y = (tmp[\"__y__\"].astype(int).to_numpy() if y_vals is not None else None)\n",
    "    retn = tmp[\"__ret_next__\"].astype(float).to_numpy()\n",
    "    idx = tmp.index\n",
    "    return X, y, retn, idx, cols\n",
    "\"\"\").lstrip()\n",
    "out = (ROOT/\"app\"/\"lib_features.py\")\n",
    "out.write_text(code, encoding=\"utf-8\")\n",
    "print(\"Wrote:\", out.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56e60e3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: C:\\.projects\\stock-direction-ml\\stock-direction-ml\\notebooks\\app\\lib_fetch.py\n"
     ]
    }
   ],
   "source": [
    "# NB18 â€” Write app/lib_fetch.py\n",
    "from textwrap import dedent\n",
    "code = dedent(\"\"\"\n",
    "# app/lib_fetch.py\n",
    "import pandas as pd\n",
    "from .config import DATA_CSV, DATA_PQ\n",
    "from .lib_features import add_nb02_features\n",
    "\n",
    "def ensure_date(df):\n",
    "    for c in [\"date\",\"Date\",\"timestamp\",\"ts\"]:\n",
    "        if c in df.columns:\n",
    "            try: df[c] = pd.to_datetime(df[c])\n",
    "            except: pass\n",
    "            if c != \"date\": df[\"date\"] = df[c]\n",
    "            return df\n",
    "    return df\n",
    "\n",
    "def load_repo_df():\n",
    "    if DATA_CSV.exists(): df = pd.read_csv(DATA_CSV)\n",
    "    elif DATA_PQ.exists(): df = pd.read_parquet(DATA_PQ)\n",
    "    else: raise FileNotFoundError(\"Missing data/df_nb02.csv or .parquet\")\n",
    "    return ensure_date(df)\n",
    "\n",
    "def fetch_equity_df(ticker, start, end):\n",
    "    import yfinance as yf\n",
    "    px = yf.download(ticker, start=start, end=end, auto_adjust=True, progress=False)\n",
    "    if px.empty: raise ValueError(f\"No data for {ticker}\")\n",
    "    df = px.rename_axis(\"date\").reset_index()\n",
    "    df[\"date\"]=pd.to_datetime(df[\"date\"])\n",
    "    df = df.rename(columns={\"Open\":\"open\",\"High\":\"high\",\"Low\":\"low\",\"Close\":\"close\",\"Volume\":\"volume\"})\n",
    "    df = df[[\"date\",\"open\",\"high\",\"low\",\"close\",\"volume\"]]; df[\"ticker\"]=ticker\n",
    "    df = add_nb02_features(df)\n",
    "\n",
    "    spy = yf.download(\"SPY\", start=start, end=end, auto_adjust=True, progress=False).rename_axis(\"date\").reset_index()[[\"date\",\"Close\"]].rename(columns={\"Close\":\"spy_close\"})\n",
    "    vix = yf.download(\"^VIX\", start=start, end=end, progress=False).rename_axis(\"date\").reset_index()[[\"date\",\"Close\"]].rename(columns={\"Close\":\"vix_close\"})\n",
    "    spy[\"date\"]=pd.to_datetime(spy[\"date\"]); vix[\"date\"]=pd.to_datetime(vix[\"date\"])\n",
    "\n",
    "    df = df.merge(spy,on=\"date\",how=\"left\").merge(vix,on=\"date\",how=\"left\")\n",
    "    df[\"mkt_ret1\"]=df[\"spy_close\"].pct_change(1); df[\"mkt_ret5\"]=df[\"spy_close\"].pct_change(5)\n",
    "    df[\"vix_chg1\"]=df[\"vix_close\"].pct_change(1)\n",
    "    df[\"ret_next\"]=df[\"close\"].pct_change().shift(-1); df[\"y\"]=(df[\"ret_next\"]>0).astype(int)\n",
    "    return df\n",
    "\n",
    "def fetch_crypto_df(ticker, start, end):\n",
    "    import yfinance as yf\n",
    "    px = yf.download(ticker, start=start, end=end, auto_adjust=True, progress=False)\n",
    "    if px.empty: raise ValueError(f\"No data for {ticker}\")\n",
    "    df = px.rename_axis(\"date\").reset_index()\n",
    "    df[\"date\"]=pd.to_datetime(df[\"date\"])\n",
    "    df = df.rename(columns={\"Open\":\"open\",\"High\":\"high\",\"Low\":\"low\",\"Close\":\"close\",\"Volume\":\"volume\"})\n",
    "    df = df[[\"date\",\"open\",\"high\",\"low\",\"close\",\"volume\"]]; df[\"ticker\"]=ticker\n",
    "    df = add_nb02_features(df)\n",
    "\n",
    "    btc = yf.download(\"BTC-USD\", start=start, end=end, auto_adjust=True, progress=False).rename_axis(\"date\").reset_index()[[\"date\",\"Close\"]].rename(columns={\"Close\":\"btc_close\"})\n",
    "    btc[\"date\"]=pd.to_datetime(btc[\"date\"])\n",
    "    df = df.merge(btc,on=\"date\",how=\"left\").rename(columns={\"btc_close\":\"spy_close\"})\n",
    "    df[\"mkt_ret1\"]=df[\"spy_close\"].pct_change(1); df[\"mkt_ret5\"]=df[\"spy_close\"].pct_change(5)\n",
    "\n",
    "    ret = df[\"spy_close\"].pct_change()\n",
    "    vix_proxy = ret.rolling(30).std() * (365 ** 0.5) * 100.0\n",
    "    df[\"vix_close\"]=vix_proxy; df[\"vix_chg1\"]=df[\"vix_close\"].pct_change(1)\n",
    "\n",
    "    df[\"ret_next\"]=df[\"close\"].pct_change().shift(-1); df[\"y\"]=(df[\"ret_next\"]>0).astype(int)\n",
    "    return df\n",
    "\"\"\").lstrip()\n",
    "out = (ROOT/\"app\"/\"lib_fetch.py\")\n",
    "out.write_text(code, encoding=\"utf-8\")\n",
    "print(\"Wrote:\", out.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba1b1b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: C:\\.projects\\stock-direction-ml\\stock-direction-ml\\notebooks\\app\\lib_eval.py\n"
     ]
    }
   ],
   "source": [
    "# NB18 â€” Write app/lib_eval.py\n",
    "from textwrap import dedent\n",
    "code = dedent(\"\"\"\n",
    "# app/lib_eval.py\n",
    "import numpy as np\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, average_precision_score, brier_score_loss, log_loss, f1_score\n",
    ")\n",
    "\n",
    "def predict_proba(model, X):\n",
    "    if hasattr(model,\"predict_proba\"):\n",
    "        p = model.predict_proba(X)\n",
    "        return p[:,1] if p.ndim==2 else p\n",
    "    if hasattr(model,\"decision_function\"):\n",
    "        s = model.decision_function(X); return 1/(1+np.exp(-s))\n",
    "    return np.clip(model.predict(X).astype(float), 0, 1)\n",
    "\n",
    "def metrics_all(y, p):\n",
    "    def safe(fn,*a):\n",
    "        try: return float(fn(*a))\n",
    "        except: return float(\"nan\")\n",
    "    return dict(\n",
    "        auc     = safe(roc_auc_score, y, p),\n",
    "        ap      = safe(average_precision_score, y, p),\n",
    "        brier   = safe(brier_score_loss, y, p),\n",
    "        logloss = safe(log_loss, y, p),\n",
    "    )\n",
    "\n",
    "def tau_sweep(y, p, retn, fee_bps=5, grid=None):\n",
    "    import numpy as np\n",
    "    if grid is None: grid = np.linspace(0.05, 0.95, 91)\n",
    "    f1s, finals = [], []\n",
    "    for t in grid:\n",
    "        sig = (p >= t).astype(int)\n",
    "        f1s.append(_safe_f1(y, sig))\n",
    "        finals.append(_final_equity(retn, sig, fee_bps))\n",
    "    return grid, np.array(f1s), np.array(finals)\n",
    "\n",
    "def _safe_f1(y, sig):\n",
    "    try: return f1_score(y, sig)\n",
    "    except: return float(\"nan\")\n",
    "\n",
    "def _final_equity(retn, sig, fee_bps):\n",
    "    import numpy as np\n",
    "    flips = np.zeros_like(sig)\n",
    "    if len(flips)>1: flips[1:] = (sig[1:] != sig[:-1]).astype(int)\n",
    "    fee = flips * (fee_bps/10000.0)\n",
    "    eq  = np.cumprod(1 + (retn*sig - fee))\n",
    "    return float(eq[-1]) if len(eq) else float(\"nan\")\n",
    "\"\"\").lstrip()\n",
    "out = (ROOT/\"app\"/\"lib_eval.py\")\n",
    "out.write_text(code, encoding=\"utf-8\")\n",
    "print(\"Wrote:\", out.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71ccf9ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: C:\\.projects\\stock-direction-ml\\stock-direction-ml\\notebooks\\app\\streamlit_app.py\n"
     ]
    }
   ],
   "source": [
    "# NB18 â€” Write modular Streamlit app (Repo/Fetch, Equity/Crypto, Ï„-map)\n",
    "from textwrap import dedent\n",
    "code = dedent(\"\"\"\n",
    "# app/streamlit_app.py â€” modular, fetch-enabled\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt, streamlit as st\n",
    "from app.config import ROOT, DEFAULT_TAU\n",
    "from app.lib_artifacts import load_artifacts\n",
    "from app.lib_fetch import load_repo_df, fetch_equity_df, fetch_crypto_df\n",
    "from app.lib_features import make_dataset\n",
    "from app.lib_eval import predict_proba, metrics_all, tau_sweep\n",
    "\n",
    "st.set_page_config(page_title=\"Direction Classifier\", layout=\"wide\")\n",
    "st.title(\"ðŸ“ˆ Direction Classifier â€” Any Ticker (Equities & Crypto)\")\n",
    "\n",
    "# ---- Sidebar: data source ----\n",
    "with st.sidebar:\n",
    "    st.header(\"Data source\")\n",
    "    src = st.radio(\"Choose\", [\"Repo file\",\"Fetch (Yahoo)\"], index=0)\n",
    "    asset_class = st.selectbox(\"Asset class\", [\"equity\",\"crypto\"], index=0)\n",
    "\n",
    "    if src == \"Repo file\":\n",
    "        df = load_repo_df()\n",
    "        if \"ticker\" in df.columns:\n",
    "            ticks = sorted(df[\"ticker\"].dropna().unique().tolist())\n",
    "            default = df[\"ticker\"].value_counts().idxmax()\n",
    "            ticker = st.selectbox(\"Ticker\", ticks, index=max(0, ticks.index(default)))\n",
    "            df = df.loc[df[\"ticker\"]==ticker].copy()\n",
    "            st.caption(f\"Ticker: **{ticker}**  â€¢  Rows: {len(df)}\")\n",
    "        else:\n",
    "            ticker = None\n",
    "            st.caption(\"No 'ticker' column; using all rows.\")\n",
    "        if \"date\" in df.columns:\n",
    "            dmin, dmax = df[\"date\"].min(), df[\"date\"].max()\n",
    "            start, end = st.date_input(\"Date range\", value=(dmin.date(), dmax.date()),\n",
    "                                       min_value=dmin.date(), max_value=dmax.date())\n",
    "            df = df.loc[df[\"date\"].dt.date.between(start, end)].copy()\n",
    "    else:\n",
    "        ticker = st.text_input(\"Ticker\", value=(\"AAPL\" if asset_class==\"equity\" else \"BTC-USD\"))\n",
    "        dates = st.date_input(\"Fetch range (UTC)\", value=(pd.to_datetime(\"2023-01-01\").date(), pd.Timestamp.today().date()))\n",
    "        btn = st.button(\"Fetch data\")\n",
    "        if not btn:\n",
    "            st.stop()\n",
    "        try:\n",
    "            if asset_class==\"equity\":\n",
    "                df = fetch_equity_df(ticker, dates[0], dates[1])\n",
    "            else:\n",
    "                df = fetch_crypto_df(ticker, dates[0], dates[1])\n",
    "            st.success(f\"Fetched {len(df)} rows for {ticker}\")\n",
    "        except Exception as e:\n",
    "            st.error(f\"Fetch failed: {e}\"); st.stop()\n",
    "\n",
    "# ---- Artifacts ----\n",
    "feature_list, scaler, model, tau_art, tau_map = load_artifacts(asset_class=(\"equity\" if asset_class==\"equity\" else \"crypto\"))\n",
    "default_tau = float(tau_map.get(ticker, tau_art if tau_art is not None else DEFAULT_TAU)) if ticker else (tau_art or DEFAULT_TAU)\n",
    "\n",
    "# ---- Dataset ----\n",
    "X, y, retn, idx, used_cols = make_dataset(df, feature_list)\n",
    "if len(X)==0: st.error(\"No usable rows after feature alignment/NA drop.\"); st.stop()\n",
    "Xs = scaler.transform(X)\n",
    "p  = np.clip(predict_proba(model, Xs), 1e-6, 1-1e-6)\n",
    "\n",
    "with st.sidebar:\n",
    "    tau     = st.slider(\"Decision threshold (Ï„)\", 0.00, 1.00, value=float(round(default_tau,2)), step=0.01)\n",
    "    fee_bps = st.number_input(\"Fee (bps) per position flip\", value=5, min_value=0, max_value=100, step=1)\n",
    "\n",
    "# ---- Metrics ----\n",
    "c1,c2,c3,c4 = st.columns(4)\n",
    "if y is not None and len(y)==len(p):\n",
    "    m = metrics_all(y, p)\n",
    "    c1.metric(\"ROC AUC\", f\"{m['auc']:.3f}\" if np.isfinite(m['auc']) else \"n/a\")\n",
    "    c2.metric(\"PR AUC\",  f\"{m['ap']:.3f}\" if np.isfinite(m['ap']) else \"n/a\")\n",
    "    c3.metric(\"Brier\",    f\"{m['brier']:.4f}\" if np.isfinite(m['brier']) else \"n/a\")\n",
    "    c4.metric(\"Log Loss\", f\"{m['logloss']:.4f}\" if np.isfinite(m['logloss']) else \"n/a\")\n",
    "else:\n",
    "    for c in (c1,c2,c3,c4): c.metric(\"â€”\",\"â€”\")\n",
    "    st.info(\"Labels not available for this selection; showing predictions/equity only.\")\n",
    "\n",
    "# ---- Equity vs B&H ----\n",
    "sig = (p >= tau).astype(int)\n",
    "flips = np.zeros_like(sig)\n",
    "if len(flips)>1: flips[1:] = (sig[1:] != sig[:-1]).astype(int)\n",
    "fee = flips * (fee_bps/10000.0)\n",
    "eq  = np.cumprod(1 + (retn*sig - fee))\n",
    "bh  = np.cumprod(1 + retn)\n",
    "\n",
    "dates_axis = (df.iloc[idx][\"date\"].values if \"date\" in df.columns else df.index.values)\n",
    "st.subheader(\"Equity Curve vs. Buy & Hold\")\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(dates_axis, bh,  label=\"Buy & Hold\")\n",
    "ax.plot(dates_axis, eq,  label=f\"Strategy (Ï„={tau:.2f}, fee={fee_bps}bps)\")\n",
    "ax.set_xlabel(\"Date\" if \"date\" in df.columns else \"Index\"); ax.set_ylabel(\"Equity (Ã—)\")\n",
    "ax.legend(); st.pyplot(fig)\n",
    "\n",
    "# ---- Ï„-sweep ----\n",
    "with st.expander(\"Ï„-sweep (F1 & Final Equity)\"):\n",
    "    if y is not None and len(y)==len(p):\n",
    "        grid, f1s, finals = tau_sweep(y, p, retn, fee_bps=fee_bps)\n",
    "        best_f1_tau = float(grid[int(np.nanargmax(f1s))])\n",
    "        best_eq_tau = float(grid[int(np.nanargmax(finals))])\n",
    "        st.write({\"best_f1_tau\":best_f1_tau, \"best_final_equity_tau\":best_eq_tau})\n",
    "        f, axf = plt.subplots(); axf.plot(grid, f1s, label=\"F1 vs Ï„\"); axf.set_xlabel(\"Ï„\"); axf.set_ylabel(\"F1\"); axf.legend(); st.pyplot(f)\n",
    "    else:\n",
    "        st.info(\"Labels not available; Ï„-sweep (F1) disabled.\")\n",
    "\n",
    "# ---- Tail & CSV ----\n",
    "pred_df = pd.DataFrame({\"date\": dates_axis, \"proba\": p, \"signal\": sig})\n",
    "if \"close\" in df.columns: pred_df[\"close\"] = df.iloc[idx][\"close\"].values\n",
    "st.subheader(\"Latest predictions (tail)\")\n",
    "st.dataframe(pred_df.tail(min(12, len(pred_df))))\n",
    "st.download_button(\"Download predictions CSV\",\n",
    "    data=pred_df.to_csv(index=False).encode(\"utf-8\"),\n",
    "    file_name=\"predictions.csv\", mime=\"text/csv\")\n",
    "\n",
    "st.caption(\"This UI runs your trained LR on repo data or live Yahoo fetch. Crypto uses BTC-based proxies. Not financial advice.\")\n",
    "\"\"\").lstrip()\n",
    "out = (ROOT/\"app\"/\"streamlit_app.py\")\n",
    "out.write_text(code, encoding=\"utf-8\")\n",
    "print(\"Wrote:\", out.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "836cdb0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "app/__init__.py             : True\n",
      "app/config.py               : True\n",
      "app/lib_artifacts.py        : True\n",
      "app/lib_features.py         : True\n",
      "app/lib_fetch.py            : True\n",
      "app/lib_eval.py             : True\n",
      "app/streamlit_app.py        : True\n"
     ]
    }
   ],
   "source": [
    "# NB18 â€” Verify files\n",
    "targets = [\n",
    "    \"app/__init__.py\",\n",
    "    \"app/config.py\",\n",
    "    \"app/lib_artifacts.py\",\n",
    "    \"app/lib_features.py\",\n",
    "    \"app/lib_fetch.py\",\n",
    "    \"app/lib_eval.py\",\n",
    "    \"app/streamlit_app.py\",\n",
    "]\n",
    "for t in targets:\n",
    "    print(f\"{t:28s}:\", (ROOT / t).exists())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c8706a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Syntax OK\n"
     ]
    }
   ],
   "source": [
    "# NB18 â€” Syntax check modules & app\n",
    "for t in [\"app/config.py\",\"app/lib_artifacts.py\",\"app/lib_features.py\",\"app/lib_fetch.py\",\"app/lib_eval.py\",\"app/streamlit_app.py\"]:\n",
    "    src = (ROOT/t).read_text(encoding=\"utf-8\")\n",
    "    compile(src, str(ROOT/t), \"exec\")\n",
    "print(\"Syntax OK\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "216b5ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated: C:\\.projects\\stock-direction-ml\\stock-direction-ml\\notebooks\\requirements.txt\n",
      "streamlit\n",
      "scikit-learn==1.7.2\n",
      "pandas==2.3.2\n",
      "numpy==2.3.3\n",
      "joblib==1.5.2\n",
      "matplotlib==3.10.6\n",
      "pyarrow==21.0.0\n",
      "yfinance\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# NB18 â€” Ensure yfinance in requirements.txt\n",
    "req = (ROOT/\"requirements.txt\")\n",
    "lines = []\n",
    "if req.exists():\n",
    "    lines = [ln.strip() for ln in req.read_text(encoding=\"utf-8\").splitlines() if ln.strip()]\n",
    "if not any(ln.lower().startswith(\"yfinance\") for ln in lines):\n",
    "    lines.append(\"yfinance\")\n",
    "    req.write_text(\"\\n\".join(lines) + \"\\n\", encoding=\"utf-8\")\n",
    "print(\"Updated:\", req.resolve())\n",
    "print(req.read_text(encoding=\"utf-8\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3822331b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local:\n",
      "  streamlit run app/streamlit_app.py\n",
      "\n",
      "Cloud:\n",
      "  Main file path â†’ app/streamlit_app.py\n"
     ]
    }
   ],
   "source": [
    "# NB18 â€” How to run\n",
    "print(\"Local:\")\n",
    "print(\"  streamlit run app/streamlit_app.py\")\n",
    "print(\"\\nCloud:\")\n",
    "print(\"  Main file path â†’ app/streamlit_app.py\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
