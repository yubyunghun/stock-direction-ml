{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f7ce9f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repo root â†’ C:\\.projects\\stock-direction-ml\\notebooks\n",
      "Has data?       True\n",
      "Has artifacts?  True\n"
     ]
    }
   ],
   "source": [
    "# NB16 â€” Repo root autodetect (data/ + artifacts/ locator)\n",
    "from pathlib import Path\n",
    "\n",
    "def find_repo_root(start: Path, must_have=(\"data\", \"artifacts\")) -> Path:\n",
    "    cur = start.resolve()\n",
    "    for _ in range(6):\n",
    "        if all((cur / m).exists() for m in must_have):\n",
    "            return cur\n",
    "        cur = cur.parent\n",
    "    if start.name.lower() == \"notebooks\" and all((start.parent / m).exists() for m in must_have):\n",
    "        return start.parent.resolve()\n",
    "    raise FileNotFoundError(f\"Could not locate repo root containing {must_have} starting at {start}\")\n",
    "\n",
    "CWD = Path.cwd()\n",
    "ROOT = find_repo_root(CWD)\n",
    "print(\"Repo root â†’\", ROOT)\n",
    "print(\"Has data?      \", (ROOT/\"data\").exists())\n",
    "print(\"Has artifacts? \", (ROOT/\"artifacts\").exists())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ead4bbaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote (enhanced): C:\\.projects\\stock-direction-ml\\notebooks\\app\\streamlit_app.py\n"
     ]
    }
   ],
   "source": [
    "# NB16 â€” Write enhanced Streamlit app (cache + Ï„-sweep + CM + CSV download)\n",
    "from pathlib import Path\n",
    "\n",
    "APP_PATH = (ROOT / \"app\" / \"streamlit_app.py\")\n",
    "APP_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "APP_CODE = r'''\n",
    "# Streamlit demo (enhanced) â€” NB16 polish\n",
    "# Adds:\n",
    "# â€¢ Caching for data/artifacts\n",
    "# â€¢ Ï„-sweep with suggested threshold (maximize F1 & Final Equity)\n",
    "# â€¢ Confusion matrix at current Ï„\n",
    "# â€¢ Download CSV of predictions\n",
    "\n",
    "import os, json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, average_precision_score, brier_score_loss, log_loss,\n",
    "    roc_curve, precision_recall_curve, confusion_matrix, f1_score\n",
    ")\n",
    "from sklearn.calibration import calibration_curve\n",
    "import streamlit as st\n",
    "\n",
    "HERE = Path(__file__).resolve()\n",
    "ROOT = HERE.parent.parent  # this file lives in app/\n",
    "\n",
    "# ----------------------\n",
    "# Caching loaders\n",
    "# ----------------------\n",
    "@st.cache_data(show_spinner=False)\n",
    "def load_df(root: Path) -> pd.DataFrame:\n",
    "    data_dir = root / \"data\"\n",
    "    csv_path = data_dir / \"df_nb02.csv\"\n",
    "    pq_path  = data_dir / \"df_nb02.parquet\"\n",
    "    if csv_path.exists():\n",
    "        df = pd.read_csv(csv_path)\n",
    "    elif pq_path.exists():\n",
    "        df = pd.read_parquet(pq_path)\n",
    "    else:\n",
    "        raise FileNotFoundError(\"Missing data file: expected data/df_nb02.csv or data/df_nb02.parquet\")\n",
    "\n",
    "    for c in [\"date\", \"Date\", \"timestamp\", \"ts\"]:\n",
    "        if c in df.columns:\n",
    "            try:\n",
    "                df[c] = pd.to_datetime(df[c])\n",
    "            except Exception:\n",
    "                pass\n",
    "            if c != \"date\":\n",
    "                df[\"date\"] = df[c]\n",
    "            break\n",
    "    return df\n",
    "\n",
    "@st.cache_resource(show_spinner=False)\n",
    "def load_artifacts(root: Path):\n",
    "    art = root / \"artifacts\"\n",
    "    feature_list = json.load(open(art / \"feature_list.json\", \"r\", encoding=\"utf-8\"))\n",
    "    scaler = joblib.load(art / \"scaler.joblib\")\n",
    "    model  = joblib.load(art / \"lr.joblib\")\n",
    "    tau_val = None\n",
    "    tfile = art / \"threshold.json\"\n",
    "    if tfile.exists():\n",
    "        try:\n",
    "            t = json.load(open(tfile, \"r\", encoding=\"utf-8\"))\n",
    "            tau_val = t.get(\"tau\") or t.get(\"threshold\") or t.get(\"value\")\n",
    "        except Exception:\n",
    "            tau_val = None\n",
    "    return feature_list, scaler, model, tau_val\n",
    "\n",
    "def infer_target(df: pd.DataFrame):\n",
    "    for c in [\"y\", \"label\", \"target\", \"y_bin\", \"direction\", \"is_up\", \"class\", \"cls\"]:\n",
    "        if c in df.columns:\n",
    "            y = df[c].astype(int).clip(0, 1).values\n",
    "            return y, c\n",
    "    if \"ret_next\" in df.columns:\n",
    "        y = (df[\"ret_next\"].astype(float) > 0).astype(int).values\n",
    "        return y, \"ret_next>0\"\n",
    "    if \"close\" in df.columns:\n",
    "        ret_next = df[\"close\"].astype(float).pct_change().shift(-1).fillna(0.0)\n",
    "        df[\"ret_next\"] = ret_next\n",
    "        y = (ret_next > 0).astype(int).values\n",
    "        return y, \"ret_next_from_close>0\"\n",
    "    return None, None\n",
    "\n",
    "def make_dataset(df: pd.DataFrame, features: list):\n",
    "    cols = [c for c in features if c in df.columns]\n",
    "    if not cols:\n",
    "        raise ValueError(\"None of the expected features are present in df_nb02. Check artifacts/feature_list.json vs data columns.\")\n",
    "    tmp = df[cols].replace([np.inf, -np.inf], np.nan)\n",
    "    y_vals, y_name = infer_target(df)\n",
    "    if y_vals is None:\n",
    "        return None, None, None, None, None\n",
    "    if \"ret_next\" in df.columns:\n",
    "        retn = df[\"ret_next\"].astype(float).values\n",
    "    elif \"close\" in df.columns:\n",
    "        retn = df[\"close\"].astype(float).pct_change().shift(-1).fillna(0.0).values\n",
    "    else:\n",
    "        retn = np.zeros(len(df), dtype=float)\n",
    "    tmp[\"__y__\"] = y_vals\n",
    "    tmp[\"__ret_next__\"] = retn\n",
    "    tmp = tmp.dropna()\n",
    "    X  = tmp[cols].to_numpy()\n",
    "    y  = tmp[\"__y__\"].astype(int).to_numpy()\n",
    "    retn = tmp[\"__ret_next__\"].astype(float).to_numpy()\n",
    "    idx = tmp.index\n",
    "    return X, y, retn, idx, y_name\n",
    "\n",
    "def predict_proba(model, X: np.ndarray) -> np.ndarray:\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        p = model.predict_proba(X)\n",
    "        if p.ndim == 2 and p.shape[1] == 2: return p[:, 1]\n",
    "        if p.ndim == 1: return p\n",
    "    if hasattr(model, \"decision_function\"):\n",
    "        s = model.decision_function(X); return 1 / (1 + np.exp(-s))\n",
    "    pred = model.predict(X); return np.clip(pred.astype(float), 0.0, 1.0)\n",
    "\n",
    "# ----------------------\n",
    "# UI\n",
    "# ----------------------\n",
    "st.set_page_config(page_title=\"Direction Classifier Demo (NB16)\", layout=\"wide\")\n",
    "st.title(\"ðŸ“ˆ Direction Classifier â€” Streamlit Demo (NB16 polish)\")\n",
    "\n",
    "df = load_df(ROOT)\n",
    "feature_list, scaler, model, tau_art = load_artifacts(ROOT)\n",
    "\n",
    "with st.sidebar:\n",
    "    st.header(\"Settings\")\n",
    "    # Date filter\n",
    "    if \"date\" in df.columns:\n",
    "        dmin, dmax = df[\"date\"].min(), df[\"date\"].max()\n",
    "        start, end = st.date_input(\"Date range\", value=(dmin.date(), dmax.date()),\n",
    "                                   min_value=dmin.date(), max_value=dmax.date())\n",
    "        mask = df[\"date\"].dt.date.between(start, end)\n",
    "        df_view = df.loc[mask].copy()\n",
    "    else:\n",
    "        df_view = df.copy()\n",
    "        st.caption(\"No 'date' column found; using all rows.\")\n",
    "    # Ï„ + fees\n",
    "    default_tau = float(tau_art) if tau_art is not None else 0.59\n",
    "    tau = st.slider(\"Decision threshold (Ï„)\", 0.00, 1.00, value=float(round(default_tau, 2)), step=0.01)\n",
    "    fee_bps = st.number_input(\"Fee (bps) per position flip\", value=5, min_value=0, max_value=100, step=1)\n",
    "\n",
    "# Dataset\n",
    "try:\n",
    "    X, y, retn, idx, y_name = make_dataset(df_view, feature_list)\n",
    "except Exception as e:\n",
    "    st.error(str(e)); st.stop()\n",
    "if X is None:\n",
    "    st.error(\"Could not infer a binary target; need y/ret_next/close.\"); st.stop()\n",
    "\n",
    "Xs = scaler.transform(X)\n",
    "proba = np.clip(predict_proba(model, Xs), 1e-6, 1-1e-6)\n",
    "\n",
    "# ----------------------\n",
    "# Metric cards\n",
    "# ----------------------\n",
    "c1, c2, c3, c4 = st.columns(4)\n",
    "def safe(fn, *a):\n",
    "    try: return fn(*a)\n",
    "    except Exception: return float(\"nan\")\n",
    "\n",
    "auc   = safe(roc_auc_score, y, proba)\n",
    "ap    = safe(average_precision_score, y, proba)\n",
    "brier = safe(brier_score_loss, y, proba)\n",
    "ll    = safe(log_loss, y, proba)\n",
    "\n",
    "c1.metric(\"ROC AUC\", f\"{auc:.3f}\" if np.isfinite(auc) else \"n/a\")\n",
    "c2.metric(\"PR AUC\",  f\"{ap:.3f}\" if np.isfinite(ap) else \"n/a\")\n",
    "c3.metric(\"Brier\",    f\"{brier:.4f}\" if np.isfinite(brier) else \"n/a\")\n",
    "c4.metric(\"Log Loss\", f\"{ll:.4f}\" if np.isfinite(ll) else \"n/a\")\n",
    "\n",
    "# ----------------------\n",
    "# Ï„-sweep (suggest a tau)\n",
    "# ----------------------\n",
    "with st.expander(\"Ï„-sweep (threshold selection)\"):\n",
    "    grid = np.linspace(0.05, 0.95, 91)\n",
    "    f1s = []\n",
    "    finals = []\n",
    "    for t in grid:\n",
    "        sig_t = (proba >= t).astype(int)\n",
    "        f1s.append(safe(f1_score, y, sig_t))\n",
    "        flips = np.zeros_like(sig_t)\n",
    "        if len(flips) > 1: flips[1:] = (sig_t[1:] != sig_t[:-1]).astype(int)\n",
    "        fee = flips * (fee_bps / 10000.0)\n",
    "        strat = (retn * sig_t) - fee\n",
    "        finals.append(np.cumprod(1 + strat)[-1])\n",
    "    df_sweep = pd.DataFrame({\"tau\": grid, \"f1\": f1s, \"final_equity\": finals})\n",
    "    best_f1_row = df_sweep.iloc[df_sweep[\"f1\"].idxmax()]\n",
    "    best_eq_row = df_sweep.iloc[df_sweep[\"final_equity\"].idxmax()]\n",
    "    st.write(\"Best by **F1**:\", {k: (float(v) if isinstance(v, (np.floating, np.integer)) else v) for k,v in best_f1_row.to_dict().items()})\n",
    "    st.write(\"Best by **Final Equity**:\", {k: (float(v) if isinstance(v, (np.floating, np.integer)) else v) for k,v in best_eq_row.to_dict().items()})\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(df_sweep[\"tau\"], df_sweep[\"f1\"], label=\"F1 vs Ï„\")\n",
    "    ax.set_xlabel(\"Ï„\"); ax.set_ylabel(\"F1\"); ax.legend()\n",
    "    st.pyplot(fig)\n",
    "\n",
    "# ----------------------\n",
    "# Equity vs B&H at current Ï„\n",
    "# ----------------------\n",
    "sig = (proba >= tau).astype(int)\n",
    "flips = np.zeros_like(sig)\n",
    "if len(flips) > 1: flips[1:] = (sig[1:] != sig[:-1]).astype(int)\n",
    "fee = flips * (fee_bps / 10000.0)\n",
    "strategy_ret = (retn * sig) - fee\n",
    "eq_strategy = np.cumprod(1.0 + strategy_ret)\n",
    "eq_bh = np.cumprod(1.0 + retn)\n",
    "\n",
    "dates = df_view.loc[idx][\"date\"].values if \"date\" in df_view.columns else df_view.index.values\n",
    "st.subheader(\"Equity Curve vs. Buy & Hold\")\n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.plot(dates, eq_bh, label=\"Buy & Hold\")\n",
    "ax1.plot(dates, eq_strategy, label=f\"Strategy (Ï„={tau:.2f}, fee={fee_bps}bps)\")\n",
    "ax1.set_xlabel(\"Date\" if \"date\" in df_view.columns else \"Index\")\n",
    "ax1.set_ylabel(\"Equity (Ã—)\")\n",
    "ax1.legend()\n",
    "st.pyplot(fig1)\n",
    "\n",
    "# ----------------------\n",
    "# ROC / PR / Calibration\n",
    "# ----------------------\n",
    "st.subheader(\"ROC, PR, Calibration\")\n",
    "fpr, tpr, _ = roc_curve(y, proba)\n",
    "fig2, ax2 = plt.subplots(); ax2.plot(fpr, tpr, label=f\"AUC={auc:.3f}\" if np.isfinite(auc) else \"AUC=n/a\")\n",
    "ax2.plot([0,1],[0,1],\"--\"); ax2.set_xlabel(\"FPR\"); ax2.set_ylabel(\"TPR\"); ax2.legend()\n",
    "st.pyplot(fig2)\n",
    "\n",
    "prec, rec, _ = precision_recall_curve(y, proba)\n",
    "fig3, ax3 = plt.subplots(); ax3.plot(rec, prec, label=f\"AP={ap:.3f}\" if np.isfinite(ap) else \"AP=n/a\")\n",
    "ax3.set_xlabel(\"Recall\"); ax3.set_ylabel(\"Precision\"); ax3.legend()\n",
    "st.pyplot(fig3)\n",
    "\n",
    "prob_true, prob_pred = calibration_curve(y, proba, n_bins=10, strategy=\"uniform\")\n",
    "fig4, ax4 = plt.subplots(); ax4.plot(prob_pred, prob_true, \"o-\", label=\"Model\")\n",
    "ax4.plot([0,1],[0,1],\"--\"); ax4.set_xlabel(\"Predicted probability\"); ax4.set_ylabel(\"Observed frequency\"); ax4.legend()\n",
    "st.pyplot(fig4)\n",
    "\n",
    "# ----------------------\n",
    "# Confusion matrix + CSV download\n",
    "# ----------------------\n",
    "st.subheader(\"Confusion matrix @ current Ï„\")\n",
    "cm = confusion_matrix(y, sig, labels=[0,1])\n",
    "cm_df = pd.DataFrame(cm, index=[\"True 0\",\"True 1\"], columns=[\"Pred 0\",\"Pred 1\"])\n",
    "st.dataframe(cm_df)\n",
    "\n",
    "pred_df = pd.DataFrame({\n",
    "    \"date\": dates,\n",
    "    \"proba\": proba,\n",
    "    \"signal\": sig,\n",
    "})\n",
    "if \"close\" in df_view.columns:\n",
    "    pred_df[\"close\"] = df_view.loc[idx][\"close\"].values\n",
    "\n",
    "st.download_button(\n",
    "    \"Download predictions CSV\",\n",
    "    data=pred_df.to_csv(index=False).encode(\"utf-8\"),\n",
    "    file_name=\"predictions_nb16.csv\",\n",
    "    mime=\"text/csv\"\n",
    ")\n",
    "\n",
    "st.subheader(\"Latest predictions (tail)\")\n",
    "tail_n = min(12, len(proba))\n",
    "st.dataframe(pred_df.tail(tail_n))\n",
    "st.caption(\"Signals are long-only (1=long, 0=cash); flips incur fee in equity curve.\")\n",
    "'''\n",
    "\n",
    "APP_PATH.write_text(APP_CODE, encoding=\"utf-8\")\n",
    "print(\"Wrote (enhanced):\", APP_PATH.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4ca956d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "app/streamlit_app.py: True\n"
     ]
    }
   ],
   "source": [
    "# NB16 â€” Verify enhanced app exists\n",
    "from pathlib import Path\n",
    "print(\"app/streamlit_app.py:\", (ROOT/\"app/streamlit_app.py\").exists())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89bb5283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Syntax OK\n"
     ]
    }
   ],
   "source": [
    "# NB16 â€” Parse the generated app to catch syntax errors early\n",
    "src = (ROOT/\"app/streamlit_app.py\").read_text(encoding=\"utf-8\")\n",
    "compile(src, str(ROOT/\"app/streamlit_app.py\"), \"exec\")\n",
    "print(\"Syntax OK\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2c06a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows used: 2686 | features: 16 | tau: 0.59\n",
      "ROC AUC: 0.5214842239444841\n",
      "PR AUC : 0.5499732888657342\n",
      "Brier  : 0.25037234088911753\n",
      "LogLoss: 0.6938680024259393\n"
     ]
    }
   ],
   "source": [
    "# NB16 â€” Smoke test: artifacts + probabilities\n",
    "import json, numpy as np, pandas as pd, joblib\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, brier_score_loss, log_loss\n",
    "\n",
    "# Data\n",
    "if (ROOT/\"data/df_nb02.csv\").exists():\n",
    "    df = pd.read_csv(ROOT/\"data/df_nb02.csv\")\n",
    "elif (ROOT/\"data/df_nb02.parquet\").exists():\n",
    "    df = pd.read_parquet(ROOT/\"data/df_nb02.parquet\")\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Missing df_nb02.* in {ROOT/'data'}\")\n",
    "\n",
    "# Target\n",
    "if \"y\" in df.columns:\n",
    "    y = df[\"y\"].astype(int).clip(0,1).values\n",
    "elif \"ret_next\" in df.columns:\n",
    "    y = (df[\"ret_next\"].astype(float) > 0).astype(int).values\n",
    "elif \"close\" in df.columns:\n",
    "    rn = df[\"close\"].astype(float).pct_change().shift(-1).fillna(0.0)\n",
    "    df[\"ret_next\"] = rn\n",
    "    y = (rn > 0).astype(int).values\n",
    "else:\n",
    "    raise ValueError(\"No target inferable (need y/ret_next/close).\")\n",
    "\n",
    "retn = df[\"ret_next\"].astype(float).values if \"ret_next\" in df.columns else np.zeros(len(df))\n",
    "\n",
    "# Artifacts\n",
    "feature_list = json.load(open(ROOT/\"artifacts/feature_list.json\",\"r\",encoding=\"utf-8\"))\n",
    "scaler = joblib.load(ROOT/\"artifacts/scaler.joblib\")\n",
    "model  = joblib.load(ROOT/\"artifacts/lr.joblib\")\n",
    "tau = 0.59\n",
    "tfile = ROOT/\"artifacts/threshold.json\"\n",
    "if tfile.exists():\n",
    "    t = json.load(open(tfile,\"r\",encoding=\"utf-8\"))\n",
    "    tau = float(t.get(\"tau\") or t.get(\"threshold\") or t.get(\"value\") or 0.59)\n",
    "\n",
    "# Align features\n",
    "cols = [c for c in feature_list if c in df.columns]\n",
    "if not cols:\n",
    "    raise ValueError(\"No overlap between feature_list.json and df columns.\")\n",
    "X = df[cols].replace([np.inf, -np.inf], np.nan).dropna()\n",
    "mask = X.index\n",
    "y = np.asarray(y)[mask]\n",
    "retn = np.asarray(retn)[mask]\n",
    "\n",
    "# Probabilities\n",
    "Xs = scaler.transform(X.values)\n",
    "if hasattr(model, \"predict_proba\"):\n",
    "    p = model.predict_proba(Xs); p = p[:,1] if p.ndim==2 else p\n",
    "elif hasattr(model, \"decision_function\"):\n",
    "    s = model.decision_function(Xs); p = 1/(1+np.exp(-s))\n",
    "else:\n",
    "    p = np.clip(model.predict(Xs).astype(float), 0, 1)\n",
    "p = np.clip(p, 1e-6, 1-1e-6)\n",
    "\n",
    "def safe(fn, *a):\n",
    "    try: return fn(*a)\n",
    "    except Exception: return float(\"nan\")\n",
    "\n",
    "print(\"Rows used:\", len(p), \"| features:\", len(cols), \"| tau:\", tau)\n",
    "print(\"ROC AUC:\", safe(roc_auc_score, y, p))\n",
    "print(\"PR AUC :\", safe(average_precision_score, y, p))\n",
    "print(\"Brier  :\", safe(brier_score_loss, y, p))\n",
    "print(\"LogLoss:\", safe(log_loss, y, p))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae9303fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suggested Ï„ by F1: 0.14999999999999997\n",
      "Suggested Ï„ by Final Equity: 0.4499999999999999\n"
     ]
    }
   ],
   "source": [
    "# NB16 â€” Ï„-sweep headless summary (F1 and Final Equity)\n",
    "import numpy as np\n",
    "\n",
    "grid = np.linspace(0.05, 0.95, 91)\n",
    "def f1_safe(y_true, y_hat):\n",
    "    from sklearn.metrics import f1_score\n",
    "    try: return f1_score(y_true, y_hat)\n",
    "    except Exception: return float(\"nan\")\n",
    "\n",
    "f1s, finals = [], []\n",
    "for t in grid:\n",
    "    sig_t = (p >= t).astype(int)\n",
    "    f1s.append(f1_safe(y, sig_t))\n",
    "    flips = np.zeros_like(sig_t)\n",
    "    if len(flips) > 1: flips[1:] = (sig_t[1:] != sig_t[:-1]).astype(int)\n",
    "    fee = flips * (5 / 10000.0)\n",
    "    strat = (retn * sig_t) - fee\n",
    "    finals.append(np.cumprod(1 + strat)[-1])\n",
    "\n",
    "best_f1_tau = float(grid[int(np.nanargmax(f1s))])\n",
    "best_eq_tau = float(grid[int(np.nanargmax(finals))])\n",
    "print(\"Suggested Ï„ by F1:\", best_f1_tau)\n",
    "print(\"Suggested Ï„ by Final Equity:\", best_eq_tau)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0bd9bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From REPO ROOT, run:\n",
      "  streamlit run app/streamlit_app.py\n",
      "\n",
      "New features: caching, Ï„-sweep, confusion matrix, CSV download\n"
     ]
    }
   ],
   "source": [
    "# NB16 â€” How to launch the Streamlit app\n",
    "print(\"From REPO ROOT, run:\")\n",
    "print(\"  streamlit run app/streamlit_app.py\")\n",
    "print(\"\\nNew features: caching, Ï„-sweep, confusion matrix, CSV download\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6af9389e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pin a port:\n",
      "  streamlit run app/streamlit_app.py --server.port=8501\n",
      "\n",
      "If running from notebooks/ directory, prefix with .. (repo root):\n",
      "  cd .. && streamlit run app/streamlit_app.py\n"
     ]
    }
   ],
   "source": [
    "# NB16 â€” Optional run tips\n",
    "print(\"Pin a port:\")\n",
    "print(\"  streamlit run app/streamlit_app.py --server.port=8501\")\n",
    "print(\"\\nIf running from notebooks/ directory, prefix with .. (repo root):\")\n",
    "print(\"  cd .. && streamlit run app/streamlit_app.py\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
