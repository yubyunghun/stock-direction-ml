{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0abadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Imports & I/O setup ---\n",
    "import os, json, pathlib, textwrap\n",
    "from datetime import datetime\n",
    "\n",
    "import sys, os\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (roc_auc_score, average_precision_score,\n",
    "                             brier_score_loss, log_loss, roc_curve,\n",
    "                             precision_recall_curve, confusion_matrix,\n",
    "                             f1_score)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import joblib\n",
    "\n",
    "# xgboost is optional; notebook will still run without it\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    HAVE_XGB = True\n",
    "except Exception:\n",
    "    HAVE_XGB = False\n",
    "\n",
    "# Paths\n",
    "os.makedirs(\"artifacts\", exist_ok=True)\n",
    "os.makedirs(\"reports/figures\", exist_ok=True)\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "# Repro\n",
    "RNG = 42\n",
    "np.random.seed(RNG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b6d7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load dataset & feature list (CSV- or Parquet-aware) ---\n",
    "storage_fmt = {\"format\": \"csv\"}\n",
    "sf_path = pathlib.Path(\"data/storage_format.json\")\n",
    "if sf_path.exists():\n",
    "    storage_fmt = json.loads(sf_path.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "fmt = storage_fmt.get(\"format\", \"csv\").lower()\n",
    "\n",
    "if fmt == \"parquet\":\n",
    "    try:\n",
    "        df = pd.read_parquet(\"data/df_nb02.parquet\")\n",
    "    except Exception as e:\n",
    "        print(\"Parquet not available, falling back to CSV:\", e)\n",
    "        df = pd.read_csv(\"data/df_nb02.csv\", parse_dates=[\"date\"])\n",
    "else:\n",
    "    df = pd.read_csv(\"data/df_nb02.csv\", parse_dates=[\"date\"])\n",
    "\n",
    "# Feature list (from NB2)\n",
    "feat_cols = json.loads(open(\"artifacts/feature_list.json\", \"r\", encoding=\"utf-8\").read())\n",
    "\n",
    "# Make sure features exist (take intersection)\n",
    "feat_cols = [c for c in feat_cols if c in df.columns]\n",
    "\n",
    "# Params (carried over for the model card)\n",
    "TICKER = df.get(\"ticker\", pd.Series([\"UNK\"]*len(df))).iloc[0] if \"ticker\" in df.columns else \"UNK\"\n",
    "START  = str(df[\"date\"].min().date())\n",
    "END    = str(df[\"date\"].max().date())\n",
    "TAU    = float(json.loads(open(\"data/label_params.json\",\"r\").read())[\"tau\"]) if pathlib.Path(\"data/label_params.json\").exists() else None\n",
    "DEAD_ZONE = bool(json.loads(open(\"data/label_params.json\",\"r\").read())[\"dead_zone\"]) if pathlib.Path(\"data/label_params.json\").exists() else None\n",
    "\n",
    "len(df), len(feat_cols), df[[\"date\"]+feat_cols[:5]].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d123e0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Time-based split: 60% train, 20% val, 20% test by date order ---\n",
    "df = df.sort_values(\"date\").reset_index(drop=True)\n",
    "\n",
    "n = len(df)\n",
    "n_train = int(0.6 * n)\n",
    "n_val   = int(0.2 * n)\n",
    "n_test  = n - n_train - n_val\n",
    "\n",
    "df_tr = df.iloc[:n_train].copy()\n",
    "df_va = df.iloc[n_train:n_train+n_val].copy()\n",
    "df_te = df.iloc[n_train+n_val:].copy()\n",
    "\n",
    "X_tr, y_tr = df_tr[feat_cols].values, df_tr[\"y\"].values\n",
    "X_va, y_va = df_va[feat_cols].values, df_va[\"y\"].values\n",
    "X_te, y_te = df_te[feat_cols].values, df_te[\"y\"].values\n",
    "\n",
    "# standardize (fit on train only)\n",
    "scaler = StandardScaler().fit(X_tr)\n",
    "X_tr_s = scaler.transform(X_tr)\n",
    "X_va_s = scaler.transform(X_va)\n",
    "X_te_s = scaler.transform(X_te)\n",
    "\n",
    "print(df_tr.shape, df_va.shape, df_te.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269aa273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Train models ---\n",
    "lr = LogisticRegression(max_iter=200, n_jobs=None, solver=\"lbfgs\", random_state=RNG)\n",
    "lr.fit(X_tr_s, y_tr)\n",
    "\n",
    "if HAVE_XGB:\n",
    "    xgb = XGBClassifier(\n",
    "        n_estimators=300,\n",
    "        max_depth=3,\n",
    "        learning_rate=0.05,\n",
    "        subsample=0.9,\n",
    "        colsample_bytree=0.9,\n",
    "        reg_lambda=1.0,\n",
    "        objective=\"binary:logistic\",\n",
    "        random_state=RNG,\n",
    "        eval_metric=\"logloss\",\n",
    "        n_jobs=0\n",
    "    )\n",
    "    xgb.fit(X_tr, y_tr)  # XGB often does better without standardization\n",
    "else:\n",
    "    xgb = None\n",
    "\n",
    "print(\"Trained LR\", \"| XGB:\", HAVE_XGB)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544e6a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Validation metrics to choose threshold later ---\n",
    "def metric_block(y_true, y_prob):\n",
    "    p = np.clip(np.asarray(y_prob), 1e-15, 1 - 1e-15)\n",
    "    return {\n",
    "        \"AUC\": roc_auc_score(y_true, p),\n",
    "        \"PR_AUC\": average_precision_score(y_true, p),\n",
    "        \"Brier\": brier_score_loss(y_true, p),\n",
    "        \"LogLoss\": log_loss(y_true, p)\n",
    "    }\n",
    "\n",
    "preds = {}\n",
    "preds[\"LR\"] = {\n",
    "    \"va\": lr.predict_proba(X_va_s)[:, 1],\n",
    "    \"te\": lr.predict_proba(X_te_s)[:, 1],\n",
    "}\n",
    "\n",
    "if HAVE_XGB:\n",
    "    preds[\"XGB\"] = {\n",
    "        \"va\": xgb.predict_proba(X_va)[:, 1],\n",
    "        \"te\": xgb.predict_proba(X_te)[:, 1],\n",
    "}\n",
    "\n",
    "rows = []\n",
    "for model_name, d in preds.items():\n",
    "    m_va = metric_block(y_va, d[\"va\"])\n",
    "    m_te = metric_block(y_te, d[\"te\"])\n",
    "    rows.append({\"model\": model_name, \"split\": \"val\", **m_va, \"PosRate\": float(np.mean(y_va))})\n",
    "    rows.append({\"model\": model_name, \"split\": \"test\", **m_te, \"PosRate\": float(np.mean(y_te))})\n",
    "\n",
    "summary = pd.DataFrame(rows)\n",
    "summary.to_csv(\"data/explainability_summary.csv\", index=False)\n",
    "summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c40200a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- ROC & PR curves (LR only for plots to keep it simple) ---\n",
    "fpr, tpr, _ = roc_curve(y_te, preds[\"LR\"][\"te\"])\n",
    "prec, rec, _ = precision_recall_curve(y_te, preds[\"LR\"][\"te\"])\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label=\"LR\")\n",
    "plt.plot([0,1],[0,1], linestyle=\"--\")\n",
    "plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\"); plt.title(\"ROC — LR (TEST)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"reports/figures/roc_nb05.png\", dpi=140)\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(rec, prec, label=\"LR\")\n",
    "plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\"); plt.title(\"PR — LR (TEST)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"reports/figures/pr_nb05.png\", dpi=140)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2d73a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Reliability (Calibration) curve & Brier ---\n",
    "bins = np.linspace(0,1,11)\n",
    "binned = pd.cut(preds[\"LR\"][\"te\"], bins=bins, include_lowest=True)\n",
    "cal = df_te.assign(prob=preds[\"LR\"][\"te\"]).groupby(binned).agg(\n",
    "    mean_prob=(\"prob\",\"mean\"), mean_true=(\"y\",\"mean\"), count=(\"y\",\"size\")\n",
    ").dropna()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot([0,1],[0,1], linestyle=\"--\")\n",
    "plt.scatter(cal[\"mean_prob\"], cal[\"mean_true\"], s=np.clip(cal[\"count\"], 10, 80))\n",
    "plt.xlabel(\"Predicted probability\")\n",
    "plt.ylabel(\"Empirical frequency\")\n",
    "plt.title(\"Reliability — LR (TEST)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"reports/figures/reliability_lr_nb05.png\", dpi=140)\n",
    "plt.show()\n",
    "\n",
    "print(\"Brier (LR): \", brier_score_loss(y_te, preds[\"LR\"][\"te\"]))\n",
    "if HAVE_XGB:\n",
    "    print(\"Brier (XGB):\", brier_score_loss(y_te, preds[\"XGB\"][\"te\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181231c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Choose threshold on validation to maximize F1, then evaluate on TEST ---\n",
    "ths = np.linspace(0.05, 0.95, 181)\n",
    "f1s = [f1_score(y_va, (preds[\"LR\"][\"va\"] >= t).astype(int)) for t in ths]\n",
    "t_star = float(ths[int(np.argmax(f1s))])\n",
    "\n",
    "yhat_test = (preds[\"LR\"][\"te\"] >= t_star).astype(int)\n",
    "cm = pd.DataFrame(confusion_matrix(y_te, yhat_test),\n",
    "                  index=pd.Index([0,1], name=\"Actual\"),\n",
    "                  columns=pd.Index([0,1], name=\"Pred\"))\n",
    "print(f\"LR threshold* (val max-F1): {t_star:.3f}\")\n",
    "cm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf44054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Stability by year (TEST) & LR coefficients ---\n",
    "df_te_eval = df_te.copy()\n",
    "df_te_eval[\"prob_lr\"] = preds[\"LR\"][\"te\"]\n",
    "df_te_eval[\"year\"] = pd.to_datetime(df_te_eval[\"date\"]).dt.year\n",
    "\n",
    "by_year = df_te_eval.groupby(\"year\").apply(\n",
    "    lambda g: pd.Series({\n",
    "        \"AUC\": roc_auc_score(g[\"y\"], g[\"prob_lr\"]) if g[\"y\"].nunique() > 1 else np.nan,\n",
    "        \"Brier\": brier_score_loss(g[\"y\"], g[\"prob_lr\"]),\n",
    "        \"PosRate\": float(g[\"y\"].mean())\n",
    "    })\n",
    ")\n",
    "by_year.to_csv(\"data/metrics_by_year_LR.csv\")\n",
    "by_year\n",
    "\n",
    "# LR \"importance\" = absolute coefficient magnitude\n",
    "coef = pd.Series(lr.coef_.ravel(), index=feat_cols).sort_values(key=np.abs, ascending=False)\n",
    "imp_tbl = (pd.DataFrame({\"feature\": coef.index, \"coef\": coef.values})\n",
    "             .assign(abs_coef=lambda d: d[\"coef\"].abs()))\n",
    "topk = imp_tbl.head(20).reset_index(drop=True)\n",
    "topk.to_csv(\"data/lr_importance_topk.csv\", index=False)\n",
    "\n",
    "plt.figure()\n",
    "plt.barh(topk[\"feature\"][::-1], topk[\"abs_coef\"][::-1])\n",
    "plt.xlabel(\"|Coefficient|\")\n",
    "plt.title(\"LR Feature Importance (Top 20)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"reports/figures/lr_feature_importance_topk.png\", dpi=140)\n",
    "plt.show()\n",
    "\n",
    "imp_tbl.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fccd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Backtest: long-only when prob >= t_star ---\n",
    "df_bt = df_te_eval.copy()\n",
    "# assume df has a next-day return column 'ret_next'; if not, compute from 'close'\n",
    "if \"ret_next\" not in df_bt.columns:\n",
    "    if \"close\" in df_bt.columns:\n",
    "        df_bt[\"ret_next\"] = df_bt[\"close\"].pct_change().shift(-1)\n",
    "    else:\n",
    "        raise ValueError(\"No 'ret_next' or 'close' column found for backtest.\")\n",
    "\n",
    "df_bt[\"signal\"] = (df_bt[\"prob_lr\"] >= t_star).astype(int)\n",
    "df_bt[\"strategy_ret\"] = df_bt[\"signal\"] * df_bt[\"ret_next\"]\n",
    "df_bt[\"equity\"] = (1 + df_bt[\"strategy_ret\"].fillna(0)).cumprod()\n",
    "\n",
    "# KPIs\n",
    "def kpis(ret_series, freq=252):\n",
    "    r = ret_series.fillna(0)\n",
    "    cagr = (1 + r).prod() ** (freq / max(len(r),1)) - 1\n",
    "    vol  = r.std() * np.sqrt(freq)\n",
    "    sharpe = cagr / vol if vol > 0 else np.nan\n",
    "    equity = (1 + r).cumprod()\n",
    "    maxdd = (equity / equity.cummax() - 1).min()\n",
    "    hit = (r > 0).mean()\n",
    "    turn = np.mean(np.abs(np.diff((r!=0).astype(int)))) * freq if len(r)>1 else 0\n",
    "    return pd.Series({\"CAGR\":cagr, \"Vol\":vol, \"Sharpe\":sharpe, \"MaxDD\":maxdd, \"HitRate\":hit, \"TurnoverYr\":turn})\n",
    "\n",
    "bt_stats = kpis(df_bt[\"strategy_ret\"])\n",
    "plt.figure()\n",
    "plt.plot(df_bt[\"date\"], df_bt[\"equity\"])\n",
    "plt.title(\"Equity Curve — LR thresholded\")\n",
    "plt.xlabel(\"Date\"); plt.ylabel(\"Equity\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"reports/figures/equity_curve_lr.png\", dpi=140)\n",
    "plt.show()\n",
    "\n",
    "bt_stats.round(4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8254c7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Save artifacts ---\n",
    "joblib.dump(scaler, \"artifacts/scaler.joblib\")\n",
    "joblib.dump(lr, \"artifacts/lr.joblib\")\n",
    "with open(\"artifacts/threshold.json\",\"w\",encoding=\"utf-8\") as f:\n",
    "    json.dump({\"threshold\": t_star}, f, indent=2)\n",
    "print(\"Saved scaler, lr, and threshold.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abd7c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Model Card: write 'reports/model_card.md' (UTC-aware, no tabulate dep) ---\n",
    "from datetime import datetime, UTC\n",
    "import json, pathlib, pandas as pd\n",
    "\n",
    "def df_to_md(tbl: pd.DataFrame, index=False, floatfmt=\".4f\"):\n",
    "    \"\"\"Minimal DataFrame -> GitHub-style markdown table (no external deps).\"\"\"\n",
    "    df2 = tbl.copy()\n",
    "    if not index and df2.index.name is not None:\n",
    "        df2 = df2.reset_index()\n",
    "\n",
    "    # Format floats\n",
    "    for col in df2.columns:\n",
    "        if pd.api.types.is_float_dtype(df2[col]):\n",
    "            df2[col] = df2[col].map(lambda x: f\"{x:{floatfmt}}\" if pd.notnull(x) else \"\")\n",
    "\n",
    "    headers = list(map(str, df2.columns))\n",
    "    lines = []\n",
    "    lines.append(\"| \" + \" | \".join(headers) + \" |\")\n",
    "    lines.append(\"| \" + \" | \".join([\"---\"] * len(headers)) + \" |\")\n",
    "    for _, row in df2.iterrows():\n",
    "        lines.append(\"| \" + \" | \".join(map(str, row.values)) + \" |\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "# Load small tables for the card\n",
    "card_tbl   = pd.read_csv(\"data/explainability_summary.csv\")                  # overall test metrics by model\n",
    "by_year_tbl = pd.read_csv(\"data/metrics_by_year_LR.csv\")                     # yearly stability (already has 'year' col)\n",
    "\n",
    "# Threshold used\n",
    "t_star = json.load(open(\"artifacts/threshold.json\", \"r\", encoding=\"utf-8\"))[\"threshold\"]\n",
    "\n",
    "# UTC-aware date string\n",
    "today_utc = datetime.now(UTC).date().isoformat()\n",
    "\n",
    "# Compose model card\n",
    "card = f\"\"\"# Model Card — Stock Direction (LogReg)\n",
    "**Date:** {today_utc}\n",
    "**Ticker:** {TICKER} | **Period:** {START} → {END}\n",
    "**Labeling:** tau={TAU}, dead_zone={DEAD_ZONE}\n",
    "**Features ({len(feat_cols)}):** {', '.join(feat_cols[:12])}{' ...' if len(feat_cols)>12 else ''}\n",
    "\n",
    "## Metrics (Val/Test summary)\n",
    "{df_to_md(card_tbl, index=False, floatfmt=\".4f\")}\n",
    "\n",
    "**Val-chosen threshold (max-F1):** {t_star:.3f}\n",
    "\n",
    "## Stability (test by year)\n",
    "{df_to_md(by_year_tbl, index=False, floatfmt=\".4f\")}\n",
    "\n",
    "## Artifacts\n",
    "- `artifacts/scaler.joblib`, `artifacts/lr.joblib`, `artifacts/threshold.json`, `artifacts/feature_list.json`\n",
    "- Curves: `reports/figures/roc_nb05.png`, `pr_nb05.png`, `reliability_lr_nb05.png`, `equity_curve_lr.png`\n",
    "- Importance: `reports/figures/lr_feature_importance_topk.png`\n",
    "\"\"\"\n",
    "\n",
    "# Write with UTF-8 (avoids Windows cp1252 issues)\n",
    "pathlib.Path(\"reports/model_card.md\").write_text(card, encoding=\"utf-8\")\n",
    "print(\"Wrote reports/model_card.md\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23053fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Quick check of outputs ---\n",
    "print(\"Artifacts:\", list(pathlib.Path(\"artifacts\").glob(\"*\")))\n",
    "print(\"Figures:\",  list(pathlib.Path(\"reports/figures\").glob(\"*.png\")))\n",
    "print(\"Data:\",     [p for p in pathlib.Path(\"data\").glob(\"*\") if p.suffix in {'.csv','.parquet','.json'}])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e377dcea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Compute & save metrics by calendar year on the TEST split (LogReg) ---\n",
    "\n",
    "import json, pathlib\n",
    "import numpy as np, pandas as pd\n",
    "from sklearn.metrics import roc_auc_score, brier_score_loss\n",
    "from joblib import load\n",
    "\n",
    "# 1) Load dataset + artifacts\n",
    "data_path = \"data/df_nb02.parquet\" if pathlib.Path(\"data/df_nb02.parquet\").exists() else \"data/df_nb02.csv\"\n",
    "df = pd.read_parquet(data_path) if data_path.endswith(\".parquet\") else pd.read_csv(data_path)\n",
    "\n",
    "feat_cols = json.load(open(\"artifacts/feature_list.json\",\"r\",encoding=\"utf-8\"))\n",
    "scaler   = load(\"artifacts/scaler.joblib\")\n",
    "lr       = load(\"artifacts/lr.joblib\")\n",
    "\n",
    "# 2) Chronological split (same 70/15/15 we used earlier)\n",
    "df = df.sort_values(\"date\").reset_index(drop=True)\n",
    "n   = len(df)\n",
    "i_tr = int(0.70*n)\n",
    "i_va = int(0.85*n)\n",
    "df_te = df.iloc[i_va:].copy()\n",
    "\n",
    "X_te = scaler.transform(df_te[feat_cols].to_numpy())\n",
    "y_te = df_te[\"y\"].to_numpy().astype(int)\n",
    "p_te = lr.predict_proba(X_te)[:,1]\n",
    "\n",
    "# 3) Group by year and compute metrics\n",
    "df_te[\"year\"] = pd.to_datetime(df_te[\"date\"]).dt.year\n",
    "\n",
    "rows = []\n",
    "for yr, g in df_te.groupby(\"year\"):\n",
    "    y = g[\"y\"].astype(int).to_numpy()\n",
    "    p = p_te[g.index - df_te.index[0]]  # align indices to p_te slice\n",
    "\n",
    "    # Metrics (handle degenerate cases)\n",
    "    try:\n",
    "        auc = roc_auc_score(y, p) if len(np.unique(y)) > 1 else np.nan\n",
    "    except Exception:\n",
    "        auc = np.nan\n",
    "    brier = brier_score_loss(y, p)\n",
    "    pos   = float(y.mean())\n",
    "\n",
    "    rows.append({\"year\": int(yr), \"AUC\": auc, \"Brier\": brier, \"PosRate\": pos})\n",
    "\n",
    "by_year = pd.DataFrame(rows).sort_values(\"year\")\n",
    "pathlib.Path(\"data\").mkdir(exist_ok=True, parents=True)\n",
    "by_year.to_csv(\"data/metrics_by_year_LR.csv\", index=False)\n",
    "print(\"Wrote data/metrics_by_year_LR.csv\")\n",
    "by_year\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.13 (.venv stock-direction-ml)",
   "language": "python",
   "name": "stock-direction-ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
