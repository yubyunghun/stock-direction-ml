{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "dbb8f7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Imports & params ---\n",
    "import json, warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "DATA_DIR = Path(\"data\")\n",
    "ART_DIR  = Path(\"artifacts\")\n",
    "FIG_DIR  = Path(\"reports/figures\")\n",
    "for p in [DATA_DIR, ART_DIR, FIG_DIR]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Backtest knobs\n",
    "FEE_BPS      = 5.0      # round-trip trading cost in bps (e.g., 5 = 0.05%)\n",
    "SLIPPAGE_BPS = 0.0      # extra bps per trade (optional)\n",
    "TAU_GRID     = np.linspace(0.50, 0.65, 16)  # probability threshold sweep\n",
    "ALLOW_SHORT  = False     # True → 3-way long/short/flat; False → long/flat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f55b0a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded splits: (1662, 14) (416, 14) | labels: 1662 416\n"
     ]
    }
   ],
   "source": [
    "# --- Split loader with Parquet-or-CSV fallback (drop-in replacement for Cell 2) ---\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def _can_parquet():\n",
    "    try:\n",
    "        import pyarrow  # noqa: F401\n",
    "        return True\n",
    "    except Exception:\n",
    "        try:\n",
    "            import fastparquet  # noqa: F401\n",
    "            return True\n",
    "        except Exception:\n",
    "            return False\n",
    "\n",
    "def write_table(df: pd.DataFrame, base: Path):\n",
    "    \"\"\"\n",
    "    Save to <base>.parquet if a parquet engine is installed, else <base>.csv\n",
    "    Returns the actual Path written.\n",
    "    \"\"\"\n",
    "    if _can_parquet():\n",
    "        path = base.with_suffix(\".parquet\")\n",
    "        df.to_parquet(path, index=False)\n",
    "    else:\n",
    "        path = base.with_suffix(\".csv\")\n",
    "        df.to_csv(path, index=False)\n",
    "    return path\n",
    "\n",
    "def read_table(base: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Read from <base>.parquet if it exists, else <base>.csv.\n",
    "    \"\"\"\n",
    "    p_parq = base.with_suffix(\".parquet\")\n",
    "    p_csv  = base.with_suffix(\".csv\")\n",
    "    if p_parq.exists():\n",
    "        return pd.read_parquet(p_parq)\n",
    "    if p_csv.exists():\n",
    "        return pd.read_csv(p_csv)\n",
    "    raise FileNotFoundError(f\"Neither {p_parq.name} nor {p_csv.name} exists.\")\n",
    "\n",
    "# --- Load train/test splits; rebuild from df_nb02.csv if missing ---\n",
    "def rebuild_splits_from_phase2():\n",
    "    base_csv = DATA_DIR / \"df_nb02.csv\"\n",
    "    if not base_csv.exists():\n",
    "        raise FileNotFoundError(\"Missing data/df_nb02.csv (Phase 2).\")\n",
    "    df = pd.read_csv(base_csv)\n",
    "\n",
    "    # time order if available\n",
    "    if \"date\" in df.columns:\n",
    "        df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\").dt.tz_localize(None)\n",
    "        df = df.dropna(subset=[\"date\"]).sort_values(\"date\").reset_index(drop=True)\n",
    "\n",
    "    label_col = next((c for c in [\"y\",\"target\",\"label\",\"y_next_up\"] if c in df.columns), None)\n",
    "    if label_col is None:\n",
    "        raise RuntimeError(\"No label column in df_nb02.csv (expected y/target/label/y_next_up).\")\n",
    "\n",
    "    y = df.pop(label_col).astype(int)\n",
    "    # drop obvious non-features\n",
    "    X = df.drop(columns=[c for c in [\"date\",\"ticker\",\"symbol\",\"spy_close\",\"vix_close\"] if c in df.columns], errors=\"ignore\")\n",
    "    X = X.select_dtypes(include=[np.number]).copy()\n",
    "\n",
    "    n = len(X); cut = int(n*0.8)\n",
    "    X_tr, X_te = X.iloc[:cut].copy(), X.iloc[cut:].copy()\n",
    "    y_tr, y_te = y.iloc[:cut].copy(), y.iloc[cut:].copy()\n",
    "\n",
    "    # persist for next runs (Parquet if possible, else CSV)\n",
    "    write_table(X_tr, DATA_DIR / \"train\")\n",
    "    write_table(X_te, DATA_DIR / \"test\")\n",
    "    write_table(pd.DataFrame({\"y\": y_tr}), DATA_DIR / \"y_train\")\n",
    "    write_table(pd.DataFrame({\"y\": y_te}), DATA_DIR / \"y_test\")\n",
    "    print(\"Rebuilt splits → data/train.*, data/test.*, y_*.{parquet|csv}\")\n",
    "    return X_tr, X_te, y_tr.values, y_te.values\n",
    "\n",
    "train_base = DATA_DIR / \"train\"\n",
    "test_base  = DATA_DIR / \"test\"\n",
    "ytr_base   = DATA_DIR / \"y_train\"\n",
    "yte_base   = DATA_DIR / \"y_test\"\n",
    "\n",
    "try:\n",
    "    X_tr = read_table(train_base).copy()\n",
    "    X_te = read_table(test_base).copy()\n",
    "except FileNotFoundError:\n",
    "    X_tr, X_te, y_tr, y_te = rebuild_splits_from_phase2()\n",
    "else:\n",
    "    # labels either embedded or separate y files\n",
    "    label_col = next((c for c in [\"y\",\"target\",\"label\",\"y_next_up\"] if c in X_tr.columns), None)\n",
    "    if label_col:\n",
    "        y_tr = X_tr.pop(label_col).astype(int).values\n",
    "        y_te = X_te.pop(label_col).astype(int).values\n",
    "    else:\n",
    "        y_tr = read_table(ytr_base).iloc[:,0].astype(int).values\n",
    "        y_te = read_table(yte_base).iloc[:,0].astype(int).values\n",
    "\n",
    "print(\"Loaded splits:\", X_tr.shape, X_te.shape, \"| labels:\", len(y_tr), len(y_te))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "45440579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HAS_LR: True | HAS_XGB: False\n"
     ]
    }
   ],
   "source": [
    "# --- Load models ---\n",
    "import joblib\n",
    "path_lr  = ART_DIR / \"lr.joblib\"      # your saved LR model\n",
    "path_xgb = ART_DIR / \"model_xgb.pkl\"  # only if you trained/saved XGB\n",
    "\n",
    "HAS_LR  = path_lr.exists()\n",
    "HAS_XGB = path_xgb.exists()\n",
    "\n",
    "mdl_lr  = joblib.load(path_lr)  if HAS_LR  else None\n",
    "mdl_xgb = joblib.load(path_xgb) if HAS_XGB else None\n",
    "print(\"HAS_LR:\", HAS_LR, \"| HAS_XGB:\", HAS_XGB)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0fdf1d55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aligned shapes: (1662, 16) (416, 16)\n",
      "Model expects: 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>AUC_test</th>\n",
       "      <th>Brier_test</th>\n",
       "      <th>mean_p</th>\n",
       "      <th>n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LR</td>\n",
       "      <td>0.4531</td>\n",
       "      <td>0.2549</td>\n",
       "      <td>0.527</td>\n",
       "      <td>416</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  model  AUC_test  Brier_test  mean_p    n\n",
       "0    LR    0.4531      0.2549   0.527  416"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Align features to training spec + apply scaler, then predict ---\n",
    "import json\n",
    "\n",
    "# 1) feature list\n",
    "feat_path   = ART_DIR / \"feature_list.json\"\n",
    "scaler_path = ART_DIR / \"scaler.joblib\"\n",
    "\n",
    "if feat_path.exists():\n",
    "    with open(feat_path, \"r\") as fh:\n",
    "        feat_list = json.load(fh)\n",
    "elif HAS_LR and hasattr(mdl_lr, \"feature_names_in_\"):\n",
    "    feat_list = list(mdl_lr.feature_names_in_)\n",
    "else:\n",
    "    raise RuntimeError(\"Missing artifacts/feature_list.json and model has no feature_names_in_.\")\n",
    "\n",
    "def conform(df, cols):\n",
    "    df = df.copy()\n",
    "    for c in cols:\n",
    "        if c not in df.columns:\n",
    "            df[c] = 0.0\n",
    "    return df[cols].copy()\n",
    "\n",
    "X_tr_aligned = conform(X_tr, feat_list)\n",
    "X_te_aligned = conform(X_te, feat_list)\n",
    "\n",
    "# 2) scaler\n",
    "if scaler_path.exists():\n",
    "    scaler = joblib.load(scaler_path)\n",
    "    X_tr_in = scaler.transform(X_tr_aligned.values)\n",
    "    X_te_in = scaler.transform(X_te_aligned.values)\n",
    "else:\n",
    "    X_tr_in = X_tr_aligned.values\n",
    "    X_te_in = X_te_aligned.values\n",
    "\n",
    "print(\"Aligned shapes:\", X_tr_in.shape, X_te_in.shape)\n",
    "print(\"Model expects:\", getattr(mdl_lr, 'n_features_in_', 'unknown'))\n",
    "\n",
    "# 3) predict (use aligned arrays!)\n",
    "from sklearn.metrics import roc_auc_score, brier_score_loss\n",
    "probs = {}\n",
    "\n",
    "if HAS_LR and mdl_lr is not None:\n",
    "    probs[\"LR\"] = mdl_lr.predict_proba(X_te_in)[:, 1]\n",
    "\n",
    "if HAS_XGB and mdl_xgb is not None:\n",
    "    # XGB usually trained on same feat_list; scale only if you trained it with scaling\n",
    "    probs[\"XGB\"] = mdl_xgb.predict_proba(X_te_aligned.values)[:, 1]\n",
    "\n",
    "# quick metrics\n",
    "rows = []\n",
    "for name, p in probs.items():\n",
    "    rows.append({\n",
    "        \"model\": name,\n",
    "        \"AUC_test\": float(roc_auc_score(y_te, p)),\n",
    "        \"Brier_test\": float(brier_score_loss(y_te, p)),\n",
    "        \"mean_p\": float(np.mean(p)),\n",
    "        \"n\": int(len(p)),\n",
    "    })\n",
    "pd.DataFrame(rows).round(4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ec1a9ced",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>AUC_test</th>\n",
       "      <th>Brier_test</th>\n",
       "      <th>mean_p</th>\n",
       "      <th>n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LR</td>\n",
       "      <td>0.4531</td>\n",
       "      <td>0.2549</td>\n",
       "      <td>0.527</td>\n",
       "      <td>416</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  model  AUC_test  Brier_test  mean_p    n\n",
       "0    LR    0.4531      0.2549   0.527  416"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- Predict on test (aligned) ---\n",
    "from sklearn.metrics import roc_auc_score, brier_score_loss\n",
    "\n",
    "probs = {}\n",
    "\n",
    "# LR uses the scaled/aligned matrix\n",
    "if HAS_LR and mdl_lr is not None:\n",
    "    probs[\"LR\"] = mdl_lr.predict_proba(X_te_in)[:, 1]\n",
    "\n",
    "# XGB (if present) uses aligned *unscaled* features unless you also scaled at train time\n",
    "if HAS_XGB and mdl_xgb is not None:\n",
    "    probs[\"XGB\"] = mdl_xgb.predict_proba(X_te_aligned.values)[:, 1]\n",
    "\n",
    "if not probs:\n",
    "    raise RuntimeError(\"No models available to predict. Ensure lr.joblib or model_xgb.pkl exists.\")\n",
    "\n",
    "# Quick metrics table\n",
    "rows = []\n",
    "for name, p in probs.items():\n",
    "    rows.append({\n",
    "        \"model\": name,\n",
    "        \"AUC_test\": float(roc_auc_score(y_te, p)),\n",
    "        \"Brier_test\": float(brier_score_loss(y_te, p)),\n",
    "        \"mean_p\": float(np.mean(p)),\n",
    "        \"n\": int(len(p)),\n",
    "    })\n",
    "metrics = pd.DataFrame(rows).round(4)\n",
    "display(metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "90474c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test span: 2024-02-14 00:00:00 → 2025-10-10 00:00:00 | n= 416\n"
     ]
    }
   ],
   "source": [
    "# --- Build next-day returns (ret1) aligned to test period length ---\n",
    "base_csv = DATA_DIR / \"df_nb02.csv\"\n",
    "df_all = pd.read_csv(base_csv)\n",
    "if \"date\" not in df_all.columns:\n",
    "    raise KeyError(\"'date' missing in df_nb02.csv\")\n",
    "df_all[\"date\"] = pd.to_datetime(df_all[\"date\"], errors=\"coerce\").dt.tz_localize(None)\n",
    "df_all = df_all.dropna(subset=[\"date\"]).sort_values(\"date\").reset_index(drop=True)\n",
    "\n",
    "price_col = next((c for c in [\"adj_close\",\"Adj Close\",\"close\",\"Close\",\"price\"] if c in df_all.columns), None)\n",
    "if price_col is None:\n",
    "    raise RuntimeError(\"No price column found in df_nb02.csv (need adj_close/close).\")\n",
    "\n",
    "df_all[\"ret1\"] = pd.Series(df_all[price_col]).pct_change().shift(-1)\n",
    "\n",
    "N = len(y_te)\n",
    "df_test_slice = df_all.iloc[-N:].copy().reset_index(drop=True)\n",
    "dates_te = df_test_slice[\"date\"].values\n",
    "r_next   = df_test_slice[\"ret1\"].fillna(0.0).values\n",
    "\n",
    "print(\"Test span:\", df_test_slice['date'].min(), \"→\", df_test_slice['date'].max(), \"| n=\", len(df_test_slice))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "464adc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Backtest helpers ---\n",
    "def positions_from_probs(p, tau, allow_short=False):\n",
    "    if allow_short:\n",
    "        return np.where(p>tau, 1.0, np.where(p<1.0-tau, -1.0, 0.0))\n",
    "    return (p > tau).astype(float)\n",
    "\n",
    "def trade_costs_from_positions(pos, fee_bps=5.0, slippage_bps=0.0):\n",
    "    return (abs(np.diff(np.r_[0.0, pos])) * (fee_bps + slippage_bps)) / 1e4\n",
    "\n",
    "def equity_curve(returns, pos, fee_bps=5.0, slippage_bps=0.0):\n",
    "    gross = pos * returns\n",
    "    costs = trade_costs_from_positions(pos, fee_bps, slippage_bps)\n",
    "    net   = gross - costs\n",
    "    eq = (1.0 + net).cumprod()\n",
    "    return eq, {\"gross\": gross, \"costs\": costs, \"net\": net}\n",
    "\n",
    "def perf_metrics(net_ret, dates=None, periods_per_year=252):\n",
    "    tot_ret = float(np.prod(1.0 + net_ret) - 1.0)\n",
    "    if dates is not None and len(dates)>1:\n",
    "        years = (pd.to_datetime(dates[-1]) - pd.to_datetime(dates[0])).days / 365.25\n",
    "        cagr = (1.0 + tot_ret)**(1.0/years) - 1.0 if years>0 else np.nan\n",
    "    else:\n",
    "        years = len(net_ret)/periods_per_year\n",
    "        cagr = (1.0 + tot_ret)**(1.0/years) - 1.0 if years>0 else np.nan\n",
    "\n",
    "    mu = np.mean(net_ret); sd = np.std(net_ret, ddof=1)\n",
    "    sharpe = (mu/(sd + 1e-12)) * np.sqrt(periods_per_year)\n",
    "\n",
    "    curve = np.cumprod(1.0 + net_ret)\n",
    "    peak  = np.maximum.accumulate(curve)\n",
    "    maxdd = float((curve/peak - 1.0).min())\n",
    "\n",
    "    hit_rate = float(np.mean(net_ret > 0.0))\n",
    "    turnover = float(np.mean(np.abs(np.diff(np.r_[0.0, (net_ret!=0).astype(float)]))))\n",
    "\n",
    "    return {\"total_return\": float(tot_ret), \"CAGR\": float(cagr), \"vol_annual\": float(sd*np.sqrt(periods_per_year)),\n",
    "            \"Sharpe\": float(sharpe), \"max_drawdown\": maxdd, \"hit_rate\": hit_rate, \"turnover\": turnover}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a1f026b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper: get test returns by index-aligning to the full series tail ---\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "def get_ret_next_tail_for_test(test_len: int):\n",
    "    \"\"\"\n",
    "    Compute next-day returns from the FULL df_nb02.* series (using 'close' or an existing 'ret_next'),\n",
    "    then take the last test_len rows and reset index, to align with the TEST split by index.\n",
    "    \"\"\"\n",
    "    df_path = Path(\"data/df_nb02.parquet\") if Path(\"data/df_nb02.parquet\").exists() else Path(\"data/df_nb02.csv\")\n",
    "    if not df_path.exists():\n",
    "        raise FileNotFoundError(\"Missing data/df_nb02.{csv|parquet}\")\n",
    "\n",
    "    df_full = pd.read_parquet(df_path) if df_path.suffix == \".parquet\" else pd.read_csv(df_path, parse_dates=[\"date\"])\n",
    "    # If ret_next is already there, use it; else compute from 'close'\n",
    "    if \"ret_next\" in df_full.columns:\n",
    "        ret_full = pd.Series(df_full[\"ret_next\"], dtype=\"float64\")\n",
    "    else:\n",
    "        price_col = next((c for c in [\"close\",\"Close\",\"Adj Close\",\"adj_close\"] if c in df_full.columns), None)\n",
    "        if price_col is None:\n",
    "            raise KeyError(\"No price column ('close'/'Adj Close') to compute returns.\")\n",
    "        ret_full = pd.Series(df_full[price_col], dtype=\"float64\").pct_change().shift(-1)\n",
    "\n",
    "    # Take the tail of the full series to match the test length, index-aligned\n",
    "    ret_tail = ret_full.iloc[-test_len:].reset_index(drop=True)\n",
    "    return ret_tail.to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "15203eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR: best Sharpe @ tau=0.590 | Sharpe=1.09 | CAGR=5.04%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>tau</th>\n",
       "      <th>total_return</th>\n",
       "      <th>CAGR</th>\n",
       "      <th>vol_annual</th>\n",
       "      <th>Sharpe</th>\n",
       "      <th>max_drawdown</th>\n",
       "      <th>hit_rate</th>\n",
       "      <th>turnover</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LR</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.084581</td>\n",
       "      <td>0.050415</td>\n",
       "      <td>0.046056</td>\n",
       "      <td>1.094631</td>\n",
       "      <td>-0.032745</td>\n",
       "      <td>0.033654</td>\n",
       "      <td>0.028846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LR</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.491386</td>\n",
       "      <td>0.273959</td>\n",
       "      <td>0.265829</td>\n",
       "      <td>1.030585</td>\n",
       "      <td>-0.326602</td>\n",
       "      <td>0.391827</td>\n",
       "      <td>0.120192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LR</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.466461</td>\n",
       "      <td>0.261019</td>\n",
       "      <td>0.258467</td>\n",
       "      <td>1.009873</td>\n",
       "      <td>-0.309151</td>\n",
       "      <td>0.358173</td>\n",
       "      <td>0.110577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LR</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.122053</td>\n",
       "      <td>0.072251</td>\n",
       "      <td>0.073834</td>\n",
       "      <td>0.978565</td>\n",
       "      <td>-0.048955</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.072115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LR</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.007635</td>\n",
       "      <td>0.004618</td>\n",
       "      <td>0.004834</td>\n",
       "      <td>0.955375</td>\n",
       "      <td>-0.000500</td>\n",
       "      <td>0.004808</td>\n",
       "      <td>0.009615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LR</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.375159</td>\n",
       "      <td>0.212858</td>\n",
       "      <td>0.273161</td>\n",
       "      <td>0.779240</td>\n",
       "      <td>-0.333605</td>\n",
       "      <td>0.430288</td>\n",
       "      <td>0.120192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LR</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.021966</td>\n",
       "      <td>0.013249</td>\n",
       "      <td>0.017988</td>\n",
       "      <td>0.736566</td>\n",
       "      <td>-0.008664</td>\n",
       "      <td>0.012019</td>\n",
       "      <td>0.019231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LR</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.030501</td>\n",
       "      <td>0.018367</td>\n",
       "      <td>0.033379</td>\n",
       "      <td>0.550251</td>\n",
       "      <td>-0.034897</td>\n",
       "      <td>0.019231</td>\n",
       "      <td>0.028846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LR</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.082119</td>\n",
       "      <td>0.048969</td>\n",
       "      <td>0.109442</td>\n",
       "      <td>0.447445</td>\n",
       "      <td>-0.123255</td>\n",
       "      <td>0.088942</td>\n",
       "      <td>0.110577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LR</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.178405</td>\n",
       "      <td>0.104557</td>\n",
       "      <td>0.246878</td>\n",
       "      <td>0.423516</td>\n",
       "      <td>-0.314957</td>\n",
       "      <td>0.302885</td>\n",
       "      <td>0.139423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LR</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.063151</td>\n",
       "      <td>0.037792</td>\n",
       "      <td>0.134708</td>\n",
       "      <td>0.280550</td>\n",
       "      <td>-0.154718</td>\n",
       "      <td>0.139423</td>\n",
       "      <td>0.144231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LR</td>\n",
       "      <td>0.54</td>\n",
       "      <td>-0.001447</td>\n",
       "      <td>-0.000877</td>\n",
       "      <td>0.192784</td>\n",
       "      <td>-0.004549</td>\n",
       "      <td>-0.300426</td>\n",
       "      <td>0.237981</td>\n",
       "      <td>0.129808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LR</td>\n",
       "      <td>0.55</td>\n",
       "      <td>-0.130181</td>\n",
       "      <td>-0.081016</td>\n",
       "      <td>0.173885</td>\n",
       "      <td>-0.465917</td>\n",
       "      <td>-0.295127</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.129808</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   model   tau  total_return      CAGR  vol_annual    Sharpe  max_drawdown  \\\n",
       "0     LR  0.59      0.084581  0.050415    0.046056  1.094631     -0.032745   \n",
       "1     LR  0.51      0.491386  0.273959    0.265829  1.030585     -0.326602   \n",
       "2     LR  0.52      0.466461  0.261019    0.258467  1.009873     -0.309151   \n",
       "3     LR  0.58      0.122053  0.072251    0.073834  0.978565     -0.048955   \n",
       "4     LR  0.62      0.007635  0.004618    0.004834  0.955375     -0.000500   \n",
       "5     LR  0.50      0.375159  0.212858    0.273161  0.779240     -0.333605   \n",
       "6     LR  0.61      0.021966  0.013249    0.017988  0.736566     -0.008664   \n",
       "7     LR  0.60      0.030501  0.018367    0.033379  0.550251     -0.034897   \n",
       "8     LR  0.57      0.082119  0.048969    0.109442  0.447445     -0.123255   \n",
       "9     LR  0.53      0.178405  0.104557    0.246878  0.423516     -0.314957   \n",
       "10    LR  0.56      0.063151  0.037792    0.134708  0.280550     -0.154718   \n",
       "11    LR  0.54     -0.001447 -0.000877    0.192784 -0.004549     -0.300426   \n",
       "12    LR  0.55     -0.130181 -0.081016    0.173885 -0.465917     -0.295127   \n",
       "\n",
       "    hit_rate  turnover  \n",
       "0   0.033654  0.028846  \n",
       "1   0.391827  0.120192  \n",
       "2   0.358173  0.110577  \n",
       "3   0.062500  0.072115  \n",
       "4   0.004808  0.009615  \n",
       "5   0.430288  0.120192  \n",
       "6   0.012019  0.019231  \n",
       "7   0.019231  0.028846  \n",
       "8   0.088942  0.110577  \n",
       "9   0.302885  0.139423  \n",
       "10  0.139423  0.144231  \n",
       "11  0.237981  0.129808  \n",
       "12  0.187500  0.129808  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: data\\multiticker_tau_sweep.csv\n"
     ]
    }
   ],
   "source": [
    "# ----- Cell 8: Threshold sweep (tau) + backtest KPIs (index-aligned returns) -----\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# fee setup (bps → fraction)\n",
    "FEE_BPS = 5.0\n",
    "fee = FEE_BPS / 10000.0\n",
    "\n",
    "# Use LR test probabilities computed earlier (from Cell 4)\n",
    "# Expect either `p_te` or `preds[\"LR\"][\"te\"]` to exist\n",
    "if 'p_te' in globals():\n",
    "    probs_te = p_te\n",
    "elif 'preds' in globals() and \"LR\" in preds and \"te\" in preds[\"LR\"]:\n",
    "    probs_te = preds[\"LR\"][\"te\"]\n",
    "else:\n",
    "    raise RuntimeError(\"No test probabilities found (expected p_te or preds['LR']['te']).\")\n",
    "\n",
    "# Returns from FULL series tail, index-aligned to TEST length (matches saved backtest)\n",
    "ret_next = get_ret_next_tail_for_test(len(probs_te))\n",
    "\n",
    "def kpis_for_tau(tau, p=probs_te, ret=ret_next, fee_frac=fee, freq=252):\n",
    "    signal = (p >= tau).astype(int)\n",
    "    flips  = (np.abs(np.diff(np.r_[0, signal])) > 0).astype(int)  # trades when position changes\n",
    "    r      = pd.Series(signal * ret - flips * fee_frac).fillna(0.0)\n",
    "\n",
    "    eq = (1 + r).cumprod()\n",
    "    cagr = (1 + r).prod() ** (freq / max(len(r), 1)) - 1\n",
    "    vol  = r.std() * np.sqrt(freq)\n",
    "    sharpe = (cagr / vol) if vol > 0 else np.nan\n",
    "    maxdd = (eq / eq.cummax() - 1).min()\n",
    "    hit_rate = (r > 0).mean()\n",
    "    turnover = flips.sum() / max(len(r), 1)\n",
    "\n",
    "    return {\n",
    "        \"total_return\": float(eq.iloc[-1] - 1) if len(eq) else 0.0,\n",
    "        \"CAGR\": float(cagr),\n",
    "        \"vol_annual\": float(vol),\n",
    "        \"Sharpe\": float(sharpe),\n",
    "        \"max_drawdown\": float(maxdd),\n",
    "        \"hit_rate\": float(hit_rate),\n",
    "        \"turnover\": float(turnover),\n",
    "    }\n",
    "\n",
    "# tau grid (matches your earlier run)\n",
    "taus = np.round(np.linspace(0.50, 0.62, 13), 2)\n",
    "\n",
    "rows = []\n",
    "for t in taus:\n",
    "    m = kpis_for_tau(t)\n",
    "    rows.append({\"model\": \"LR\", \"tau\": float(t), **m})\n",
    "\n",
    "tau_df = pd.DataFrame(rows).sort_values(\"Sharpe\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Report best by Sharpe\n",
    "best = tau_df.iloc[0]\n",
    "print(f\"LR: best Sharpe @ tau={best['tau']:.3f} | Sharpe={best['Sharpe']:.2f} | CAGR={best['CAGR']*100:.2f}%\")\n",
    "display(tau_df)\n",
    "\n",
    "# Save sweep\n",
    "out_path = Path(\"data/multiticker_tau_sweep.csv\")\n",
    "tau_df.to_csv(out_path, index=False)\n",
    "print(f\"Saved: {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "80538428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: reports\\figures\\equity_curve_lr.png\n"
     ]
    }
   ],
   "source": [
    "# --- Equity curve plots (best per model) ---\n",
    "for name, (eq, pos, parts) in curves.items():\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.plot(eq, label=f\"{name}\")\n",
    "    plt.title(f\"Equity Curve – {name}\")\n",
    "    plt.xlabel(\"Time (test)\")\n",
    "    plt.ylabel(\"Equity (x)\")\n",
    "    plt.grid(True, alpha=.3)\n",
    "    plt.legend()\n",
    "    out = FIG_DIR / f\"equity_curve_{name.lower()}.png\"\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out, dpi=150)\n",
    "    plt.close()\n",
    "    print(\"Saved:\", out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "68e09544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: artifacts\\backtest_summary.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'fee_bps': 5.0,\n",
       " 'slippage_bps': 0.0,\n",
       " 'allow_short': False,\n",
       " 'models': {'LR': {'tau': 0.5900000000000001,\n",
       "   'Sharpe': 1.0908623916378921,\n",
       "   'CAGR': 0.05032491896714353,\n",
       "   'max_drawdown': -0.03274495697381219,\n",
       "   'hit_rate': 0.03365384615384615,\n",
       "   'total_return': 0.08458136662655846,\n",
       "   'vol_annual': 0.04605617349906653}}}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Backtest summary JSON ---\n",
    "summary = {\"fee_bps\": FEE_BPS, \"slippage_bps\": SLIPPAGE_BPS, \"allow_short\": ALLOW_SHORT, \"models\": {}}\n",
    "\n",
    "for name in probs.keys():\n",
    "    best_row = bt_df[bt_df[\"model\"]==name].iloc[0].to_dict()\n",
    "    summary[\"models\"][name] = {\n",
    "        \"tau\": best_row[\"tau\"],\n",
    "        \"Sharpe\": best_row[\"Sharpe\"],\n",
    "        \"CAGR\": best_row[\"CAGR\"],\n",
    "        \"max_drawdown\": best_row[\"max_drawdown\"],\n",
    "        \"hit_rate\": best_row[\"hit_rate\"],\n",
    "        \"total_return\": best_row[\"total_return\"],\n",
    "        \"vol_annual\": best_row[\"vol_annual\"]\n",
    "    }\n",
    "\n",
    "out_json = ART_DIR / \"backtest_summary.json\"\n",
    "with open(out_json, \"w\") as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "print(\"Wrote:\", out_json)\n",
    "summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6e6d95be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: reports\\figures\\equity_curve_vs_buyhold.png\n"
     ]
    }
   ],
   "source": [
    "# --- Optional: overlay buy & hold with best-τ strategy ---\n",
    "# best τ for LR from the sweep table\n",
    "best_tau_lr = float(\n",
    "    bt_df[bt_df[\"model\"]==\"LR\"].sort_values(\"Sharpe\", ascending=False).iloc[0][\"tau\"]\n",
    ")\n",
    "\n",
    "# Buy & hold = always in the market, no fees\n",
    "bh_eq, _ = equity_curve(r_next, np.ones_like(r_next), fee_bps=0.0, slippage_bps=0.0)\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(bh_eq, label=\"Buy & Hold\")\n",
    "\n",
    "# Plot each strategy curve you saved in `curves`\n",
    "for name, (eq, _, _) in curves.items():\n",
    "    plt.plot(eq, label=f\"{name} (best τ)\")\n",
    "\n",
    "plt.title(\"Equity Curve — Strategy vs Buy & Hold\")\n",
    "plt.xlabel(\"Time (test)\"); plt.ylabel(\"Equity (x)\")\n",
    "plt.grid(True, alpha=.3); plt.legend(); plt.tight_layout()\n",
    "out = FIG_DIR / \"equity_curve_vs_buyhold.png\"\n",
    "plt.savefig(out, dpi=150); plt.close()\n",
    "print(\"Saved:\", out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c08c34d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fee_bps</th>\n",
       "      <th>total_return</th>\n",
       "      <th>CAGR</th>\n",
       "      <th>vol_annual</th>\n",
       "      <th>Sharpe</th>\n",
       "      <th>max_drawdown</th>\n",
       "      <th>hit_rate</th>\n",
       "      <th>turnover</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0911</td>\n",
       "      <td>0.0541</td>\n",
       "      <td>0.0464</td>\n",
       "      <td>1.1618</td>\n",
       "      <td>-0.0323</td>\n",
       "      <td>0.0337</td>\n",
       "      <td>0.0288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.5</td>\n",
       "      <td>0.0878</td>\n",
       "      <td>0.0522</td>\n",
       "      <td>0.0462</td>\n",
       "      <td>1.1266</td>\n",
       "      <td>-0.0325</td>\n",
       "      <td>0.0337</td>\n",
       "      <td>0.0192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0846</td>\n",
       "      <td>0.0503</td>\n",
       "      <td>0.0461</td>\n",
       "      <td>1.0909</td>\n",
       "      <td>-0.0327</td>\n",
       "      <td>0.0337</td>\n",
       "      <td>0.0192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0781</td>\n",
       "      <td>0.0465</td>\n",
       "      <td>0.0458</td>\n",
       "      <td>1.0181</td>\n",
       "      <td>-0.0332</td>\n",
       "      <td>0.0337</td>\n",
       "      <td>0.0192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0653</td>\n",
       "      <td>0.0390</td>\n",
       "      <td>0.0453</td>\n",
       "      <td>0.8678</td>\n",
       "      <td>-0.0350</td>\n",
       "      <td>0.0337</td>\n",
       "      <td>0.0192</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fee_bps  total_return    CAGR  vol_annual  Sharpe  max_drawdown  hit_rate  \\\n",
       "0      0.0        0.0911  0.0541      0.0464  1.1618       -0.0323    0.0337   \n",
       "1      2.5        0.0878  0.0522      0.0462  1.1266       -0.0325    0.0337   \n",
       "2      5.0        0.0846  0.0503      0.0461  1.0909       -0.0327    0.0337   \n",
       "3     10.0        0.0781  0.0465      0.0458  1.0181       -0.0332    0.0337   \n",
       "4     20.0        0.0653  0.0390      0.0453  0.8678       -0.0350    0.0337   \n",
       "\n",
       "   turnover  \n",
       "0    0.0288  \n",
       "1    0.0192  \n",
       "2    0.0192  \n",
       "3    0.0192  \n",
       "4    0.0192  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: data\\multiticker_fee_sweep_AAPL_LR.csv\n"
     ]
    }
   ],
   "source": [
    "# --- Optional: fee sensitivity for LR at best τ ---\n",
    "fees_bps = [0, 2.5, 5, 10, 20]\n",
    "best_tau_lr = float(\n",
    "    bt_df[bt_df[\"model\"]==\"LR\"].sort_values(\"Sharpe\", ascending=False).iloc[0][\"tau\"]\n",
    ")\n",
    "pos_lr = (probs[\"LR\"] > best_tau_lr).astype(float)\n",
    "\n",
    "rows = []\n",
    "for fbps in fees_bps:\n",
    "    eq, parts = equity_curve(r_next, pos_lr, fee_bps=fbps, slippage_bps=0.0)\n",
    "    m = perf_metrics(parts[\"net\"], dates_te)\n",
    "    rows.append({\"fee_bps\": fbps, **m})\n",
    "\n",
    "sens = pd.DataFrame(rows).round(4)\n",
    "display(sens)\n",
    "sens.to_csv(DATA_DIR / \"multiticker_fee_sweep_AAPL_LR.csv\", index=False)\n",
    "print(\"Saved:\", DATA_DIR / \"multiticker_fee_sweep_AAPL_LR.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c500ce70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved best LR threshold: 0.59\n"
     ]
    }
   ],
   "source": [
    "import json, pandas as pd\n",
    "best_tau = float(pd.read_csv(\"data/multiticker_tau_sweep.csv\")\n",
    "                 .sort_values([\"model\",\"Sharpe\"], ascending=[True, False])\n",
    "                 .query(\"model=='LR'\").iloc[0][\"tau\"])\n",
    "with open(\"artifacts/threshold.json\",\"w\") as f:\n",
    "    json.dump({\"LR\":{\"tau\":best_tau}}, f, indent=2)\n",
    "print(\"Saved best LR threshold:\", best_tau)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
