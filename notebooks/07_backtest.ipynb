{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbb8f7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Imports & params ---\n",
    "import json, warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "DATA_DIR = Path(\"data\")\n",
    "ART_DIR  = Path(\"artifacts\")\n",
    "FIG_DIR  = Path(\"reports/figures\")\n",
    "for p in [DATA_DIR, ART_DIR, FIG_DIR]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Backtest knobs\n",
    "FEE_BPS      = 5.0      # round-trip trading cost in bps (e.g., 5 = 0.05%)\n",
    "SLIPPAGE_BPS = 0.0      # extra bps per trade (optional)\n",
    "TAU_GRID     = np.linspace(0.50, 0.65, 16)  # probability threshold sweep\n",
    "ALLOW_SHORT  = False     # True → 3-way long/short/flat; False → long/flat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f55b0a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded splits: (1662, 14) (416, 14) | labels: 1662 416\n"
     ]
    }
   ],
   "source": [
    "# --- Split loader with Parquet-or-CSV fallback (drop-in replacement for Cell 2) ---\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def _can_parquet():\n",
    "    try:\n",
    "        import pyarrow  # noqa: F401\n",
    "        return True\n",
    "    except Exception:\n",
    "        try:\n",
    "            import fastparquet  # noqa: F401\n",
    "            return True\n",
    "        except Exception:\n",
    "            return False\n",
    "\n",
    "def write_table(df: pd.DataFrame, base: Path):\n",
    "    \"\"\"\n",
    "    Save to <base>.parquet if a parquet engine is installed, else <base>.csv\n",
    "    Returns the actual Path written.\n",
    "    \"\"\"\n",
    "    if _can_parquet():\n",
    "        path = base.with_suffix(\".parquet\")\n",
    "        df.to_parquet(path, index=False)\n",
    "    else:\n",
    "        path = base.with_suffix(\".csv\")\n",
    "        df.to_csv(path, index=False)\n",
    "    return path\n",
    "\n",
    "def read_table(base: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Read from <base>.parquet if it exists, else <base>.csv.\n",
    "    \"\"\"\n",
    "    p_parq = base.with_suffix(\".parquet\")\n",
    "    p_csv  = base.with_suffix(\".csv\")\n",
    "    if p_parq.exists():\n",
    "        return pd.read_parquet(p_parq)\n",
    "    if p_csv.exists():\n",
    "        return pd.read_csv(p_csv)\n",
    "    raise FileNotFoundError(f\"Neither {p_parq.name} nor {p_csv.name} exists.\")\n",
    "\n",
    "# --- Load train/test splits; rebuild from df_nb02.csv if missing ---\n",
    "def rebuild_splits_from_phase2():\n",
    "    base_csv = DATA_DIR / \"df_nb02.csv\"\n",
    "    if not base_csv.exists():\n",
    "        raise FileNotFoundError(\"Missing data/df_nb02.csv (Phase 2).\")\n",
    "    df = pd.read_csv(base_csv)\n",
    "\n",
    "    # time order if available\n",
    "    if \"date\" in df.columns:\n",
    "        df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\").dt.tz_localize(None)\n",
    "        df = df.dropna(subset=[\"date\"]).sort_values(\"date\").reset_index(drop=True)\n",
    "\n",
    "    label_col = next((c for c in [\"y\",\"target\",\"label\",\"y_next_up\"] if c in df.columns), None)\n",
    "    if label_col is None:\n",
    "        raise RuntimeError(\"No label column in df_nb02.csv (expected y/target/label/y_next_up).\")\n",
    "\n",
    "    y = df.pop(label_col).astype(int)\n",
    "    # drop obvious non-features\n",
    "    X = df.drop(columns=[c for c in [\"date\",\"ticker\",\"symbol\",\"spy_close\",\"vix_close\"] if c in df.columns], errors=\"ignore\")\n",
    "    X = X.select_dtypes(include=[np.number]).copy()\n",
    "\n",
    "    n = len(X); cut = int(n*0.8)\n",
    "    X_tr, X_te = X.iloc[:cut].copy(), X.iloc[cut:].copy()\n",
    "    y_tr, y_te = y.iloc[:cut].copy(), y.iloc[cut:].copy()\n",
    "\n",
    "    # persist for next runs (Parquet if possible, else CSV)\n",
    "    write_table(X_tr, DATA_DIR / \"train\")\n",
    "    write_table(X_te, DATA_DIR / \"test\")\n",
    "    write_table(pd.DataFrame({\"y\": y_tr}), DATA_DIR / \"y_train\")\n",
    "    write_table(pd.DataFrame({\"y\": y_te}), DATA_DIR / \"y_test\")\n",
    "    print(\"Rebuilt splits → data/train.*, data/test.*, y_*.{parquet|csv}\")\n",
    "    return X_tr, X_te, y_tr.values, y_te.values\n",
    "\n",
    "train_base = DATA_DIR / \"train\"\n",
    "test_base  = DATA_DIR / \"test\"\n",
    "ytr_base   = DATA_DIR / \"y_train\"\n",
    "yte_base   = DATA_DIR / \"y_test\"\n",
    "\n",
    "try:\n",
    "    X_tr = read_table(train_base).copy()\n",
    "    X_te = read_table(test_base).copy()\n",
    "except FileNotFoundError:\n",
    "    X_tr, X_te, y_tr, y_te = rebuild_splits_from_phase2()\n",
    "else:\n",
    "    # labels either embedded or separate y files\n",
    "    label_col = next((c for c in [\"y\",\"target\",\"label\",\"y_next_up\"] if c in X_tr.columns), None)\n",
    "    if label_col:\n",
    "        y_tr = X_tr.pop(label_col).astype(int).values\n",
    "        y_te = X_te.pop(label_col).astype(int).values\n",
    "    else:\n",
    "        y_tr = read_table(ytr_base).iloc[:,0].astype(int).values\n",
    "        y_te = read_table(yte_base).iloc[:,0].astype(int).values\n",
    "\n",
    "print(\"Loaded splits:\", X_tr.shape, X_te.shape, \"| labels:\", len(y_tr), len(y_te))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45440579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HAS_LR: True | HAS_XGB: False\n"
     ]
    }
   ],
   "source": [
    "# --- Load models ---\n",
    "import joblib\n",
    "path_lr  = ART_DIR / \"lr.joblib\"      # your saved LR model\n",
    "path_xgb = ART_DIR / \"model_xgb.pkl\"  # only if you trained/saved XGB\n",
    "\n",
    "HAS_LR  = path_lr.exists()\n",
    "HAS_XGB = path_xgb.exists()\n",
    "\n",
    "mdl_lr  = joblib.load(path_lr)  if HAS_LR  else None\n",
    "mdl_xgb = joblib.load(path_xgb) if HAS_XGB else None\n",
    "print(\"HAS_LR:\", HAS_LR, \"| HAS_XGB:\", HAS_XGB)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0fdf1d55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aligned shapes: (1662, 8) (416, 8)\n",
      "Model expects: 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>AUC_test</th>\n",
       "      <th>Brier_test</th>\n",
       "      <th>mean_p</th>\n",
       "      <th>n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LR</td>\n",
       "      <td>0.4525</td>\n",
       "      <td>0.275</td>\n",
       "      <td>0.5395</td>\n",
       "      <td>416</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  model  AUC_test  Brier_test  mean_p    n\n",
       "0    LR    0.4525       0.275  0.5395  416"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Align features to training spec + apply scaler, then predict ---\n",
    "import json\n",
    "\n",
    "# 1) feature list\n",
    "feat_path   = ART_DIR / \"feature_list.json\"\n",
    "scaler_path = ART_DIR / \"scaler.joblib\"\n",
    "\n",
    "if feat_path.exists():\n",
    "    with open(feat_path, \"r\") as fh:\n",
    "        feat_list = json.load(fh)\n",
    "elif HAS_LR and hasattr(mdl_lr, \"feature_names_in_\"):\n",
    "    feat_list = list(mdl_lr.feature_names_in_)\n",
    "else:\n",
    "    raise RuntimeError(\"Missing artifacts/feature_list.json and model has no feature_names_in_.\")\n",
    "\n",
    "def conform(df, cols):\n",
    "    df = df.copy()\n",
    "    for c in cols:\n",
    "        if c not in df.columns:\n",
    "            df[c] = 0.0\n",
    "    return df[cols].copy()\n",
    "\n",
    "X_tr_aligned = conform(X_tr, feat_list)\n",
    "X_te_aligned = conform(X_te, feat_list)\n",
    "\n",
    "# 2) scaler\n",
    "if scaler_path.exists():\n",
    "    scaler = joblib.load(scaler_path)\n",
    "    X_tr_in = scaler.transform(X_tr_aligned.values)\n",
    "    X_te_in = scaler.transform(X_te_aligned.values)\n",
    "else:\n",
    "    X_tr_in = X_tr_aligned.values\n",
    "    X_te_in = X_te_aligned.values\n",
    "\n",
    "print(\"Aligned shapes:\", X_tr_in.shape, X_te_in.shape)\n",
    "print(\"Model expects:\", getattr(mdl_lr, 'n_features_in_', 'unknown'))\n",
    "\n",
    "# 3) predict (use aligned arrays!)\n",
    "from sklearn.metrics import roc_auc_score, brier_score_loss\n",
    "probs = {}\n",
    "\n",
    "if HAS_LR and mdl_lr is not None:\n",
    "    probs[\"LR\"] = mdl_lr.predict_proba(X_te_in)[:, 1]\n",
    "\n",
    "if HAS_XGB and mdl_xgb is not None:\n",
    "    # XGB usually trained on same feat_list; scale only if you trained it with scaling\n",
    "    probs[\"XGB\"] = mdl_xgb.predict_proba(X_te_aligned.values)[:, 1]\n",
    "\n",
    "# quick metrics\n",
    "rows = []\n",
    "for name, p in probs.items():\n",
    "    rows.append({\n",
    "        \"model\": name,\n",
    "        \"AUC_test\": float(roc_auc_score(y_te, p)),\n",
    "        \"Brier_test\": float(brier_score_loss(y_te, p)),\n",
    "        \"mean_p\": float(np.mean(p)),\n",
    "        \"n\": int(len(p)),\n",
    "    })\n",
    "pd.DataFrame(rows).round(4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec1a9ced",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>AUC_test</th>\n",
       "      <th>Brier_test</th>\n",
       "      <th>mean_p</th>\n",
       "      <th>n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LR</td>\n",
       "      <td>0.4525</td>\n",
       "      <td>0.275</td>\n",
       "      <td>0.5395</td>\n",
       "      <td>416</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  model  AUC_test  Brier_test  mean_p    n\n",
       "0    LR    0.4525       0.275  0.5395  416"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- Predict on test (aligned) ---\n",
    "from sklearn.metrics import roc_auc_score, brier_score_loss\n",
    "\n",
    "probs = {}\n",
    "\n",
    "# LR uses the scaled/aligned matrix\n",
    "if HAS_LR and mdl_lr is not None:\n",
    "    probs[\"LR\"] = mdl_lr.predict_proba(X_te_in)[:, 1]\n",
    "\n",
    "# XGB (if present) uses aligned *unscaled* features unless you also scaled at train time\n",
    "if HAS_XGB and mdl_xgb is not None:\n",
    "    probs[\"XGB\"] = mdl_xgb.predict_proba(X_te_aligned.values)[:, 1]\n",
    "\n",
    "if not probs:\n",
    "    raise RuntimeError(\"No models available to predict. Ensure lr.joblib or model_xgb.pkl exists.\")\n",
    "\n",
    "# Quick metrics table\n",
    "rows = []\n",
    "for name, p in probs.items():\n",
    "    rows.append({\n",
    "        \"model\": name,\n",
    "        \"AUC_test\": float(roc_auc_score(y_te, p)),\n",
    "        \"Brier_test\": float(brier_score_loss(y_te, p)),\n",
    "        \"mean_p\": float(np.mean(p)),\n",
    "        \"n\": int(len(p)),\n",
    "    })\n",
    "metrics = pd.DataFrame(rows).round(4)\n",
    "display(metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90474c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test span: 2022-03-29 00:00:00 → 2023-12-28 00:00:00 | n= 416\n"
     ]
    }
   ],
   "source": [
    "# --- Build next-day returns (ret1) aligned to test period length ---\n",
    "base_csv = DATA_DIR / \"df_nb02.csv\"\n",
    "df_all = pd.read_csv(base_csv)\n",
    "if \"date\" not in df_all.columns:\n",
    "    raise KeyError(\"'date' missing in df_nb02.csv\")\n",
    "df_all[\"date\"] = pd.to_datetime(df_all[\"date\"], errors=\"coerce\").dt.tz_localize(None)\n",
    "df_all = df_all.dropna(subset=[\"date\"]).sort_values(\"date\").reset_index(drop=True)\n",
    "\n",
    "price_col = next((c for c in [\"adj_close\",\"Adj Close\",\"close\",\"Close\",\"price\"] if c in df_all.columns), None)\n",
    "if price_col is None:\n",
    "    raise RuntimeError(\"No price column found in df_nb02.csv (need adj_close/close).\")\n",
    "\n",
    "df_all[\"ret1\"] = pd.Series(df_all[price_col]).pct_change().shift(-1)\n",
    "\n",
    "N = len(y_te)\n",
    "df_test_slice = df_all.iloc[-N:].copy().reset_index(drop=True)\n",
    "dates_te = df_test_slice[\"date\"].values\n",
    "r_next   = df_test_slice[\"ret1\"].fillna(0.0).values\n",
    "\n",
    "print(\"Test span:\", df_test_slice['date'].min(), \"→\", df_test_slice['date'].max(), \"| n=\", len(df_test_slice))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "464adc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Backtest helpers ---\n",
    "def positions_from_probs(p, tau, allow_short=False):\n",
    "    if allow_short:\n",
    "        return np.where(p>tau, 1.0, np.where(p<1.0-tau, -1.0, 0.0))\n",
    "    return (p > tau).astype(float)\n",
    "\n",
    "def trade_costs_from_positions(pos, fee_bps=5.0, slippage_bps=0.0):\n",
    "    return (abs(np.diff(np.r_[0.0, pos])) * (fee_bps + slippage_bps)) / 1e4\n",
    "\n",
    "def equity_curve(returns, pos, fee_bps=5.0, slippage_bps=0.0):\n",
    "    gross = pos * returns\n",
    "    costs = trade_costs_from_positions(pos, fee_bps, slippage_bps)\n",
    "    net   = gross - costs\n",
    "    eq = (1.0 + net).cumprod()\n",
    "    return eq, {\"gross\": gross, \"costs\": costs, \"net\": net}\n",
    "\n",
    "def perf_metrics(net_ret, dates=None, periods_per_year=252):\n",
    "    tot_ret = float(np.prod(1.0 + net_ret) - 1.0)\n",
    "    if dates is not None and len(dates)>1:\n",
    "        years = (pd.to_datetime(dates[-1]) - pd.to_datetime(dates[0])).days / 365.25\n",
    "        cagr = (1.0 + tot_ret)**(1.0/years) - 1.0 if years>0 else np.nan\n",
    "    else:\n",
    "        years = len(net_ret)/periods_per_year\n",
    "        cagr = (1.0 + tot_ret)**(1.0/years) - 1.0 if years>0 else np.nan\n",
    "\n",
    "    mu = np.mean(net_ret); sd = np.std(net_ret, ddof=1)\n",
    "    sharpe = (mu/(sd + 1e-12)) * np.sqrt(periods_per_year)\n",
    "\n",
    "    curve = np.cumprod(1.0 + net_ret)\n",
    "    peak  = np.maximum.accumulate(curve)\n",
    "    maxdd = float((curve/peak - 1.0).min())\n",
    "\n",
    "    hit_rate = float(np.mean(net_ret > 0.0))\n",
    "    turnover = float(np.mean(np.abs(np.diff(np.r_[0.0, (net_ret!=0).astype(float)]))))\n",
    "\n",
    "    return {\"total_return\": float(tot_ret), \"CAGR\": float(cagr), \"vol_annual\": float(sd*np.sqrt(periods_per_year)),\n",
    "            \"Sharpe\": float(sharpe), \"max_drawdown\": maxdd, \"hit_rate\": hit_rate, \"turnover\": turnover}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15203eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR: best Sharpe @ tau=0.540 | Sharpe=0.43 | CAGR=6.17%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>tau</th>\n",
       "      <th>total_return</th>\n",
       "      <th>CAGR</th>\n",
       "      <th>vol_annual</th>\n",
       "      <th>Sharpe</th>\n",
       "      <th>max_drawdown</th>\n",
       "      <th>hit_rate</th>\n",
       "      <th>turnover</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LR</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.110391</td>\n",
       "      <td>0.061681</td>\n",
       "      <td>0.191302</td>\n",
       "      <td>0.426684</td>\n",
       "      <td>-0.161143</td>\n",
       "      <td>0.286058</td>\n",
       "      <td>0.086538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LR</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.073879</td>\n",
       "      <td>0.041583</td>\n",
       "      <td>0.182496</td>\n",
       "      <td>0.327327</td>\n",
       "      <td>-0.202796</td>\n",
       "      <td>0.269231</td>\n",
       "      <td>0.081731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LR</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.046769</td>\n",
       "      <td>0.026471</td>\n",
       "      <td>0.200241</td>\n",
       "      <td>0.237934</td>\n",
       "      <td>-0.202824</td>\n",
       "      <td>0.300481</td>\n",
       "      <td>0.086538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LR</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.038721</td>\n",
       "      <td>0.021953</td>\n",
       "      <td>0.168304</td>\n",
       "      <td>0.220221</td>\n",
       "      <td>-0.170964</td>\n",
       "      <td>0.233173</td>\n",
       "      <td>0.091346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LR</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.028869</td>\n",
       "      <td>0.016401</td>\n",
       "      <td>0.175484</td>\n",
       "      <td>0.185351</td>\n",
       "      <td>-0.196387</td>\n",
       "      <td>0.252404</td>\n",
       "      <td>0.076923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LR</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.021197</td>\n",
       "      <td>0.012062</td>\n",
       "      <td>0.141483</td>\n",
       "      <td>0.159846</td>\n",
       "      <td>-0.122411</td>\n",
       "      <td>0.122596</td>\n",
       "      <td>0.081731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LR</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.010087</td>\n",
       "      <td>0.005753</td>\n",
       "      <td>0.145827</td>\n",
       "      <td>0.113896</td>\n",
       "      <td>-0.142390</td>\n",
       "      <td>0.141827</td>\n",
       "      <td>0.096154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LR</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.002878</td>\n",
       "      <td>0.001644</td>\n",
       "      <td>0.205808</td>\n",
       "      <td>0.110968</td>\n",
       "      <td>-0.221052</td>\n",
       "      <td>0.310096</td>\n",
       "      <td>0.076923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LR</td>\n",
       "      <td>0.50</td>\n",
       "      <td>-0.011609</td>\n",
       "      <td>-0.006652</td>\n",
       "      <td>0.206687</td>\n",
       "      <td>0.068728</td>\n",
       "      <td>-0.223608</td>\n",
       "      <td>0.317308</td>\n",
       "      <td>0.076923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LR</td>\n",
       "      <td>0.52</td>\n",
       "      <td>-0.015098</td>\n",
       "      <td>-0.008658</td>\n",
       "      <td>0.203788</td>\n",
       "      <td>0.056298</td>\n",
       "      <td>-0.229025</td>\n",
       "      <td>0.300481</td>\n",
       "      <td>0.081731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LR</td>\n",
       "      <td>0.64</td>\n",
       "      <td>-0.002210</td>\n",
       "      <td>-0.001264</td>\n",
       "      <td>0.133987</td>\n",
       "      <td>0.056272</td>\n",
       "      <td>-0.154883</td>\n",
       "      <td>0.105769</td>\n",
       "      <td>0.086538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LR</td>\n",
       "      <td>0.65</td>\n",
       "      <td>-0.012117</td>\n",
       "      <td>-0.006944</td>\n",
       "      <td>0.131351</td>\n",
       "      <td>0.008746</td>\n",
       "      <td>-0.157904</td>\n",
       "      <td>0.093750</td>\n",
       "      <td>0.081731</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   model   tau  total_return      CAGR  vol_annual    Sharpe  max_drawdown  \\\n",
       "4     LR  0.54      0.110391  0.061681    0.191302  0.426684     -0.161143   \n",
       "5     LR  0.55      0.073879  0.041583    0.182496  0.327327     -0.202796   \n",
       "3     LR  0.53      0.046769  0.026471    0.200241  0.237934     -0.202824   \n",
       "7     LR  0.57      0.038721  0.021953    0.168304  0.220221     -0.170964   \n",
       "6     LR  0.56      0.028869  0.016401    0.175484  0.185351     -0.196387   \n",
       "13    LR  0.63      0.021197  0.012062    0.141483  0.159846     -0.122411   \n",
       "12    LR  0.62      0.010087  0.005753    0.145827  0.113896     -0.142390   \n",
       "1     LR  0.51      0.002878  0.001644    0.205808  0.110968     -0.221052   \n",
       "0     LR  0.50     -0.011609 -0.006652    0.206687  0.068728     -0.223608   \n",
       "2     LR  0.52     -0.015098 -0.008658    0.203788  0.056298     -0.229025   \n",
       "14    LR  0.64     -0.002210 -0.001264    0.133987  0.056272     -0.154883   \n",
       "15    LR  0.65     -0.012117 -0.006944    0.131351  0.008746     -0.157904   \n",
       "\n",
       "    hit_rate  turnover  \n",
       "4   0.286058  0.086538  \n",
       "5   0.269231  0.081731  \n",
       "3   0.300481  0.086538  \n",
       "7   0.233173  0.091346  \n",
       "6   0.252404  0.076923  \n",
       "13  0.122596  0.081731  \n",
       "12  0.141827  0.096154  \n",
       "1   0.310096  0.076923  \n",
       "0   0.317308  0.076923  \n",
       "2   0.300481  0.081731  \n",
       "14  0.105769  0.086538  \n",
       "15  0.093750  0.081731  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: data\\multiticker_tau_sweep.csv\n"
     ]
    }
   ],
   "source": [
    "# --- Sweep tau and pick best Sharpe per model ---\n",
    "results = []\n",
    "curves  = {}\n",
    "\n",
    "for name, p in probs.items():\n",
    "    best = None\n",
    "    for tau in TAU_GRID:\n",
    "        pos = positions_from_probs(p, tau, allow_short=ALLOW_SHORT)\n",
    "        eq, parts = equity_curve(r_next, pos, FEE_BPS, SLIPPAGE_BPS)\n",
    "        metrics = perf_metrics(parts[\"net\"], dates_te)\n",
    "        row = {\"model\": name, \"tau\": float(tau), **metrics}\n",
    "        results.append(row)\n",
    "        if best is None or metrics[\"Sharpe\"] > best[\"Sharpe\"]:\n",
    "            best = {**row}\n",
    "            curves[name] = (eq, pos, parts)\n",
    "    print(f\"{name}: best Sharpe @ tau={best['tau']:.3f} | Sharpe={best['Sharpe']:.2f} | CAGR={best['CAGR']:.2%}\")\n",
    "\n",
    "bt_df = pd.DataFrame(results).sort_values([\"model\",\"Sharpe\"], ascending=[True, False])\n",
    "display(bt_df.head(12))\n",
    "\n",
    "out_csv = DATA_DIR / \"multiticker_tau_sweep.csv\"\n",
    "bt_df.to_csv(out_csv, index=False)\n",
    "print(\"Saved:\", out_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80538428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: reports\\figures\\equity_curve_lr.png\n"
     ]
    }
   ],
   "source": [
    "# --- Equity curve plots (best per model) ---\n",
    "for name, (eq, pos, parts) in curves.items():\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.plot(eq, label=f\"{name}\")\n",
    "    plt.title(f\"Equity Curve – {name}\")\n",
    "    plt.xlabel(\"Time (test)\")\n",
    "    plt.ylabel(\"Equity (x)\")\n",
    "    plt.grid(True, alpha=.3)\n",
    "    plt.legend()\n",
    "    out = FIG_DIR / f\"equity_curve_{name.lower()}.png\"\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out, dpi=150)\n",
    "    plt.close()\n",
    "    print(\"Saved:\", out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "68e09544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: artifacts\\backtest_summary.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'fee_bps': 5.0,\n",
       " 'slippage_bps': 0.0,\n",
       " 'allow_short': False,\n",
       " 'models': {'LR': {'tau': 0.54,\n",
       "   'Sharpe': 0.42668389532869555,\n",
       "   'CAGR': 0.06168067915059794,\n",
       "   'max_drawdown': -0.16114303068015778,\n",
       "   'hit_rate': 0.2860576923076923,\n",
       "   'total_return': 0.11039118539082615,\n",
       "   'vol_annual': 0.19130180407665096}}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Backtest summary JSON ---\n",
    "summary = {\"fee_bps\": FEE_BPS, \"slippage_bps\": SLIPPAGE_BPS, \"allow_short\": ALLOW_SHORT, \"models\": {}}\n",
    "\n",
    "for name in probs.keys():\n",
    "    best_row = bt_df[bt_df[\"model\"]==name].iloc[0].to_dict()\n",
    "    summary[\"models\"][name] = {\n",
    "        \"tau\": best_row[\"tau\"],\n",
    "        \"Sharpe\": best_row[\"Sharpe\"],\n",
    "        \"CAGR\": best_row[\"CAGR\"],\n",
    "        \"max_drawdown\": best_row[\"max_drawdown\"],\n",
    "        \"hit_rate\": best_row[\"hit_rate\"],\n",
    "        \"total_return\": best_row[\"total_return\"],\n",
    "        \"vol_annual\": best_row[\"vol_annual\"]\n",
    "    }\n",
    "\n",
    "out_json = ART_DIR / \"backtest_summary.json\"\n",
    "with open(out_json, \"w\") as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "print(\"Wrote:\", out_json)\n",
    "summary\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.13 (.venv stock-direction-ml)",
   "language": "python",
   "name": "stock-direction-ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
